# Wagstaff-Lab Project Snapshot

- Generated: 2026-01-20T23:23:27
- Mode: llm
- Template: core

## 1. Environment Diagnostics
```yaml
Time: 2026-01-20 23:23:27
User: steam
Host: VM-8-2-ubuntu (Linux 6.8.0-49-generic)
Python: 3.13.11 (/home/steam/miniconda3/bin/python)
Rich Ver: Installed (ver unknown)
--------------------
Branch: main [DIRTY]
Commit: 8dee9c5
Message: feat(core): add mechanism schema and diff tools
```

## 2. Project Overview
### Toolbox (apps/cli/registry.py)
```text
alias           | file                     | type | desc                            | usage                                           
----------------+--------------------------+------+---------------------------------+-------------------------------------------------
dash            | dash.py                  | CLI  | Wagstaff-Lab 控制台主面板             | wagstaff dash                                   
doctor          | doctor.py                | CLI  | 环境配置与依赖健康检查                     | wagstaff doctor                                 
wiki            | wiki.py                  | CLI  | 物品/配方/数值查询百科                    | wagstaff wiki <item_code>                       
exp             | explorer.py              | CLI  | 源码结构浏览与深度分析                     | wagstaff exp                                    
mgmt            | mgmt.py                  | CLI  | 项目管理：状态展示与同步                    | wagstaff mgmt <status|sync|dump|check>          
server          | server.py                | CLI  | DST 服务器管理 (screen-based)        | wagstaff server <status|start|stop|restart|up...
report          | report_hub.py            | Dev  | 报告中心：build/list/open            | wagstaff report [build|list|open] [--all] [--...
portal          | portal_hub.py            | Dev  | 聚合视图：管理 + 报告 + 质量               | wagstaff portal [build|list|open]               
catalog2        | build_catalog_v2.py      | Dev  | 生成 Catalog v2 (item-centric)    | wagstaff catalog2 [--dst-root PATH] [--tuning...
catalog-sqlite  | build_catalog_sqlite.py  | Dev  | 生成 Catalog SQLite v4            | wagstaff catalog-sqlite [--catalog PATH] [--o...
i18n            | build_i18n_index.py      | Dev  | 生成 i18n 索引 (names + UI strings) | wagstaff i18n [--lang zh] [--dst-root PATH]     
farming-defs    | build_farming_defs.py    | Dev  | 生成耕种机制索引 (farming defs)         | wagstaff farming-defs [--dst-root PATH]         
icons           | build_icons.py           | Dev  | 生成物品图标 PNG + icon index         | wagstaff icons [--dst-root PATH] [--all-eleme...
catindex        | build_catalog_index.py   | Dev  | 生成 Catalog 紧凑索引（列表 + 多维倒排）      | wagstaff catindex [--catalog PATH] [--icon-in...
mechanism-index | build_mechanism_index.py | Dev  | 生成机制索引（组件 + prefab 关系）          | wagstaff mechanism-index [build|validate|diff...
index-manifest  | build_index_manifest.py  | Dev  | 生成索引版本清单 (manifest)             | wagstaff index-manifest [--out PATH]            
quality         | quality_gate.py          | Dev  | 质量门禁自检（默认仅提示）                   | wagstaff quality [--enforce] [--strict]         
snap            | snapshot.py              | Dev  | 生成 LLM 友好代码快照                   | wagstaff snap [-h] [--mode {llm,core,archive,...
samples         | sampler.py               | Dev  | 生成 DST Lua 样本包（用于扩展解析器）         | wagstaff samples [--categories ...] [--n N] [...
resindex        | build_resource_index.py  | Dev  | 生成 DST 资源索引（scripts + data）     | wagstaff resindex [--data-full] [--bundle-ful...
web             | serve_webcraft.py        | Dev  | 启动 WebCraft (FastAPI + Uvicorn) | wagstaff web [--host 0.0.0.0 --port 20000]      
```

### Project Context (PROJECT_STATUS.json)
```text
DEV MANIFESTO:
* 分层清晰：core 负责解析与索引，apps 负责交互与服务，devtools 提供流程工具
* 单向依赖：apps/devtools 可以依赖 core，core 不依赖上层
* 数据契约：对外数据统一落盘 data/，版本化命名并携带元信息
* 路径自适应：禁止硬编码绝对路径，统一通过 __file__ 推导
* 可追溯与可复用：产物记录来源 hash，核心能力被 CLI/Web 共享
* 稳健降级：优先 scripts.zip，缺失时降级 scripts/ 目录
* LLM 快照规范：snapshot.py 作为统一导出入口，模板与 sections 控制噪声
--------------------
OBJECTIVE: Wagstaff Lab v3.0 架构重构：core/apps 分层与 WebCraft 服务化

TASKS TODO:
- Catalog stats 解析继续扩展：优先补齐 equippable/rechargeable/heater 等仍有缺口的组件字段，并提升动态/条件赋值的覆盖率。
- i18n 覆盖率提升：扩充 names/desc/quotes 索引并完善 UI 词条。
- CatalogV2 索引与质量报告持续迭代：对比覆盖率与新增字段。

TASKS DONE:
- 重构 Wiki 和 Explorer 使用 Engine
- 实现情报扫描结果保存到 data/reports/
- 创建 apps/cli/commands/dash.py 项目概况面板
- 文档化：在 README.md 中记录新工具接入 SOP
- 重构工具注册逻辑：引入 apps/cli/registry.py 实现单一数据源
- Hotfix(pm): 修复 STATUS_FILE 路径依赖并实现原子化写入 (.tmp -> rename)
- Wiki v2.3: 实现交互式搜索 (find) 与详细组件展示
- Analyzer: 支持内联掉落表解析 (Krampus) 与 Tuning 递归数值追踪
- PM Tool: 修复 JSON 数据结构并实现自动迁移 (v1->v2)
- WebCraft: Catalog UI 修复 i18n helper 缺失导致的空白与标签切换异常
- Refactor: core/apps 拆分，CLI 与 WebCraft 应用层分离
- CatalogV2: WebCraft 后端切换到 v2 items 索引，提供更完整的 item 元信息。
- CatalogV2: Catalog UI 支持 v2 元信息检索与详情展示。
- WebCraft: tuning trace API 与 UI 按需加载实现。
- CatalogV2: item stats 提取（weapon/armor/uses 等）并在 UI 展示。
- CLI: commands 子目录与 main dispatcher 重构完成。
- Tooling: 引入 pyproject.toml 与 Makefile 统一依赖与任务入口。
- Tooling: 移除 bin/ 包装器与 installer，CLI 仅通过 pyproject 入口提供。
- Server: 将 tempforcodex 服务器管理脚本重构为 wagstaff server 子命令。
- Mgmt: 退役 pm 工具，统一使用 PROJECT_MANAGEMENT + PROJECT_STATUS。
- Stats 缺口定位：清点 equippable/rechargeable/heater 缺失样本并抽查 prefab 动态/条件赋值。
- Stats 解析补齐：按 low_coverage_keys 优先补 recharge_max/recharge_time 与 carried_heat/heat_radius 的解析路径。
- Storage: 导出 SQLite catalog 并在 WebCraft 优先加载 SQLite、缺失时回退 JSON。
- WebCraft: catalog 分页 + 缓存头，UI 分页加载与搜索改造。
- WebCraft: tuning trace 前缀索引优化，i18n 切换为 index-only。
- WebCraft: 重构 Craft/Cooking/Catalog 前端布局与交互，统一页面结构与移动端体验。
- WebCraft: Craft/Cooking/Catalog 搜索改为后端 i18n index 驱动，移除前端中文回退逻辑。
- Icons: 修复 KTEX 图标方向翻转并同步 build_icons 导出流程。
- Tooling: 修复 build_icons 导入异常导致的 CLI 启动失败。
- CatalogV2: 新增 cooking_ingredients 解析与落盘（ingredients.lua/cooking.lua）。
- WebCraft: Cooking 探索/模拟重做，支持接近可做与排序公式说明。
- Farming: 新增耕种机制索引与报告（farming defs）
- 新增 index manifest 生成器与 Makefile 入口（统一索引版本清单）
- SQLite 产物新增 db_schema_version 标记（catalog/mechanism）
- core 收口：klei_atlas_tex 迁移至 core/assets，配置加载迁移至 core/config
- 全量重建索引并生成 wagstaff_index_manifest.json
- SQLite v4 schema 规范草案（docs/specs/SQLITE_V4_SPEC.md）
- SQLite v4 架构规范完成（SQLITE_V4_SPEC）
- SQLite v4 构建器：catalog/mechanism 输出 + WebCraft 优先加载 v4 表
- SQLite v4: 新增 SQLite 校验工具并接入 quality_gate
- Mechanism: prefab 解析扩张（events/assets/component_calls）并同步 SQLite/schema
- Portal: 新增聚合视图 (管理 + 报告 + 质量) 并提供 portal_hub

RECENT LOGS:
- [2026-01-20 19:15] SQLite v4: 新增 validate_sqlite_v4 校验工具并接入 quality_gate。
- [2026-01-20 19:15] Mechanism: prefab 解析扩张（events/assets/component_calls），同步 SQLite/规范/Schema。
- [2026-01-20 19:45] Portal: 新增 portal_hub 聚合管理/报告/质量视图。
- [2026-01-20 23:00] Tooling: 收口质量/机制索引工具链，校验逻辑内退并统一入口。
- [2026-01-21 10:15] CLI: Dashboard 展示与命令分组更新，强化报告/产物可视化。
```

## 3. Project Structure
```text
├── .gitignore
├── apps
│   ├── __init__.py
│   ├── cli
│   │   ├── __init__.py
│   │   ├── cli_common.py
│   │   ├── commands
│   │   │   ├── __init__.py
│   │   │   ├── dash.py
│   │   │   ├── doctor.py
│   │   │   ├── explorer.py
│   │   │   ├── mgmt.py
│   │   │   ├── server.py
│   │   │   └── wiki.py
│   │   ├── i18n.py
│   │   ├── main.py
│   │   ├── mgmt_parser.py
│   │   └── registry.py
│   ├── server
│   │   ├── __init__.py
│   │   ├── cli.py
│   │   ├── config.py
│   │   ├── manager.py
│   │   └── ui.py
│   └── webcraft
│       ├── __init__.py
│       ├── api.py
│       ├── app.py
│       ├── catalog_store.py
│       ├── cooking_planner.py
│       ├── i18n_index.py
│       ├── i18n_service.py
│       ├── icon_service.py
│       ├── mechanism_store.py
│       ├── planner.py
│       ├── settings.py
│       ├── static
│       │   ├── css
│       │   │   ├── base.css
│       │   │   ├── catalog.css
│       │   │   ├── cooking.css
│       │   │   └── craft.css
│       │   ├── fonts
│       │   │   ├── BricolageGrotesque-normal-400.ttf
│       │   │   ├── BricolageGrotesque-normal-600.ttf
│       │   │   ├── BricolageGrotesque-normal-700.ttf
│       │   │   ├── IBMPlexSans-normal-400.ttf
│       │   │   ├── IBMPlexSans-normal-500.ttf
│       │   │   ├── IBMPlexSans-normal-600.ttf
│       │   │   ├── JetBrainsMono-normal-400.ttf
│       │   │   └── JetBrainsMono-normal-500.ttf
│       │   └── js
│       │       ├── app.js
│       │       ├── core
│       │       │   ├── .gitkeep
│       │       │   ├── api.js
│       │       │   ├── dom.js
│       │       │   └── env.js
│       │       └── pages
│       │           ├── .gitkeep
│       │           ├── catalog.js
│       │           ├── cooking_encyclopedia.js
│       │           ├── cooking_shared.js
│       │           ├── cooking_tool.js
│       │           └── craft.js
│       ├── templates
│       │   ├── catalog.html
│       │   ├── cooking.html
│       │   ├── cooking_tools.html
│       │   └── index.html
│       ├── tuning_trace.py
│       └── ui.py
├── conf
│   ├── i18n_tags.json
│   ├── i18n_ui.json
│   ├── samples
│   │   ├── parse_other_data.json
│   │   └── tag_overrides.example.json
│   ├── settings.ini
│   ├── snapshot_templates.json
│   └── version.json
├── core
│   ├── __init__.py
│   ├── assets
│   │   ├── __init__.py
│   │   └── klei_atlas_tex.py
│   ├── config
│   │   ├── __init__.py
│   │   └── loader.py
│   ├── craft_recipes.py
│   ├── engine.py
│   ├── indexers
│   │   ├── __init__.py
│   │   ├── catalog_index.py
│   │   ├── catalog_v2.py
│   │   ├── farming_defs.py
│   │   ├── i18n_index.py
│   │   ├── mechanism_index.py
│   │   ├── resource_index.py
│   │   └── shared.py
│   ├── lua
│   │   ├── __init__.py
│   │   ├── call_extractor.py
│   │   ├── expr.py
│   │   ├── match.py
│   │   ├── scan.py
│   │   └── split.py
│   ├── parsers
│   │   ├── __init__.py
│   │   ├── base.py
│   │   ├── component.py
│   │   ├── cooking.py
│   │   ├── loot.py
│   │   ├── lua_analyzer.py
│   │   ├── prefab.py
│   │   ├── strings.py
│   │   ├── tuning.py
│   │   └── widget.py
│   ├── schemas
│   │   ├── __init__.py
│   │   ├── catalog_v2.py
│   │   └── meta.py
│   ├── tagging.py
│   └── version.py
├── data
│   ├── index
│   │   ├── .build_cache.json
│   │   ├── wagstaff_catalog_index_v1.json
│   │   ├── wagstaff_catalog_v2.json
│   │   ├── wagstaff_farming_defs_v1.json
│   │   ├── wagstaff_i18n_v1.json
│   │   ├── wagstaff_icon_index_v1.json
│   │   ├── wagstaff_index_manifest.json
│   │   ├── wagstaff_mechanism_index_v1.json
│   │   ├── wagstaff_resource_index_v1.json
│   │   └── wagstaff_tuning_trace_v1.json
│   ├── reports
│   │   ├── asset_registry.md
│   │   ├── catalog_index_summary.md
│   │   ├── catalog_quality_report.json
│   │   ├── catalog_quality_report.md
│   │   ├── catalog_v2_summary.md
│   │   ├── dst_raw_coverage.json
│   │   ├── dst_raw_coverage.md
│   │   ├── index.html
│   │   ├── mechanism_crosscheck_report.md
│   │   ├── mechanism_index_summary.md
│   │   ├── portal_index.html
│   │   ├── quality_gate_report.json
│   │   ├── quality_gate_report.md
│   │   ├── recipe_distribution.md
│   │   ├── resource_index_summary.md
│   │   ├── stats_gap_inspect.json
│   │   ├── stats_gap_inspect.md
│   │   ├── wagstaff_portal_manifest.json
│   │   └── wagstaff_report_manifest.json
│   └── static
│       ├── i18n
│       │   └── names_zh.json
│       └── icons
├── devtools
│   ├── __init__.py
│   ├── build_cache.py
│   ├── build_catalog_index.py
│   ├── build_catalog_sqlite.py
│   ├── build_catalog_v2.py
│   ├── build_farming_defs.py
│   ├── build_i18n_index.py
│   ├── build_icons.py
│   ├── build_index_manifest.py
│   ├── build_mechanism_index.py
│   ├── build_resource_index.py
│   ├── catalog_quality.py
│   ├── portal_hub.py
│   ├── quality_gate.py
│   ├── raw_scan.py
│   ├── report_hub.py
│   ├── report_utils.py
│   ├── sampler.py
│   ├── serve_webcraft.py
│   ├── snapshot.py
│   ├── snapshot_gui.py
│   ├── stats_gap_inspect.py
│   └── validators.py
├── docs
│   ├── architecture
│   │   └── WEBCRAFT_NETWORK_STACK.md
│   ├── archived
│   │   ├── COOKING_UPGRADE.md
│   │   └── WEBCRAFT_UI_MODULARIZATION.md
│   ├── guides
│   │   ├── CLI_GUIDE.md
│   │   └── DEV_GUIDE.md
│   ├── management
│   │   ├── PROJECT_MANAGEMENT.md
│   │   ├── ROADMAP.md
│   │   ├── VNEXT_REFACTOR_PLAN.md
│   │   ├── WEBCRAFT_ATLAS_LEDGER_UI_REFACTOR.md
│   │   └── WEBCRAFT_MOBILE_APP_SHELL.md
│   ├── README.md
│   └── specs
│       ├── CATALOG_V2_SPEC.md
│       ├── FARMING_MECHANICS_REPORT.md
│       ├── MECHANISM_INDEX_SPEC.md
│       ├── mechanism_index_v1.schema.json
│       └── SQLITE_V4_SPEC.md
├── Makefile
├── PROJECT_STATUS.json
├── pyproject.toml
├── README.md
├── setup.sh
├── tempforcodex
│   ├── boot.sh
│   └── dst_tool.sh
├── tests
│   └── test_recipes.py
└── wagstaff_lab.egg-info
    ├── dependency_links.txt
    ├── entry_points.txt
    ├── PKG-INFO
    ├── requires.txt
    ├── SOURCES.txt
    └── top_level.txt
```

## 4. File Inventory
(mode: full/interface/head/skip; '*' means truncated when rendered)

```text
mode      | bytes | sha256_12 | path                                                
----------+-------+-----------+-----------------------------------------------------
head      | 4899  | -         | README.md                                           
full      | 52    | -         | apps/__init__.py                                    
full      | 43    | -         | apps/cli/__init__.py                                
full      | 1915  | -         | apps/cli/cli_common.py                              
full      | 51    | -         | apps/cli/commands/__init__.py                       
full      | 11582 | -         | apps/cli/commands/dash.py                           
full      | 6232  | -         | apps/cli/commands/doctor.py                         
full      | 9805  | -         | apps/cli/commands/explorer.py                       
full      | 7876  | -         | apps/cli/commands/mgmt.py                           
full      | 209   | -         | apps/cli/commands/server.py                         
full      | 22341 | -         | apps/cli/commands/wiki.py                           
full      | 3712  | -         | apps/cli/i18n.py                                    
full      | 1747  | -         | apps/cli/main.py                                    
full      | 2233  | -         | apps/cli/mgmt_parser.py                             
full      | 5733  | -         | apps/cli/registry.py                                
full      | 66    | -         | apps/server/__init__.py                             
full      | 4691  | -         | apps/server/cli.py                                  
full      | 2974  | -         | apps/server/config.py                               
full      | 7508  | -         | apps/server/manager.py                              
full      | 39402 | -         | apps/server/ui.py                                   
full      | 48    | -         | apps/webcraft/__init__.py                           
full      | 35980 | -         | apps/webcraft/api.py                                
full      | 9336  | -         | apps/webcraft/app.py                                
full      | 59435 | -         | apps/webcraft/catalog_store.py                      
full      | 41532 | -         | apps/webcraft/cooking_planner.py                    
full      | 4390  | -         | apps/webcraft/i18n_index.py                         
full      | 26143 | -         | apps/webcraft/i18n_service.py                       
full      | 12077 | -         | apps/webcraft/icon_service.py                       
full      | 15278 | -         | apps/webcraft/mechanism_store.py                    
full      | 3808  | -         | apps/webcraft/planner.py                            
full      | 919   | -         | apps/webcraft/settings.py                           
full      | 3858  | -         | apps/webcraft/tuning_trace.py                       
full      | 1260  | -         | apps/webcraft/ui.py                                 
full      | 353   | -         | conf/settings.ini                                   
full      | 61    | -         | core/__init__.py                                    
full      | 61    | -         | core/assets/__init__.py                             
full      | 17536 | -         | core/assets/klei_atlas_tex.py                       
full      | 155   | -         | core/config/__init__.py                             
full      | 1022  | -         | core/config/loader.py                               
full      | 28217 | -         | core/craft_recipes.py                               
full      | 16480 | -         | core/engine.py                                      
full      | 48    | -         | core/indexers/__init__.py                           
full      | 7063  | -         | core/indexers/catalog_index.py                      
full      | 25945 | -         | core/indexers/catalog_v2.py                         
full      | 19303 | -         | core/indexers/farming_defs.py                       
full      | 6512  | -         | core/indexers/i18n_index.py                         
full      | 16549 | -         | core/indexers/mechanism_index.py                    
full      | 23514 | -         | core/indexers/resource_index.py                     
full      | 2067  | -         | core/indexers/shared.py                             
full      | 1089  | -         | core/lua/__init__.py                                
full      | 5394  | -         | core/lua/call_extractor.py                          
full      | 4804  | -         | core/lua/expr.py                                    
full      | 1691  | -         | core/lua/match.py                                   
full      | 3332  | -         | core/lua/scan.py                                    
full      | 3617  | -         | core/lua/split.py                                   
full      | 812   | -         | core/parsers/__init__.py                            
full      | 540   | -         | core/parsers/base.py                                
full      | 3047  | -         | core/parsers/component.py                           
full      | 27295 | -         | core/parsers/cooking.py                             
full      | 2175  | -         | core/parsers/loot.py                                
full      | 1623  | -         | core/parsers/lua_analyzer.py                        
full      | 3102  | -         | core/parsers/prefab.py                              
full      | 610   | -         | core/parsers/strings.py                             
full      | 12559 | -         | core/parsers/tuning.py                              
full      | 575   | -         | core/parsers/widget.py                              
full      | 47    | -         | core/schemas/__init__.py                            
full      | 824   | -         | core/schemas/catalog_v2.py                          
full      | 808   | -         | core/schemas/meta.py                                
full      | 5775  | -         | core/tagging.py                                     
full      | 1182  | -         | core/version.py                                     
head      | 2096  | -         | data/reports/asset_registry.md                      
head      | 198   | -         | data/reports/catalog_index_summary.md               
head      | 3351  | -         | data/reports/catalog_quality_report.md              
head      | 341   | -         | data/reports/catalog_v2_summary.md                  
head      | 12001 | -         | data/reports/dst_raw_coverage.md                    
head      | 1824  | -         | data/reports/mechanism_crosscheck_report.md         
head      | 821   | -         | data/reports/mechanism_index_summary.md             
head      | 1581  | -         | data/reports/quality_gate_report.md                 
head      | 1655  | -         | data/reports/recipe_distribution.md                 
head      | 1247  | -         | data/reports/resource_index_summary.md              
head      | 1124  | -         | data/reports/stats_gap_inspect.md                   
interface | 48    | -         | devtools/__init__.py                                
interface | 2553  | -         | devtools/build_cache.py                             
interface | 2031  | -         | devtools/build_catalog_index.py                     
interface | 24035 | -         | devtools/build_catalog_sqlite.py                    
interface | 6479  | -         | devtools/build_catalog_v2.py                        
interface | 3327  | -         | devtools/build_farming_defs.py                      
interface | 9722  | -         | devtools/build_i18n_index.py                        
interface | 29929 | -         | devtools/build_icons.py                             
interface | 5866  | -         | devtools/build_index_manifest.py                    
interface | 22780 | -         | devtools/build_mechanism_index.py                   
interface | 5320  | -         | devtools/build_resource_index.py                    
interface | 14996 | -         | devtools/catalog_quality.py                         
interface | 24138 | -         | devtools/portal_hub.py                              
interface | 13291 | -         | devtools/quality_gate.py                            
interface | 29214 | -         | devtools/raw_scan.py                                
interface | 24028 | -         | devtools/report_hub.py                              
interface | 7650  | -         | devtools/report_utils.py                            
interface | 14614 | -         | devtools/sampler.py                                 
interface | 6049  | -         | devtools/serve_webcraft.py                          
full      | 51404 | -         | devtools/snapshot.py                                
interface | 66112 | -         | devtools/snapshot_gui.py                            
interface | 13516 | -         | devtools/stats_gap_inspect.py                       
interface | 12630 | -         | devtools/validators.py                              
head      | 1002  | -         | docs/README.md                                      
head      | 1868  | -         | docs/architecture/WEBCRAFT_NETWORK_STACK.md         
head      | 2022  | -         | docs/archived/COOKING_UPGRADE.md                    
head      | 2829  | -         | docs/archived/WEBCRAFT_UI_MODULARIZATION.md         
head      | 3291  | -         | docs/guides/CLI_GUIDE.md                            
head      | 11820 | -         | docs/guides/DEV_GUIDE.md                            
head      | 5613  | -         | docs/management/PROJECT_MANAGEMENT.md               
head      | 2099  | -         | docs/management/ROADMAP.md                          
head      | 7679  | -         | docs/management/VNEXT_REFACTOR_PLAN.md              
head      | 1395  | -         | docs/management/WEBCRAFT_ATLAS_LEDGER_UI_REFACTOR.md
head      | 2388  | -         | docs/management/WEBCRAFT_MOBILE_APP_SHELL.md        
head      | 11740 | -         | docs/specs/CATALOG_V2_SPEC.md                       
head      | 4401  | -         | docs/specs/FARMING_MECHANICS_REPORT.md              
head      | 3037  | -         | docs/specs/MECHANISM_INDEX_SPEC.md                  
head      | 7953  | -         | docs/specs/SQLITE_V4_SPEC.md                        
head      | 1653  | -         | tests/test_recipes.py                               
```

## 5. File Contents

### File: README.md
- mode: head
- size_bytes: 4899
- sha256_12: 51d4e400a967

```md
# Wagstaff-Lab (v4.0.0-dev)

Wagstaff-Lab 是 DST（Don't Starve Together）数据实验室：负责索引、分析与 WebCraft UI，所有上层展示都基于稳定的索引产物。当前架构为 `core/` 解析与索引、`apps/` 应用层、`devtools/` 构建与报告工具。

## 必读

- `docs/guides/DEV_GUIDE.md`：开发规范与强制约束
- `docs/management/PROJECT_MANAGEMENT.md`：管理与进度执行入口

## 当前能力

- **Catalog v2**：以物品为中心的可标签化目录，含 stats 与 assets
- **Tuning trace**：可选输出 TUNING 解析链路
- **i18n index**：名称 + UI 词条（数据层与语言解耦）
- **Icon pipeline**：静态图标 + 动态回退
- **Mechanism index**：组件解析 + prefab 链路 + SQLite 输出
- **Quality gate + Report hub**：质量门禁 + 报告汇总入口
- **Index manifest**：索引清单与版本汇总
- **WebCraft**：FastAPI UI，严格使用索引产物

## 安装（pyproject 入口）

建议在 `dst_lab` 环境中执行：

```bash
python -m pip install -e ".[cli]"
```

全量依赖（web + icons + quality）：

```bash
python -m pip install -e ".[all]"
```

CLI 入口为 `wagstaff`。

## 配置 DST 路径

在 `conf/settings.ini` 中配置 `DST_ROOT`，或通过命令参数 `--dst-root` 覆盖。

示例：
```
[PATHS]
DST_ROOT=/path/to/dontstarvetogether_dedicated_server
```

## 构建流程

所有产物落盘在 `data/`，并带版本后缀。

一键构建：
```bash
make all
```
包含 farming-defs（耕种机制索引）与质量门禁。

报告构建：
```bash
wagstaff report build --all
```

更多构建子命令见 `docs/guides/CLI_GUIDE.md`。

## 启动 WebCraft

```bash
wagstaff web --host 0.0.0.0 --port 20000 --reload-catalog
```

WebCraft 优先读取 `data/index/wagstaff_catalog_v2.sqlite`（缺失时回退 JSON）。
i18n 仅使用 `data/index/wagstaff_i18n_v1.json`（运行时不解析 PO）。

默认本地启动：
```bash
wagstaff web
```

## CLI 核心命令

- `wagstaff` / `wagstaff dash`：项目概览面板
- `wagstaff doctor`：环境与产物检查（信息提示）
- `wagstaff resindex`：资源索引构建
- `wagstaff catalog2`：Catalog v2 构建
- `wagstaff catalog-sqlite`：Catalog SQLite v4 构建
- `wagstaff catindex`：Catalog 紧凑索引构建
- `wagstaff mechanism-index`：机制索引（build/validate/diff）
- `wagstaff quality`：质量/校验总入口
- `wagstaff report`：报告中心（build/list/open）
- `wagstaff portal`：管理+报告+质量聚合视图
- `wagstaff web`：启动 WebCraft
- `wagstaff server`：DST 服务器管理（screen 会话）

完整命令清单见 `docs/guides/CLI_GUIDE.md`。

## 服务器管理示例

```bash
wagstaff server status
wagstaff server ui
wagstaff server start
wagstaff server stop --timeout 40 --force
wagstaff server backup
wagstaff server restore --latest --yes --start
wagstaff server logs --shard master --follow
wagstaff server cmd "c_announce(\"hello\")"
```

## 关键产物

索引产物（data/index）：
```
data/index/wagstaff_resource_index_v1.json
data/index/wagstaff_catalog_v2.json
data/index/wagstaff_catalog_v2.sqlite
data/index/wagstaff_catalog_index_v1.json
data/index/wagstaff_farming_defs_v1.json
data/index/wagstaff_mechanism_index_v1.json
data/index/wagstaff_mechanism_index_v1.sqlite
data/index/wagstaff_i18n_v1.json
data/index/wagstaff_icon_index_v1.json
data/index/wagstaff_tuning_trace_v1.json
data/index/wagstaff_index_manifest.json
```

报告产物（data/reports，report hub 生成）：
```
data/reports/quality_gate_report.md
data/reports/mechanism_index_summary.md
data/reports/mechanism_crosscheck_report.md
data/reports/catalog_quality_report.md
data/reports/wagstaff_report_manifest.json
data/reports/index.html
data/reports/portal_index.html
```

## 项目结构

```
core/            解析 + 索引 + schemas
core/lua/        Lua 解析基元
core/parsers/    Prefab/Loot/Cooking 等解析器
core/indexers/   索引构建逻辑
core/schemas/    数据契约 + meta 辅助
apps/cli/        CLI dispatcher + commands
apps/server/     DST server ops (isolated from data analysis)
apps/webcraft/   WebCraft API + UI
devtools/        构建/报告/快照工具
conf/            配置与快照模板
data/            产物与报告
docs/            guides/ specs/ management/ architecture
```

WebCraft UI 模板与静态资源：
- `apps/webcraft/templates/`：HTML 模板
- `apps/webcraft/static/`：CSS/JS/字体（对外挂载 `/static/app`）

## 文档入口

- `docs/README.md`：文档索引
- `docs/guides/DEV_GUIDE.md`：开发规范
- `docs/guides/CLI_GUIDE.md`：CLI 角色与职责
- `docs/specs/CATALOG_V2_SPEC.md`：Catalog v2 规范
- `docs/management/ROADMAP.md`：项目路线图
- `docs/management/PROJECT_MANAGEMENT.md`：项目管理与进度
- `docs/management/VNEXT_REFACTOR_PLAN.md`：vNext 重构规划（破兼容版）
```

### File: docs/README.md
- mode: head
- size_bytes: 1002
- sha256_12: c49ff2098dd4

```md
# Docs Index

文档按用途分层，避免规划/规范/实现混杂。

## guides/

- `guides/DEV_GUIDE.md`：开发规范与强制约束
- `guides/CLI_GUIDE.md`：CLI 角色与职责

## management/

- `management/PROJECT_MANAGEMENT.md`：执行管理与进度
- `management/ROADMAP.md`：长期方向
- `management/VNEXT_REFACTOR_PLAN.md`：vNext 重构规划（破兼容版）

## specs/

- `specs/CATALOG_V2_SPEC.md`：Catalog v2 + API 契约
- `specs/MECHANISM_INDEX_SPEC.md`：Mechanism index v1 规范草案
- `specs/mechanism_index_v1.schema.json`：Mechanism index v1 JSON schema
- `specs/SQLITE_V4_SPEC.md`：SQLite v4 结构设计
- `specs/FARMING_MECHANICS_REPORT.md`：DST 耕种机制报告与关键调参

## architecture/

- `architecture/WEBCRAFT_NETWORK_STACK.md`：WebCraft 网络与服务栈

## archived/

- `archived/WEBCRAFT_UI_MODULARIZATION.md`：已完成的 WebCraft UI 模块化改造计划
- `archived/COOKING_UPGRADE.md`：Cooking 探索/模拟升级方案（归档）
```

### File: conf/settings.ini
- mode: full
- size_bytes: 353
- sha256_12: 15952c921af7

```toml
[PATHS]
# 游戏安装目录
DST_ROOT = ~/dontstarvetogether_dedicated_server
# SteamCMD 目录
STEAMCMD_DIR = ~/steamcmd
# 备份存放目录
BACKUP_DIR = ~/dst_backups

[SERVER]
# 你的存档文件夹名 (Cluster Name)
CLUSTER_NAME = MyDediServer
# Klei 存档根目录 (通常是 ~/.klei/DoNotStarveTogether)
KLEI_HOME = ~/.klei/DoNotStarveTogether
```

### File: docs/guides/DEV_GUIDE.md
- mode: head
- size_bytes: 11820
- sha256_12: 9bdf3a820c53

```md
# Wagstaff-Lab 开发规范 (v4.0.0-dev)

本指南用于约束核心架构边界与开发协作方式，确保可维护与可扩展。

## 0. 快速必读（人/LLM）

- **本文件是强制入口**：任何架构/入口/产物变更都必须先对照并更新本文件。
- **执行顺序建议**：`README.md` → `DEV_GUIDE.md` → `PROJECT_MANAGEMENT.md` → `CATALOG_V2_SPEC.md`。
- **规范优先级**：若文档冲突，以 `DEV_GUIDE.md` 为准。

### DEV_GUIDE_META

```yaml
dev_guide:
  version: v4.0.0-dev
  must_update_on:
    - 架构/目录调整
    - CLI/Web/Server 入口变更
    - 索引/产物结构变更
    - 依赖/构建流程变更
  entrypoints:
    - wagstaff
    - make
  management:
    - docs/management/PROJECT_MANAGEMENT.md
    - PROJECT_STATUS.json
```

## 1. 分层职责

- `core/`：解析、索引、算法与数据模型。不得依赖 `apps/` 或 `devtools/`。
- `core/lua/`：Lua 解析基元（scan/split/match/expr/call）。
- `core/parsers/`：Prefab/Loot/Cooking/String 等领域解析器。
- `core/schemas/`：核心数据结构与元信息规范（仅类型/结构，不含流程）。
- `core/indexers/`：索引构建逻辑（依赖 `core/` 但不触碰上层）。
- `apps/cli/`：CLI 交互层。调用 `core/`，仅做输入/输出组织。
- `apps/cli/commands/`：CLI 子命令实现（dashboard/doctor/wiki/explorer 等）。
- `apps/server/`：服务器运维（DST 运行/备份/恢复），与数据分析解耦。
- `apps/webcraft/`：WebCraft 服务层（API + UI）。只通过索引产物与 `core/` 暴露的能力。
- `devtools/`：构建、报表、快照等流程工具。
- `devtools/validators.py`：质量/校验库（无独立 CLI 入口），由质量门禁与机制索引调用。
- `devtools/report_hub.py`：报告中心（build/list/open），统一汇总输出与 manifest。
- `devtools/portal_hub.py`：聚合视图（管理 + 报告 + 质量）。
- `data/`：所有产物、报告、索引、静态资源的统一落盘目录。
- CLI 角色与职责见 `docs/guides/CLI_GUIDE.md`。

## 2. 依赖与导入约定

- 依赖方向：`apps/`、`devtools/` -> `core/`。
- 入口脚本只挂载项目根目录到 `sys.path`，通过 `core.*` / `apps.*` / `devtools.*` 进行导入。
- `core/` 不得自行修改 `sys.path`。
- 任何跨层访问，优先通过 `core` 的稳定 API，而非直接读文件或 copy 逻辑。

## 3. 包管理 (pyproject.toml)

- 依赖统一由 `pyproject.toml` 管理，禁止散落在脚本内。
- 采用可选依赖分组：`cli` / `web` / `icons` / `quality` / `all`。
- 入口注册统一通过：`python -m pip install -e ".[cli]"`（需要完整能力时用 `.[all]`）。

## 4. 数据产物与命名

- 统一落盘到 `data/`，并带版本号后缀，例如：
  - `data/index/wagstaff_resource_index_v1.json`
  - `data/index/wagstaff_catalog_v2.json`
  - `data/index/wagstaff_catalog_v2.sqlite`
  - `data/index/wagstaff_catalog_index_v1.json`
  - `data/index/wagstaff_icon_index_v1.json`
  - `data/index/wagstaff_i18n_v1.json`
  - `data/index/wagstaff_tuning_trace_v1.json`
  - `data/index/wagstaff_farming_defs_v1.json`
  - `data/index/wagstaff_mechanism_index_v1.json`
  - `data/index/wagstaff_mechanism_index_v1.sqlite`
  - `data/index/wagstaff_index_manifest.json`
  - `data/reports/quality_gate_report.md`
  - `data/reports/catalog_quality_report.md`
  - `data/reports/dst_raw_coverage.md`
  - `data/reports/mechanism_index_summary.md`
  - `data/reports/mechanism_crosscheck_report.md`
  - `data/reports/wagstaff_report_manifest.json`
  - `data/reports/index.html`
  - `data/reports/portal_index.html`
- 产物默认不入库：`data/index/` 与 `data/static/icons/` 由工具生成，需要时用 `make catalog` / `make icons` 重建。
- 产物需携带统一元信息（schema / generated / tool / sources / scripts hash），SQLite 额外记录 `db_schema_version`。
- SQLite v4 结构定义见 `docs/specs/SQLITE_V4_SPEC.md`，catalog/mechanism 优先遵循该表结构。
- 索引清单由 `devtools/build_index_manifest.py` 汇总，包含每个产物的 schema 与版本。
- 统一版本入口为 `conf/version.json`：`project_version` / `index_version` 写入各索引 `meta`，文件名的 `v1/v2` 仅代表 `schema_version`。
- WebCraft UI 不应直接读取原始脚本或 datastream，仅消费 `data/index` 等稳定产物。
- Catalog v2 产物新增 `cooking_ingredients` 字段用于料理食材 tags 索引。
- Mechanism index 的 JSON schema 见 `docs/specs/mechanism_index_v1.schema.json`。

## 5. WebCraft 约定

- API 统一在 `/api/v1` 下，UI 与 API 使用同一 `root_path`。
- UI 路由固定：`/`(Cooking 模拟主页)、`/craft`(Craft 图鉴)、`/cooking`(Cooking 图鉴)、`/cooking/explore`、`/cooking/simulate`、`/catalog`。
- UI 仅通过 API 访问数据；静态资源来自 `data/static/`。
- 新增字段须保证向后兼容或同步更新 `schema_version`。
- WebCraft 运行时优先使用 `data/index/wagstaff_catalog_v2.sqlite`，缺失时回退 JSON。
- i18n 仅使用 `data/index/wagstaff_i18n_v1.json`（运行时不解析 PO）。
- WebCraft 应用静态资源（CSS/字体等）放在 `apps/webcraft/static/`，对外挂载为 `/static/app`；数据产物静态资源（如图标）继续落盘 `data/static/` 并对外挂载 `/static/data`。

## 5.1 UI 设计规范

- 任何 UI 设计/重构任务默认使用 `frontend-design` skill 产出方案与代码。

## 5.2 i18n 与文本规范

- **ID 优先**：代码层/数据层以游戏内 ID（prefab/item id）为唯一主键与引用标准。
- **英文为默认语义**：英文是 UI fallback 与默认显示语言；中文仅作为 i18n 映射，方便扩展更多语言。
- **统一 i18n 链路**：
  - UI 文本一律使用 `t('key', 'English fallback')`，不得硬编码中文/英文。
  - UI 字符串来源：`conf/i18n_ui.json` → `devtools/build_i18n_index.py` → `data/index/wagstaff_i18n_v1.json`。
  - 新增/调整 UI 文本必须补齐 `conf/i18n_ui.json` 并执行 `make i18n`。
- **名称映射**：前端展示名称使用 `/api/v1/i18n/names/{lang}`，禁止额外本地映射或重复维护。
- **标签映射**：`conf/i18n_tags.json` 维护 cooking tags，多语言条目必须附带 `source`（`game`/`manual`/`virtual`），优先对齐游戏内资源；前端使用 `/api/v1/i18n/tags/{lang}` 获取标签文本（`tags_meta` 不用于展示）。
- **ID 模式展示**：当 UI 使用 `id` 模式时，标签文本需附带 `tags_meta` 的 `source` 信息，用于标识翻译来源。
- **流程收敛**：去除分散的字符串表或临时翻译逻辑，确保同一 key 在全站复用。

## 5.3 WebCraft UI 模块化规范

- `apps/webcraft/ui.py` 仅保留模板渲染与变量注入，禁止大段内联 CSS/JS。
- UI 静态资源统一落盘 `apps/webcraft/static/`，目录规范：
  - `static/css/`：样式层（base + page）。
  - `static/js/core/`：共享工具与基础能力（api/dom/i18n/icons）。
  - `static/js/pages/`：页面入口（catalog/craft/cooking），不得互相依赖。
  - `static/js/app.js`：统一启动脚本，按 `body` 标记加载页面模块。
- 模板目录统一为 `apps/webcraft/templates/`（index/catalog/cooking）。
- 共享 UI 片段必须收敛到 `core/`，避免跨页函数缺失与重复定义。
- 大型 UI 重构必须先产出计划文档（`docs/management/*.md`），并在 `PROJECT_STATUS.json` 记录。

## 5.4 Mobile App Shell 规范

- 移动端 App Shell 通过 `body[data-app-shell="1"]` 启用，桌面端不受影响。
- 模板加入 `<nav class="app-nav">` 底部导航（`appNavCraft/appNavCooking/appNavCatalog`），页面 JS 负责 href 与 active。
- 通用样式位于 `apps/webcraft/static/css/base.css`，页面差异样式放在各自 page CSS。

## 6. 变更与文档

- 重要重构必须同步更新：
  - `README.md`
  - `docs/guides/DEV_GUIDE.md`
  - `PROJECT_STATUS.json`
  - `docs/management/PROJECT_MANAGEMENT.md`
  - `docs/` 相关文档
- 所有结构调整需记录到 `RECENT_LOGS`。

## 6.1 变更检查清单（强制）

- [ ] DEV_GUIDE 是否需要更新（架构/入口/产物/依赖）
- [ ] PROJECT_MANAGEMENT 是否需要更新（里程碑/任务）
- [ ] PROJECT_STATUS 是否同步（RECENT_LOGS/任务）
- [ ] 文档入口是否一致（README / SPEC / ROADMAP）

## 6.2 Git 流程（强制）

- 分支命名：`feat/` `fix/` `refactor/` `docs/` `chore/` `ui/`
- 提交信息：`type(scope): summary`（示例：`feat(cooking): add explore scoring`）
- 任何 `push`（含本地）必须满足：
  - `git status` 为 clean
  - `git pull --rebase` 同步最新
  - 完成与改动相关的自检（参考 6.1 检查清单）
- 禁止对共享分支强推（`--force`/`--force-with-lease`）
- 收尾（强制）：
  - `git diff --stat` 自检改动范围，确认未误入 `data/index`/`data/static/icons`
  - `PROJECT_STATUS.json` 追加 `RECENT_LOGS`（必要时同步 README/管理文档）
  - `git add -A` → `git commit -m "type(scope): summary"` → `git status` clean

## 7. Snapshot 友好开发规范 (面向后续开发)

- 公开接口集中在少数入口文件（例如 `core/engine.py`、`apps/*/app.py`），避免散落式 API。
- 模块顶部写清楚职责与输入/输出约束，复杂模块必须有模块级 docstring。
- 对外数据结构必须有字段说明（注释或类型定义），避免“隐式字段”。
- 重要函数保持稳定签名，新增参数必须给默认值并写清变更意图。
- 新增索引/产物必须落盘 `data/` 并记录 schema_version 与生成来源。
- 关键流程写最小示例（1-3 行 usage），便于快照直接引用。

## 8. LLM 快照与文档导出 (snapshot.py)

- `devtools/snapshot.py` 是统一的 LLM 友好导出工具；`wagstaff snap` 默认使用 llm 模板输出 `project_context.txt`。
- 模板集中在 `conf/snapshot_templates.json`，通过 `sections` 控制 env/tree/inventory/contents/stats 等模块输出。
- 聚焦导出使用 `--focus path|glob`（可多次传入），默认仍保留 `README.md`/`PROJECT_STATUS.json` 作为上下文。
- 若需更清爽输出，可用 `--no-tree`/`--no-inventory`/`--no-contents` 等开关精简。
- 重要重构必须同步更新 `PROJECT_STATUS.json`/`README.md`，确保快照上下文准确。

## 9. 任务入口 (Makefile)

- `make all`：resindex + catalog + catalog-index + i18n + farming-defs + quality
- `make resindex` / `make catalog` / `make catalog-index`
- `make catalog-sqlite`
- `make i18n` / `make farming-defs` / `make mechanism-index` / `make icons`
- `make index-manifest`
- `make quality`（运行质量门禁）
- `make webcraft` / `make snap`

## 9.1 增量构建 (devtools)

- `build_resource_index.py` / `build_farming_defs.py` / `build_mechanism_index.py` / `build_icons.py` / `build_catalog_v2.py` / `build_catalog_sqlite.py` 默认走增量缓存，缓存落盘 `data/index/.build_cache.json`。
- `build_mechanism_index.py` 依赖 `scripts` 与 `resource_index` 签名，JSON 与 SQLite 输出统一比对。
- `build_mechanism_index.py --strict` 会在任意校验告警时返回非零。
- `build_mechanism_index.py validate` 用于校验机制索引 JSON 结构与关键字段。
- `build_mechanism_index.py diff` 用于对比两份机制索引的增量变化。
- 需强制全量重建时，追加 `--force`。

## 10. 最低自检清单

- `wagstaff dash` (主界面可运行)
- `python devtools/build_catalog_v2.py --silent`
- `python devtools/quality_gate.py` (信息提示，CI 可加 --enforce/--strict)
- `python devtools/report_hub.py list`
- `python devtools/portal_hub.py list`
- `python devtools/snapshot.py --mode llm --plan` (快照计划可生成)
- `python devtools/serve_webcraft.py --help` (需要 uvicorn)
```

### File: docs/management/ROADMAP.md
- mode: head
- size_bytes: 2099
- sha256_12: 40bb5fc18729

```md
# Wagstaff-Lab 版本演进方向 (v4+)

本文件仅保留**长期方向**，执行计划与进度请统一查看：
- `docs/management/PROJECT_MANAGEMENT.md`
vNext 破兼容重构规划见：
- `docs/management/VNEXT_REFACTOR_PLAN.md`

## 1. 架构与工程化

- 模块化与包化：逐步引入 `pyproject.toml`，形成可安装的内核与应用包。
- 入口统一：提供明确的 CLI/Web 启动入口与配置模板。
- 插件接口：定义解析器/索引器的扩展协议，降低新增功能成本。

## 2. 数据层演进

- Catalog schema v2：更多字段归一化（prefab、asset、tuning trace）。
- 增量构建：对比 scripts hash，支持局部重建与 cache reuse。
- 存储升级：从 JSON 过渡到 SQLite/Parquet（大规模检索性能）。
- SQLite v4 结构：统一 db_schema_version 与表分组设计（见 `docs/specs/SQLITE_V4_SPEC.md`）。

## 3. WebCraft 体验

- 多维检索：标签、来源、制作链路、组件属性联动检索。
- 探索/模拟：料理食材驱动探索、配方模拟与发现流。
- 食材索引：解析 ingredients/cooking 定义，提供 cooking_ingredients tags 数据契约。
- 信息密度：表格/列表切换，中英文/调试 ID 同屏，快捷复制。
- 结果解释：TUNING 解析链路可视化、配方链路图与条件展示。
- 体验与性能：统一页面结构、键盘/移动端优化、静态资源本地化与缓存。

## 4. CLI 与工具链

- CLI 统一输出规范（JSON/表格/纯文本）。
- devtools 统一日志与报告产出格式，便于对比与自动化。
- 建立脚手架：新工具生成器（模板 + 注册）。

## 5. 质量与可观测

- 核心功能最小测试集：解析器、索引器、Web API 合同测试。
- 产物验证：索引一致性检查（assets/recipes/cooking）。
- 运行指标：构建时间、产物规模、缺失率统计。

## 6. 生态与协作

- 明确贡献指南与代码风格约定。
- 对外文档与示例数据集（便于复现与扩展）。
- 分离实验区与稳定区（feature branch/experimental modules）。
```

### File: apps/cli/main.py
- mode: full
- size_bytes: 1747
- sha256_12: 1f13f3912194

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Unified CLI dispatcher for Wagstaff-Lab."""

from __future__ import annotations

import runpy
import sys
from pathlib import Path
from typing import List, Optional, Tuple

PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))


def _tool_path(tool: dict) -> Path:
    folder = tool.get("folder") or "apps/cli"
    if folder == "apps/cli":
        base = PROJECT_ROOT / "apps" / "cli"
    elif folder == "devtools":
        base = PROJECT_ROOT / "devtools"
    else:
        base = PROJECT_ROOT / folder
    return base / str(tool.get("file"))


def _resolve_tool(alias: Optional[str]) -> Tuple[Path, List[str]]:
    from apps.cli.registry import get_tools

    tools = get_tools()
    dash = next((t for t in tools if t.get("alias") == "dash"), None)
    default_path = _tool_path(dash) if dash else (PROJECT_ROOT / "apps" / "cli" / "commands" / "dash.py")

    if not alias:
        return default_path, []

    key = str(alias).strip()
    if not key:
        return default_path, []

    for tool in tools:
        if tool.get("alias") == key or tool.get("file") == key:
            return _tool_path(tool), []

    # fallback: show dashboard, pass through as arg to help locate typos
    return default_path, [key]


def main(argv: Optional[List[str]] = None) -> None:
    argv = list(argv) if argv is not None else list(sys.argv[1:])
    alias = argv[0] if argv else None
    path, injected = _resolve_tool(alias)

    if argv and alias:
        argv = argv[1:]
    argv = injected + argv

    sys.argv = [str(path)] + argv
    runpy.run_path(str(path), run_name="__main__")


if __name__ == "__main__":
    main()
```

### File: apps/cli/registry.py
- mode: full
- size_bytes: 5733
- sha256_12: bf4db21c5938

```py
#!/usr/bin/env python3
"""Wagstaff-Lab 工具注册中心."""

TOOLS = [
    # --- CLI 工具 (apps/cli) ---
    {
        "file": "dash.py",
        "alias": "dash",
        "desc": "Wagstaff-Lab 控制台主面板",
        "usage": "wagstaff dash",
        "type": "CLI",
        "folder": "apps/cli/commands"
    },
    {
        "file": "doctor.py",
        "alias": "doctor",
        "desc": "环境配置与依赖健康检查",
        "usage": "wagstaff doctor",
        "type": "CLI",
        "folder": "apps/cli/commands"
    },
    {
        "file": "wiki.py",
        "alias": "wiki",
        "desc": "物品/配方/数值查询百科",
        "usage": "wagstaff wiki <item_code>",
        "type": "CLI",
        "folder": "apps/cli/commands"
    },
    {
        "file": "explorer.py",
        "alias": "exp",
        "desc": "源码结构浏览与深度分析",
        "usage": "wagstaff exp",
        "type": "CLI",
        "folder": "apps/cli/commands"
    },
    {
        "file": "mgmt.py",
        "alias": "mgmt",
        "desc": "项目管理：状态展示与同步",
        "usage": "wagstaff mgmt <status|sync|dump|check>",
        "type": "CLI",
        "folder": "apps/cli/commands"
    },
    {
        "file": "server.py",
        "alias": "server",
        "desc": "DST 服务器管理 (screen-based)",
        "usage": "wagstaff server <status|start|stop|restart|update|backup|restore|logs|cmd|ui>",
        "type": "CLI",
        "folder": "apps/cli/commands"
    },

    # --- 开发工具 (devtools/) ---
    {
        "file": "report_hub.py",
        "alias": "report",
        "desc": "报告中心：build/list/open",
        "usage": "wagstaff report [build|list|open] [--all] [--stats-gap]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "portal_hub.py",
        "alias": "portal",
        "desc": "聚合视图：管理 + 报告 + 质量",
        "usage": "wagstaff portal [build|list|open]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "build_catalog_v2.py",
        "alias": "catalog2",
        "desc": "生成 Catalog v2 (item-centric)",
        "usage": "wagstaff catalog2 [--dst-root PATH] [--tuning-mode value_only|full] [--tuning-trace-out PATH]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "build_catalog_sqlite.py",
        "alias": "catalog-sqlite",
        "desc": "生成 Catalog SQLite v4",
        "usage": "wagstaff catalog-sqlite [--catalog PATH] [--out PATH] [--tuning-trace PATH]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "build_i18n_index.py",
        "alias": "i18n",
        "desc": "生成 i18n 索引 (names + UI strings)",
        "usage": "wagstaff i18n [--lang zh] [--dst-root PATH]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "build_farming_defs.py",
        "alias": "farming-defs",
        "desc": "生成耕种机制索引 (farming defs)",
        "usage": "wagstaff farming-defs [--dst-root PATH]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "build_icons.py",
        "alias": "icons",
        "desc": "生成物品图标 PNG + icon index",
        "usage": "wagstaff icons [--dst-root PATH] [--all-elements]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "build_catalog_index.py",
        "alias": "catindex",
        "desc": "生成 Catalog 紧凑索引（列表 + 多维倒排）",
        "usage": "wagstaff catindex [--catalog PATH] [--icon-index PATH]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "build_mechanism_index.py",
        "alias": "mechanism-index",
        "desc": "生成机制索引（组件 + prefab 关系）",
        "usage": "wagstaff mechanism-index [build|validate|diff] ...",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "build_index_manifest.py",
        "alias": "index-manifest",
        "desc": "生成索引版本清单 (manifest)",
        "usage": "wagstaff index-manifest [--out PATH]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "quality_gate.py",
        "alias": "quality",
        "desc": "质量门禁自检（默认仅提示）",
        "usage": "wagstaff quality [--enforce] [--strict]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "snapshot.py",
        "alias": "snap",
        "desc": "生成 LLM 友好代码快照",
        "usage": "wagstaff snap [-h] [--mode {llm,core,archive,custom}] [--template TEMPLATE] [--config CONFIG] [--output OUTPUT] [--focus PATH|GLOB ...] [--list-templates] [--no-redact] [--zip] [--no-tree] [--no-inventory] [--no-contents] [--no-stats] [--verbose] [--plan]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "sampler.py",
        "alias": "samples",
        "desc": "生成 DST Lua 样本包（用于扩展解析器）",
        "usage": "wagstaff samples [--categories ...] [--n N] [--head-lines N] ...",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "build_resource_index.py",
        "alias": "resindex",
        "desc": "生成 DST 资源索引（scripts + data）",
        "usage": "wagstaff resindex [--data-full] [--bundle-full] [--dst-root PATH]",
        "type": "Dev",
        "folder": "devtools"
    },
    {
        "file": "serve_webcraft.py",
        "alias": "web",
        "desc": "启动 WebCraft (FastAPI + Uvicorn)",
        "usage": "wagstaff web [--host 0.0.0.0 --port 20000]",
        "type": "Dev",
        "folder": "devtools"
    },
]

def get_tools():
    return TOOLS
```

### File: apps/__init__.py
- mode: full
- size_bytes: 52
- sha256_12: 358b6db47998

```py
# -*- coding: utf-8 -*-
"""Applications package."""
```

### File: apps/cli/__init__.py
- mode: full
- size_bytes: 43
- sha256_12: f8d7e7942c05

```py
# -*- coding: utf-8 -*-
"""CLI package."""
```

### File: apps/cli/cli_common.py
- mode: full
- size_bytes: 1915
- sha256_12: 3ee63244bc5c

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Shared helpers for CLI tools."""

from __future__ import annotations

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

PROJECT_ROOT = Path(__file__).resolve().parents[2]
DATA_DIR = PROJECT_ROOT / "data"
INDEX_DIR = DATA_DIR / "index"
REPORT_DIR = DATA_DIR / "reports"
CONF_DIR = PROJECT_ROOT / "conf"


def read_json(path: Path) -> Optional[Dict[str, Any]]:
    if not path.exists():
        return None
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return None


def read_text(path: Path) -> Optional[str]:
    if not path.exists():
        return None
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return None


def load_status() -> Dict[str, Any]:
    status_path = PROJECT_ROOT / "PROJECT_STATUS.json"
    doc = read_json(status_path)
    return doc if isinstance(doc, dict) else {}


def file_info(path: Path) -> Dict[str, Any]:
    if not path.exists():
        return {"exists": False, "size": 0, "mtime": None}
    st = path.stat()
    return {"exists": True, "size": int(st.st_size), "mtime": float(st.st_mtime)}


def human_size(num: int) -> str:
    if num <= 0:
        return "-"
    for unit in ("B", "KiB", "MiB", "GiB"):
        if num < 1024.0:
            return f"{num:.0f} {unit}" if unit == "B" else f"{num:.1f} {unit}"
        num /= 1024.0
    return f"{num:.1f} TiB"


def human_mtime(ts: Optional[float]) -> str:
    if not ts:
        return "-"
    return datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M")


def env_hint() -> Tuple[str, str]:
    env = os.environ.get("CONDA_DEFAULT_ENV", "").strip()
    if env:
        return env, "conda"
    venv = os.environ.get("VIRTUAL_ENV", "").strip()
    if venv:
        return venv, "venv"
    return "system", "system"
```

### File: apps/cli/commands/__init__.py
- mode: full
- size_bytes: 51
- sha256_12: 6ea567813f48

```py
# -*- coding: utf-8 -*-
"""CLI command modules."""
```

### File: apps/cli/commands/dash.py
- mode: full
- size_bytes: 11582
- sha256_12: 5b127fa90c44

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from apps.cli.cli_common import (
    DATA_DIR,
    INDEX_DIR,
    REPORT_DIR,
    PROJECT_ROOT,
    env_hint,
    file_info,
    human_mtime,
    human_size,
    load_status,
    read_json,
)
from core.version import project_version
from apps.cli.registry import get_tools

console = Console()


def _artifact_rows() -> List[Tuple[str, Path]]:
    return [
        ("resource_index", INDEX_DIR / "wagstaff_resource_index_v1.json"),
        ("catalog_v2", INDEX_DIR / "wagstaff_catalog_v2.json"),
        ("catalog_sqlite", INDEX_DIR / "wagstaff_catalog_v2.sqlite"),
        ("catalog_index", INDEX_DIR / "wagstaff_catalog_index_v1.json"),
        ("icon_index", INDEX_DIR / "wagstaff_icon_index_v1.json"),
        ("i18n_index", INDEX_DIR / "wagstaff_i18n_v1.json"),
        ("tuning_trace", INDEX_DIR / "wagstaff_tuning_trace_v1.json"),
        ("farming_defs", INDEX_DIR / "wagstaff_farming_defs_v1.json"),
        ("mechanism_index", INDEX_DIR / "wagstaff_mechanism_index_v1.json"),
        ("mechanism_sqlite", INDEX_DIR / "wagstaff_mechanism_index_v1.sqlite"),
        ("index_manifest", INDEX_DIR / "wagstaff_index_manifest.json"),
    ]


def _render_artifacts() -> None:
    table = Table(title="Artifacts", box=None, show_header=True, header_style="bold cyan")
    table.add_column("Name", style="bold")
    table.add_column("Status")
    table.add_column("Updated", style="dim")
    table.add_column("Size", style="green")
    table.add_column("Path", style="dim")

    for name, path in _artifact_rows():
        info = file_info(path)
        status = "[green]OK[/green]" if info["exists"] else "[red]MISSING[/red]"
        table.add_row(
            name,
            status,
            human_mtime(info["mtime"]),
            human_size(info["size"]),
            str(path.relative_to(PROJECT_ROOT)),
        )
    console.print(table)


def _render_quality() -> None:
    qpath = REPORT_DIR / "quality_gate_report.json"
    doc = read_json(qpath) or {}
    summary = doc.get("summary") if isinstance(doc, dict) else None
    summary = summary if isinstance(summary, dict) else {}

    table = Table(title="Quality Snapshot", box=None, show_header=False)
    table.add_column("Key", style="bold")
    table.add_column("Value")

    issues_total = summary.get("issues_total")
    issues_fail = summary.get("issues_fail")
    issues_warn = summary.get("issues_warn")
    table.add_row("issues", f"total={issues_total} fail={issues_fail} warn={issues_warn}")

    items_total = summary.get("catalog_items_total")
    items_stats = summary.get("catalog_items_with_stats")
    stats_ratio = summary.get("catalog_stats_ratio")
    if items_total is not None:
        ratio_str = f"{float(stats_ratio):.1%}" if isinstance(stats_ratio, (int, float)) else "-"
        table.add_row("catalog", f"items={items_total} stats={items_stats} ({ratio_str})")

    trace_items = summary.get("trace_items")
    trace_cooking = summary.get("trace_cooking")
    if trace_items is not None:
        table.add_row("tuning_trace", f"items={trace_items} cooking={trace_cooking}")

    i18n_cov = summary.get("i18n_coverage") if isinstance(summary.get("i18n_coverage"), dict) else {}
    for lang, row in (i18n_cov or {}).items():
        try:
            ratio = row.get("ratio")
            ratio_str = f"{float(ratio):.1%}" if isinstance(ratio, (int, float)) else "-"
            table.add_row(f"i18n:{lang}", f"names={row.get('names')} ({ratio_str})")
        except Exception:
            continue

    mech_components = summary.get("mechanism_components_total")
    mech_prefabs = summary.get("mechanism_prefabs_total")
    if mech_components is not None:
        table.add_row("mechanism", f"components={mech_components} prefabs={mech_prefabs}")

    sqlite_catalog = summary.get("sqlite_catalog_db_schema_version")
    sqlite_mechanism = summary.get("sqlite_mechanism_db_schema_version")
    if sqlite_catalog or sqlite_mechanism:
        table.add_row("sqlite_v4", f"catalog={sqlite_catalog} mechanism={sqlite_mechanism}")

    console.print(table)


def _render_tasks(status: Dict[str, Any]) -> None:
    todo = status.get("TASKS_TODO") or []
    done = status.get("TASKS_DONE") or []
    logs = status.get("RECENT_LOGS") or []

    focus = str(todo[0]) if todo else "-"
    table = Table(title="Tasks", box=None, show_header=False)
    table.add_column("Key", style="bold")
    table.add_column("Value")
    table.add_row("todo", str(len(todo)))
    table.add_row("done", str(len(done)))
    table.add_row("focus", focus)
    console.print(table)

    if todo:
        console.print("[bold yellow]Active Tasks (top 8)[/bold yellow]")
        for t in todo[:8]:
            console.print(f"- {t}")
    if done:
        console.print("\n[bold green]Done (recent 5)[/bold green]")
        for t in done[-5:]:
            console.print(f"- {t}")
    if logs:
        console.print("\n[bold cyan]Recent Logs (last 6)[/bold cyan]")
        for line in logs[-6:]:
            console.print(f"- {line}")


def _render_docs() -> None:
    table = Table(title="Docs", box=None, show_header=True, header_style="bold cyan")
    table.add_column("Name", style="bold")
    table.add_column("Updated", style="dim")
    table.add_column("Path", style="dim")

    docs = [
        ("DEV_GUIDE", PROJECT_ROOT / "docs" / "guides" / "DEV_GUIDE.md"),
        ("CLI_GUIDE", PROJECT_ROOT / "docs" / "guides" / "CLI_GUIDE.md"),
        ("PROJECT_MANAGEMENT", PROJECT_ROOT / "docs" / "management" / "PROJECT_MANAGEMENT.md"),
        ("CATALOG_V2_SPEC", PROJECT_ROOT / "docs" / "specs" / "CATALOG_V2_SPEC.md"),
        ("MECHANISM_INDEX_SPEC", PROJECT_ROOT / "docs" / "specs" / "MECHANISM_INDEX_SPEC.md"),
        ("SQLITE_V4_SPEC", PROJECT_ROOT / "docs" / "specs" / "SQLITE_V4_SPEC.md"),
        ("ROADMAP", PROJECT_ROOT / "docs" / "management" / "ROADMAP.md"),
    ]

    for name, path in docs:
        info = file_info(path)
        table.add_row(name, human_mtime(info["mtime"]), str(path.relative_to(PROJECT_ROOT)))
    console.print(table)


def _status_badge(status: str) -> str:
    if status == "ok":
        return "[green]OK[/green]"
    if status == "partial":
        return "[yellow]PARTIAL[/yellow]"
    if status == "missing":
        return "[red]MISSING[/red]"
    if status == "skipped":
        return "[dim]SKIPPED[/dim]"
    return status or "-"


def _render_reports() -> None:
    manifest_path = REPORT_DIR / "wagstaff_report_manifest.json"
    manifest = read_json(manifest_path) or {}
    counts = manifest.get("counts") if isinstance(manifest, dict) else None
    counts = counts if isinstance(counts, dict) else {}

    summary = Table(title="Reports Summary", box=None, show_header=False)
    summary.add_column("Key", style="bold")
    summary.add_column("Value")
    summary.add_row("reports", str(counts.get("reports", 0)))
    summary.add_row("missing", str(counts.get("missing", 0)))
    summary.add_row("partial", str(counts.get("partial", 0)))
    summary.add_row("manifest", str(manifest_path.relative_to(PROJECT_ROOT)))
    console.print(summary)

    reports = manifest.get("reports") if isinstance(manifest, dict) else None
    reports = reports if isinstance(reports, list) else []
    table = Table(title="Report Inventory", box=None, show_header=True, header_style="bold cyan")
    table.add_column("Report", style="bold")
    table.add_column("Status")
    table.add_column("Updated", style="dim")
    table.add_column("Files", style="green")

    for rep in reports:
        if not isinstance(rep, dict):
            continue
        status = rep.get("status") or "-"
        updated = rep.get("updated")
        files = rep.get("files") if isinstance(rep.get("files"), list) else []
        names = ", ".join(Path(str(f.get("path"))).name for f in files if isinstance(f, dict))
        table.add_row(
            str(rep.get("title") or "-"),
            _status_badge(str(status)),
            human_mtime(float(updated)) if updated else "-",
            names or "-",
        )
    console.print(table)

    portal_path = REPORT_DIR / "portal_index.html"
    portal_info = file_info(portal_path)
    portal_row = Table(title="Report UI", box=None, show_header=False)
    portal_row.add_column("Key", style="bold")
    portal_row.add_column("Value")
    portal_row.add_row("report_hub", str((REPORT_DIR / "index.html").relative_to(PROJECT_ROOT)))
    portal_row.add_row("portal", str(portal_path.relative_to(PROJECT_ROOT)))
    portal_row.add_row("portal_updated", human_mtime(portal_info["mtime"]))
    console.print(portal_row)


def _render_tools() -> None:
    tools = get_tools()
    order = [
        "Entry",
        "Health",
        "Query",
        "Explore",
        "Mgmt",
        "Build",
        "Quality",
        "Reports",
        "Ops",
        "Server",
        "Utility",
        "Other",
    ]
    mapping = {
        "Entry": {"dash"},
        "Health": {"doctor"},
        "Query": {"wiki"},
        "Explore": {"exp"},
        "Mgmt": {"mgmt"},
        "Build": {
            "resindex",
            "catalog2",
            "catalog-sqlite",
            "catindex",
            "i18n",
            "icons",
            "farming-defs",
            "mechanism-index",
            "index-manifest",
        },
        "Quality": {"quality"},
        "Reports": {"report", "portal"},
        "Ops": {"web"},
        "Server": {"server"},
        "Utility": {"snap", "samples"},
    }

    grouped: Dict[str, List[Dict[str, Any]]] = {k: [] for k in order}
    for tool in tools:
        alias = tool.get("alias") or tool.get("file") or ""
        bucket = "Other"
        for label, names in mapping.items():
            if alias in names:
                bucket = label
                break
        grouped[bucket].append(tool)

    for label in order:
        rows = grouped.get(label) or []
        if not rows:
            continue
        table = Table(title=f"Commands · {label}", box=None, show_header=True, header_style="bold cyan")
        table.add_column("Command", style="bold")
        table.add_column("Desc")
        table.add_column("Usage", style="green")
        for tool in rows:
            name = tool.get("alias") or tool.get("file") or "-"
            cmd = f"wagstaff {name}" if tool.get("alias") else "wagstaff"
            table.add_row(cmd, tool.get("desc", "-"), tool.get("usage", "-"))
        console.print(table)


def main() -> None:
    status = load_status()
    objective = status.get("OBJECTIVE") or status.get("objective") or "-"
    env_name, env_kind = env_hint()
    ver = project_version()

    header = f"[bold white on blue] Wagstaff-Lab Dashboard ({ver}) [/bold white on blue]"
    console.print(Panel(header, border_style="blue"))
    console.print(f"[bold green]Objective:[/bold green] {objective}")
    console.print(f"[dim]Root: {PROJECT_ROOT} | Env: {env_name} ({env_kind}) | Data: {DATA_DIR}[/dim]")

    console.print("")
    _render_tasks(status)
    console.print("")
    _render_quality()
    console.print("")
    _render_reports()
    console.print("")
    _render_docs()
    console.print("")
    _render_artifacts()
    console.print("")
    _render_tools()
    console.print("\n[dim]Tips: wagstaff quality | wagstaff report build --quality | wagstaff catindex | wagstaff snap[/dim]")


if __name__ == "__main__":
    main()
```

### File: apps/cli/commands/doctor.py
- mode: full
- size_bytes: 6232
- sha256_12: 9f344e461317

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from __future__ import annotations

import argparse
import configparser
import os
import shutil
import subprocess
from pathlib import Path

from rich.console import Console
from rich.panel import Panel
from rich.table import Table

from apps.cli.cli_common import (
    CONF_DIR,
    INDEX_DIR,
    PROJECT_ROOT,
    env_hint,
    file_info,
    human_mtime,
    human_size,
)

console = Console()
CONFIG_PATH = CONF_DIR / "settings.ini"


def _expand(p: str) -> str:
    return os.path.expanduser(p.strip())


def _cfg_get(cfg: configparser.ConfigParser, section: str, key: str) -> str:
    try:
        v = cfg.get(section, key, fallback="").strip()
    except Exception:
        v = ""
    return _expand(v) if v else ""


def _status(level: str) -> str:
    if level == "PASS":
        return "[green]PASS[/green]"
    if level == "WARN":
        return "[yellow]WARN[/yellow]"
    return "[red]FAIL[/red]"


def _check_path_exists(path: Path, kind: str, fix: str = ""):
    if kind == "file":
        ok = path.is_file()
    elif kind == "dir":
        ok = path.is_dir()
    else:
        ok = path.exists()
    level = "PASS" if ok else "WARN"
    return ok, level, str(path), fix


def main() -> int:
    p = argparse.ArgumentParser(description="Wagstaff Doctor (environment + data health check)")
    p.add_argument("--enforce", action="store_true", help="exit non-zero on failures (CI)")
    p.add_argument("--strict", action="store_true", help="treat WARN as FAIL (only when --enforce)")
    args = p.parse_args()

    env_name, env_kind = env_hint()
    console.print(Panel(f"[bold cyan]Wagstaff Doctor[/bold cyan]\nEnv: {env_name} ({env_kind})", border_style="cyan"))

    table = Table(title="Health Checks", box=None, show_header=True, header_style="bold cyan")
    table.add_column("Check", style="bold")
    table.add_column("Status", justify="center")
    table.add_column("Details", style="dim")
    table.add_column("Fix Hint", style="green")

    fail = 0
    warn = 0

    # 1) config file (optional)
    ok, level, details, fix = _check_path_exists(CONFIG_PATH, "file", "Optional: configure conf/settings.ini to enable DST path checks")
    table.add_row("conf/settings.ini", _status(level), details, fix if not ok else "")
    if not ok:
        warn += 1

    cfg = configparser.ConfigParser()
    if ok:
        try:
            cfg.read(CONFIG_PATH)
        except Exception as e:
            table.add_row("parse settings.ini", _status("WARN"), str(e), "Check ini format")
            warn += 1

    dst_root = _cfg_get(cfg, "PATHS", "DST_ROOT") if ok else ""
    steamcmd_dir = _cfg_get(cfg, "PATHS", "STEAMCMD_DIR") if ok else ""
    backup_dir = _cfg_get(cfg, "PATHS", "BACKUP_DIR") if ok else ""

    # 2) data artifacts
    artifacts = [
        ("catalog_v2", INDEX_DIR / "wagstaff_catalog_v2.json"),
        ("catalog_index", INDEX_DIR / "wagstaff_catalog_index_v1.json"),
        ("icon_index", INDEX_DIR / "wagstaff_icon_index_v1.json"),
        ("i18n_index", INDEX_DIR / "wagstaff_i18n_v1.json"),
        ("tuning_trace", INDEX_DIR / "wagstaff_tuning_trace_v1.json"),
    ]
    for name, path in artifacts:
        info = file_info(path)
        level = "PASS" if info["exists"] else "WARN"
        details = f"{human_mtime(info['mtime'])} | {human_size(info['size'])}"
        table.add_row(
            f"data/{name}",
            _status(level),
            details if info["exists"] else "missing",
            "Run the matching build_* script" if not info["exists"] else "",
        )
        if not info["exists"]:
            warn += 1

    # 3) DST root + scripts source (optional)
    if dst_root:
        dst_root_p = Path(dst_root)
        ok, level, details, fix = _check_path_exists(dst_root_p, "dir", "Ensure DST is installed at this path")
        table.add_row("DST_ROOT exists", _status(level), details, fix if level != "PASS" else "")
        if not ok:
            warn += 1

        scripts_zip = dst_root_p / "data" / "databundles" / "scripts.zip"
        scripts_dir = dst_root_p / "data" / "scripts"
        ok_zip = scripts_zip.is_file()
        ok_dir = scripts_dir.is_dir()
        level = "PASS" if (ok_zip or ok_dir) else "WARN"
        details = f"zip={scripts_zip} ({'ok' if ok_zip else 'missing'}), dir={scripts_dir} ({'ok' if ok_dir else 'missing'})"
        table.add_row("scripts source", _status(level), details, "Ensure scripts.zip exists or data/scripts is available" if level != "PASS" else "")
        if level != "PASS":
            warn += 1

    # 4) steamcmd (optional)
    if steamcmd_dir:
        steamcmd = Path(steamcmd_dir) / "steamcmd.sh"
        ok, level, details, fix = _check_path_exists(steamcmd, "file", "Ensure SteamCMD is installed and steamcmd.sh exists")
        table.add_row("steamcmd.sh", _status(level), details, fix if level != "PASS" else "")
        if not ok:
            warn += 1

    # 5) screen (optional)
    screen_path = shutil.which("screen")
    if not screen_path:
        table.add_row("screen installed", _status("WARN"), "(not found)", "sudo apt-get install screen")
        warn += 1
    else:
        try:
            r = subprocess.run(["screen", "-version"], capture_output=True, text=True)
            level = "PASS" if (r.returncode == 0) else "WARN"
            table.add_row("screen installed", _status(level), screen_path, "")
            if level == "WARN":
                warn += 1
        except Exception as e:
            table.add_row("screen installed", _status("WARN"), str(e), "Verify screen is executable")
            warn += 1

    # 6) backup dir (optional)
    if backup_dir:
        bdir = Path(backup_dir)
        level = "PASS" if bdir.exists() else "WARN"
        table.add_row("BACKUP_DIR exists", _status(level), str(bdir), "mkdir -p this directory" if not bdir.exists() else "")
        if level != "PASS":
            warn += 1

    console.print(table)
    console.print(f"[dim]Root: {PROJECT_ROOT} | Summary: FAIL={fail}, WARN={warn}[/dim]")

    if not args.enforce:
        return 0
    if fail:
        return 2
    if args.strict and warn:
        return 1
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
```

### File: apps/cli/commands/explorer.py
- mode: full
- size_bytes: 9805
- sha256_12: 6d063399516d

```py
#!/usr/bin/env python3
import os
import sys
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.tree import Tree
from rich.panel import Panel
from rich.prompt import Prompt, IntPrompt
from rich.syntax import Syntax
from rich import box
from apps.cli.cli_common import PROJECT_ROOT
from core.version import project_version

from core.engine import WagstaffEngine

console = Console()

class DSTExplorer:
    def __init__(self):
        # 初始化引擎
        try:
            self.engine = WagstaffEngine(load_db=True)
        except Exception as e:
            console.print(f"[red]引擎启动失败: {e}[/red]")
            sys.exit(1)
        
        ver = project_version()
        console.print(Panel(f"[bold cyan]Wagstaff 源码透视镜 {ver}[/bold cyan]\n模式: {self.engine.mode.upper()} | 解析核心: Multi-Parser", border_style="blue"))
        if self.engine.tuning:
            console.print(f"[dim]⚡ Tuning 数值库就绪 ({len(self.engine.tuning.raw_map)} 条目)[/dim]")

    def get_structure_tree(self):
        """展示源码目录结构"""
        tree = Tree(f"📁 [bold yellow]源码结构[/bold yellow]")
        dir_counts = {}
        for f in self.engine.file_list:
            clean_path = f.replace("scripts/", "", 1) if f.startswith("scripts/") else f
            top_dir = clean_path.split('/')[0] if '/' in clean_path else "[Root Files]"
            dir_counts[top_dir] = dir_counts.get(top_dir, 0) + 1

        for d, count in sorted(dir_counts.items(), key=lambda x: x[1], reverse=True):
            if d == "[Root Files]":
                tree.add(f"📄 {d} ({count})")
            else:
                style = "bold cyan" if d in ["prefabs", "components", "tuning.lua"] else "white"
                tree.add(f"📂 [{style}]{d}[/{style}] ([dim]{count}[/dim])")
        return tree

    def search_files(self):
        """文件名搜索"""
        keyword = Prompt.ask("[bold green]🔍 搜索关键词[/bold green]")
        if not keyword: return
        
        matches = [f for f in self.engine.file_list if keyword.lower() in f.lower()]
        
        if not matches:
            console.print("[yellow]无结果[/yellow]")
            return

        table = Table(title=f"Results: '{keyword}'", box=box.SIMPLE)
        table.add_column("路径", style="dim")
        table.add_column("文件", style="bold green")
        for m in matches[:15]:
            d, f = os.path.split(m)
            table.add_row(d, f)
        console.print(table)
        if len(matches) > 15: console.print(f"[dim]...剩余 {len(matches)-15} 项隐藏[/dim]")

    def analyze_content(self, filename, content):
        """核心分析逻辑：根据 analyzer 返回的类型进行多态渲染"""
        from core.parsers import LuaAnalyzer
        
        try:
            # 1. 统一入口解析 (Facade)
            data = LuaAnalyzer(content).get_report()
        except Exception as e:
            console.print(f"[red]解析失败: {e}[/red]")
            return
        
        # 2. 根据数据类型分发渲染
        dtype = data.get("type", "prefab")
        tree = Tree(f"🧬 [bold green]深度解析: {dtype.upper()}[/bold green]")
        
        if dtype == "loot":
            self._render_loot(tree, data)
        elif dtype == "widget":
            self._render_widget(tree, data)
        elif dtype == "strings":
            self._render_strings(tree, data)
        else:
            self._render_prefab(tree, data)

        console.print(Panel(tree, border_style="green"))
        input("按回车返回...")

    # === 子渲染器 (Renderers) ===

    def _render_loot(self, tree, data):
        """渲染掉落表数据"""
        if data.get('table_name'):
            tree.add(f"📜 表名: [bold gold1]{data['table_name']}[/bold gold1]")
        
        entries = data.get('entries', [])
        if entries:
            branch = tree.add(f"💰 掉落项 ({len(entries)})")
            for item in entries:
                if item.get('method') == 'Random':
                    branch.add(f"[cyan]{item['item']}[/cyan]: 权重 [yellow]{item['weight']}[/yellow]")
                else:
                    chance = item.get('chance', 0)
                    branch.add(f"[cyan]{item['item']}[/cyan]: 几率 [magenta]{chance}[/magenta]")

    def _render_widget(self, tree, data):
        """渲染 UI Widget 数据"""
        if data.get('classes'):
            c_branch = tree.add("🧩 UI 类定义")
            for c in data['classes']:
                c_branch.add(f"[bold white]{c['name']}[/bold white] (extends [dim]{c['parent']}[/dim])")
        
        if data.get('dependencies'):
            d_branch = tree.add("🔗 依赖模块")
            for d in data['dependencies']:
                d_branch.add(f"[dim]{d}[/dim]")

    def _render_strings(self, tree, data):
        """渲染文本配置数据"""
        if data.get('includes'):
            tree.add(f"📥 引入文件: {', '.join(data['includes'])}")
        
        if data.get('roots'):
            r_branch = tree.add("🔤 文本根节点 (Roots)")
            for root in data['roots']:
                r_branch.add(f"STRINGS.[bold yellow]{root}[/bold yellow]")

    def _render_prefab(self, tree, data):
        """渲染实体 Prefab 数据 (包含 Tuning 增强)"""
        # 1. 资源
        if data.get('assets'):
            asset_branch = tree.add(f"📦 资源引用 ({len(data['assets'])})")
            for a in data['assets']:
                style = "magenta" if "Anim" in a['type'] else "blue"
                asset_branch.add(f"[{style}]{a['type']}[/{style}]: {a['path']}")

        # 2. 逻辑 (Brain/SG/Tags)
        logic_branch = tree.add("🧠 核心逻辑")
        has_logic = False
        if data.get('brain'): 
            logic_branch.add(f"AI: [magenta]{data['brain']}[/magenta]")
            has_logic = True
        if data.get('stategraph'): 
            logic_branch.add(f"SG: [magenta]{data['stategraph']}[/magenta]")
            has_logic = True
        if data.get('tags'): 
            tags = data['tags']
            tag_str = ", ".join([f"[dim]{t}[/dim]" for t in tags[:8]])
            if len(tags) > 8: tag_str += "..."
            logic_branch.add(f"Tags: {tag_str}")
            has_logic = True
        if not has_logic: logic_branch.label = "[dim]🧠 核心逻辑 (无)[/dim]"

        # 3. 组件 (使用 Engine 的 Tuning 进行增强)
        if data.get('components'):
            comp_branch = tree.add(f"⚙️ 功能组件 ({len(data['components'])})")
            for comp in data['components']:
                node = comp_branch.add(f"[bold yellow]{comp['name']}[/bold yellow]")
                
                # 属性
                if comp['properties']:
                    target = node if len(comp['properties']) <=3 else node.add("[dim]属性配置[/dim]")
                    for p in comp['properties']:
                        # 使用 Engine 传入的 Tuning 进行增强
                        p_text = self.engine.tuning.enrich(p) if self.engine.tuning else p
                        target.add(f"[cyan]{p_text}[/cyan]")
                
                # 方法
                if comp['methods']:
                    target = node if len(comp['methods']) <=3 else node.add("[dim]函数调用[/dim]")
                    for m in comp['methods']:
                        # 使用 Engine 传入的 Tuning 进行增强
                        m_text = self.engine.tuning.enrich(m) if self.engine.tuning else m
                        target.add(f"[green]ƒ[/green] {m_text}")
        else:
            tree.add("[dim]⚙️ 功能组件 (无)[/dim]")

    def preview_file(self):
        """文件预览入口"""
        target = Prompt.ask("[bold green]👀 文件名[/bold green]")
        path = self.engine.find_file(target, fuzzy=True)
        if not path:
            console.print("[red]未找到[/red]")
            return
        
        console.print(f"[yellow]打开: {path}[/yellow]")
        content = self.engine.read_file(path)
        
        if content:
            # 只显示前 50 行以供概览
            syntax = Syntax("\n".join(content.splitlines()[:50]), "lua", theme="monokai", line_numbers=True)
            console.print(Panel(syntax, title=f"{path} (Top 50 lines)", border_style="blue"))
            
            action = Prompt.ask("[bold cyan]操作[/bold cyan]", choices=["q", "a"], default="q")
            if action == "a":
                self.analyze_content(path, content)

    def show_tuning(self):
        """展示 Tuning 样本"""
        if not self.engine.tuning: 
            return console.print("[red]Tuning 未加载[/red]")
        
        console.print("[bold magenta]🔢 Tuning 数值采样[/bold magenta]")
        count = 0
        for k, v in list(self.engine.tuning.raw_map.items())[:10]:
             console.print(f"  [cyan]{k}[/cyan] = {v}")
             count += 1

def main():
    explorer = DSTExplorer()
    while True:
        ver = project_version()
        console.print(f"\n[bold white on blue] 🦁 Wagstaff 探索面板 {ver} [/bold white on blue]")
        console.print("1. [bold]📁 结构[/]  2. [bold]🔍 搜索[/]  3. [bold]👀 预览&分析[/]  4. [bold]🔢 数值[/]  0. [bold red]退出[/]")
        choice = IntPrompt.ask("选择", choices=["0","1","2","3","4"], default=1)
        if choice == 0: break
        elif choice == 1: console.print(explorer.get_structure_tree())
        elif choice == 2: explorer.search_files()
        elif choice == 3: explorer.preview_file()
        elif choice == 4: explorer.show_tuning()

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
```

### File: apps/cli/commands/mgmt.py
- mode: full
- size_bytes: 7876
- sha256_12: 004cb61bd8a5

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Project management tools (docs + status sync)."""

from __future__ import annotations

import argparse
import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from apps.cli.cli_common import PROJECT_ROOT, load_status
from apps.cli.i18n import resolve_lang, status_label, t
from apps.cli.mgmt_parser import Milestone, Task, parse_milestones, parse_tasks, read_text

try:
    from rich.console import Console
    from rich.table import Table
except Exception:  # pragma: no cover
    Console = None
    Table = None


def _default_mgmt_path() -> Path:
    status = load_status()
    doc = status.get("MANAGEMENT_DOC")
    if isinstance(doc, str) and doc:
        return (PROJECT_ROOT / doc).resolve()
    return PROJECT_ROOT / "docs" / "management" / "PROJECT_MANAGEMENT.md"


def _render_status(tasks: List[Task], milestones: List[Milestone], doc_path: Path, lang: str) -> None:
    if Console is None or Table is None:
        print(f"{t('mgmt.doc_label', lang)}: {doc_path}")
        print(f"{t('mgmt.milestones_count', lang)}: {len(milestones)}")
        print(f"{t('mgmt.tasks_count', lang)}: {len(tasks)}")
        for task in tasks:
            print(f"- {task.key} {task.desc}")
        return

    console = Console()
    console.print(f"[bold]{t('mgmt.doc_label', lang)}:[/bold] {doc_path}")

    counts = {"done": 0, "in_progress": 0, "planned": 0, "unknown": 0}
    for m in milestones:
        counts[m.status] = counts.get(m.status, 0) + 1

    table = Table(title=t("mgmt.milestones_title", lang), box=None, show_header=True, header_style="bold cyan")
    table.add_column(t("mgmt.key", lang), style="bold")
    table.add_column(t("mgmt.title", lang))
    table.add_column(t("mgmt.status", lang), style="green")
    for m in milestones:
        table.add_row(m.key, m.title, status_label(m.status, lang))
    console.print(table)

    summary = (
        f"{t('status.done', lang)}={counts.get('done',0)} "
        f"{t('status.in_progress', lang)}={counts.get('in_progress',0)} "
        f"{t('status.planned', lang)}={counts.get('planned',0)}"
    )
    console.print(f"[dim]{summary}[/dim]")

    if tasks:
        console.print(f"[bold]{t('mgmt.tasks_title', lang)}[/bold]")
        for task in tasks:
            console.print(f"- {task.key}: {task.desc}")


def _sync_tasks(status_path: Path, tasks: List[Task], write: bool, lang: str) -> int:
    status_doc = load_status()
    new_tasks = [f"{t.key}：{t.desc}" for t in tasks]

    old_tasks = status_doc.get("TASKS_TODO") if isinstance(status_doc, dict) else None
    old_tasks = list(old_tasks) if isinstance(old_tasks, list) else []

    if new_tasks == old_tasks:
        print(t("mgmt.no_changes", lang))
        return 0

    if not write:
        print(t("mgmt.pending_update", lang))
        print(json.dumps({"old": old_tasks, "new": new_tasks}, ensure_ascii=False, indent=2))
        return 0

    status_doc["TASKS_TODO"] = new_tasks
    logs = status_doc.get("RECENT_LOGS")
    if not isinstance(logs, list):
        logs = []
    stamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    logs.append(f"[{stamp}] Mgmt: sync TASKS_TODO from PROJECT_MANAGEMENT.md")
    status_doc["RECENT_LOGS"] = logs

    status_path.write_text(json.dumps(status_doc, ensure_ascii=False, indent=2), encoding="utf-8")
    print(t("mgmt.tasks_updated", lang))
    return 0


def _dump_json(tasks: List[Task], milestones: List[Milestone], doc_path: Path) -> None:
    payload = {
        "doc": str(doc_path),
        "milestones": [m.__dict__ for m in milestones],
        "tasks": [t.__dict__ for t in tasks],
    }
    print(json.dumps(payload, ensure_ascii=False, indent=2))


def _file_age_days(path: Path) -> Optional[int]:
    try:
        mtime = path.stat().st_mtime
    except Exception:
        return None
    delta = datetime.now() - datetime.fromtimestamp(mtime)
    return int(delta.total_seconds() // 86400)


def _check_dev_guide(lang: str) -> int:
    guide_path = PROJECT_ROOT / "docs" / "guides" / "DEV_GUIDE.md"
    readme_path = PROJECT_ROOT / "README.md"
    mgmt_path = _default_mgmt_path()

    if not guide_path.exists():
        print(t("mgmt.devguide_missing", lang).format(path=guide_path))
        return 2

    text = read_text(guide_path)
    meta_ok = "DEV_GUIDE_META" in text
    age_days = _file_age_days(guide_path)
    age_note = f"{age_days}d" if age_days is not None else "unknown"
    age_ok = (age_days is None) or (age_days <= 30)

    readme_text = read_text(readme_path)
    readme_ok = "DEV_GUIDE" in readme_text

    mgmt_ok = mgmt_path.exists()

    if Console is None or Table is None:
        ok_label = t("status.ok", lang)
        warn_label = t("status.warn", lang)
        print(f"{t('mgmt.devguide_meta', lang)}: {ok_label if meta_ok else warn_label}")
        print(f"{t('mgmt.devguide_age', lang)}: {age_note}")
        print(f"{t('mgmt.readme_link', lang)}: {readme_ok}")
        print(f"{t('mgmt.mgmt_doc', lang)}: {mgmt_ok}")
        return 0

    console = Console()
    ok_label = t("status.ok", lang)
    warn_label = t("status.warn", lang)
    table = Table(title=t("mgmt.devguide_check", lang), box=None, show_header=True, header_style="bold cyan")
    table.add_column(t("mgmt.check", lang), style="bold")
    table.add_column(t("mgmt.status", lang))
    table.add_column(t("mgmt.details", lang), style="dim")
    table.add_row(
        t("mgmt.devguide_meta", lang),
        f"[green]{ok_label}[/green]" if meta_ok else f"[yellow]{warn_label}[/yellow]",
        t("mgmt.devguide_meta_hint", lang),
    )
    table.add_row(
        t("mgmt.devguide_age", lang),
        f"[green]{ok_label}[/green]" if age_ok else f"[yellow]{warn_label}[/yellow]",
        t("mgmt.devguide_age_hint", lang).format(age=age_note),
    )
    table.add_row(
        t("mgmt.readme_link", lang),
        f"[green]{ok_label}[/green]" if readme_ok else f"[yellow]{warn_label}[/yellow]",
        t("mgmt.readme_link_hint", lang),
    )
    table.add_row(
        t("mgmt.mgmt_doc", lang),
        f"[green]{ok_label}[/green]" if mgmt_ok else f"[yellow]{warn_label}[/yellow]",
        t("mgmt.mgmt_doc_hint", lang).format(path=str(mgmt_path)),
    )
    console.print(table)
    return 0


def main() -> int:
    p = argparse.ArgumentParser(description="Project management tools (docs + status sync)")
    p.add_argument("--doc", default=None, help="Override management doc path")
    p.add_argument("--lang", default=None, help="Language override (default: WAGSTAFF_LANG)")

    sub = p.add_subparsers(dest="action", required=True)
    sub.add_parser("status", help="Show milestones + active tasks")
    p_sync = sub.add_parser("sync", help="Sync TASKS_TODO from management doc")
    p_sync.add_argument("--write", action="store_true", help="Write changes to PROJECT_STATUS.json")
    sub.add_parser("dump", help="Dump management doc as JSON")
    sub.add_parser("check", help="Check DEV_GUIDE emphasis + freshness")

    args = p.parse_args()

    lang = resolve_lang(args.lang)
    doc_path = Path(args.doc).resolve() if args.doc else _default_mgmt_path()
    text = read_text(doc_path)
    if not text:
        raise SystemExit(t("mgmt.doc_missing", lang).format(path=doc_path))

    tasks = parse_tasks(text)
    milestones = parse_milestones(text)

    if args.action == "status":
        _render_status(tasks, milestones, doc_path, lang)
        return 0
    if args.action == "sync":
        status_path = PROJECT_ROOT / "PROJECT_STATUS.json"
        return _sync_tasks(status_path, tasks, write=bool(args.write), lang=lang)
    if args.action == "dump":
        _dump_json(tasks, milestones, doc_path)
        return 0
    if args.action == "check":
        return _check_dev_guide(lang)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
```

### File: apps/cli/commands/server.py
- mode: full
- size_bytes: 209
- sha256_12: 6c94b0d9aa63

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""CLI entrypoint: server ops."""

from __future__ import annotations

from apps.server.cli import main


if __name__ == "__main__":
    raise SystemExit(main())
```

### File: apps/cli/commands/wiki.py
- mode: full
- size_bytes: 22341
- sha256_12: a442b723e453

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""apps/cli/commands/wiki.py

CLI-oriented "wiki" front-end.

Notes
- This module is intentionally a thin UI layer.
- Core parsing/indexing lives in `engine.py`, `craft_recipes.py`, `analyzer.py`, etc.
"""

import math
import os
import re
import sys
from pathlib import Path
from typing import Dict

from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt
from rich.table import Table
from rich.tree import Tree

from apps.cli.cli_common import PROJECT_ROOT
from core.engine import WagstaffEngine  # noqa: E402
from core.parsers import LuaAnalyzer, LootParser  # noqa: E402
from core.version import project_version  # noqa: E402

console = Console()


def _parse_inventory_spec(spec: str) -> Dict[str, float]:
    """Parse inventory spec into {item: count}.

    Accepted examples
    - "twigs=2,flint=1"
    - "twigs:2 flint:1"
    - "twigs flint" (defaults to 1)

    Non-numeric counts are ignored.
    """
    out: Dict[str, float] = {}
    if not spec:
        return out

    s = spec.strip()
    if not s:
        return out

    # Fast path: key=value / key:value pairs
    pairs = re.findall(r"([A-Za-z0-9_]+)\s*[:=]\s*([0-9]+(?:\.[0-9]+)?)", s)
    for k, v in pairs:
        try:
            out[k] = out.get(k, 0.0) + float(v)
        except Exception:
            pass

    if out:
        return out

    # Fallback: plain tokens => count=1
    tokens = re.split(r"[\s,]+", s)
    for t in tokens:
        t = (t or "").strip()
        if not t:
            continue
        out[t] = out.get(t, 0.0) + 1.0
    return out


class WagstaffWiki:
    def __init__(self):
        try:
            self.engine = WagstaffEngine(load_db=True)
        except Exception as e:
            console.print(f"[red]引擎初始化失败: {e}[/red]")
            sys.exit(1)

    def run(self, args):
        if not args:
            self._print_help()
            return

        command = args[0].lower()

        if command == "recipe":
            # Supported:
            #   wiki recipe <name>
            #   wiki recipe tab <TAB>
            #   wiki recipe filter <FILTER>
            #   wiki recipe who <BUILDER_TAG>
            #   wiki recipe tech <TECH>
            #   wiki recipe uses <ITEM>
            #   wiki recipe can <INV_SPEC>
            #   wiki recipe missing <RECIPE> <INV_SPEC>
            if len(args) >= 2 and args[1].lower() in ("tab", "filter", "who", "tech", "uses", "can", "missing", "tabs", "filters"):
                sub = args[1].lower()

                if sub == "tabs":
                    self._list_recipe_tabs()
                elif sub == "filters":
                    self._list_recipe_filters()
                elif sub == "missing":
                    if len(args) < 4:
                        return console.print("[red]用法: wiki recipe missing <recipe> <inv>[/red]")
                    recipe = args[2]
                    inv = _parse_inventory_spec(" ".join(args[3:]))
                    self._recipe_missing(recipe, inv)
                else:
                    if len(args) < 3:
                        return console.print("[red]缺少参数[/red]")
                    q = " ".join(args[2:])
                    if sub == "tab":
                        self._list_recipe_by_tab(q)
                    elif sub == "filter":
                        self._list_recipe_by_filter(q)
                    elif sub == "who":
                        self._list_recipe_by_builder_tag(q)
                    elif sub == "tech":
                        self._list_recipe_by_tech(q)
                    elif sub == "uses":
                        self._list_recipe_by_ingredient(q)
                    elif sub == "can":
                        inv = _parse_inventory_spec(q)
                        self._list_recipe_craftable(inv)
            else:
                q = args[1] if len(args) > 1 else None
                self._search_recipe(q)

        elif command in ("mob", "item"):
            q = args[1] if len(args) > 1 else None
            self._analyze_prefab(q)

        elif command == "loot":
            q = args[1] if len(args) > 1 else None
            self._find_loot_table(q)

        elif command == "food":
            # Minimal preparedfoods index
            #   wiki food <name>
            #   wiki food can <INV_SPEC>
            if len(args) >= 2 and args[1].lower() == "can":
                inv = _parse_inventory_spec(" ".join(args[2:]))
                self._list_food_cookable(inv)
            else:
                q = args[1] if len(args) > 1 else None
                self._show_food(q)

        elif command == "find":
            q = args[1] if len(args) > 1 else None
            self._global_search_interactive(q)

        else:
            self._print_help()

    def _print_help(self):
        ver = project_version()
        console.print(
            Panel(
                f"""
[bold cyan]📖 Wagstaff Wiki {ver} (Craft + Cooking)[/bold cyan]

[green]wagstaff wiki recipe <配方名/产物名>[/green]
[green]wagstaff wiki recipe tab <TAB>[/green]            按制作栏大类列出
[green]wagstaff wiki recipe filter <FILTER>[/green]      按筛选分类列出
[green]wagstaff wiki recipe who <TAG>[/green]            按角色专属列出 (builder_tag)
[green]wagstaff wiki recipe tech <TECH>[/green]          按科技需求列出
[green]wagstaff wiki recipe uses <ITEM>[/green]          反查：哪些配方需要该材料
[green]wagstaff wiki recipe can <INV>[/green]            给定材料，列出可制作配方
[green]wagstaff wiki recipe missing <R> <INV>[/green]    给定材料，查看缺少哪些
[green]wagstaff wiki recipe tabs[/green]                 查看 TAB 顺序
[green]wagstaff wiki recipe filters[/green]              查看 FILTER 定义(含icon字段)

[green]wagstaff wiki food <食谱名>[/green]                查询烹饪食谱(准备食物)
[green]wagstaff wiki food can <INV>[/green]              近似：按 card_ingredients 判断可做食谱

[green]wagstaff wiki mob <生物名>[/green]                查询生物/物品详情
[green]wagstaff wiki loot <表名>[/green]                 查询掉落表
[green]wagstaff wiki find <关键词>[/green]               交互式代码搜索

INV 格式例：twigs=2,flint=1  或  twigs:2 flint:1
""",
                title="Help",
                border_style="blue",
            )
        )

    # ---------- recipe detail ----------

    def _search_recipe(self, query):
        if not query:
            return console.print("[red]请输入配方名称[/red]")

        real_name, recipe_data = self.engine.recipes.get(query)  # type: ignore[union-attr]

        if not recipe_data:
            # fallback: 子串匹配
            db = self.engine.recipes  # type: ignore[assignment]
            candidates = [k for k in db.recipes.keys() if query in k]
            if not candidates:
                return console.print(f"[red]未找到配方: {query}[/red]")
            if len(candidates) > 1:
                console.print(f"[yellow]可能的匹配: {', '.join(candidates[:8])}...[/yellow]")
                return
            real_name, recipe_data = self.engine.recipes.get(candidates[0])  # type: ignore[union-attr]

        tab = str(recipe_data.get("tab", "UNKNOWN"))
        tech = str(recipe_data.get("tech", "UNKNOWN"))

        filters = recipe_data.get("filters") or []
        builder_tags = recipe_data.get("builder_tags") or ([] if recipe_data.get("builder_tag") is None else [recipe_data.get("builder_tag")])
        product = recipe_data.get("product") or None

        grid = Table.grid(expand=True)
        grid.add_column()
        grid.add_column(justify="right")

        grid.add_row(f"[bold gold1]{real_name.upper()}[/bold gold1]", f"[dim]{tab}[/dim]")
        grid.add_row(f"[bold]科技:[/bold] {tech}", "")

        if filters:
            grid.add_row(f"[bold]Filters:[/bold] {', '.join(filters)}", "")
        if builder_tags:
            grid.add_row(f"[bold]角色专属:[/bold] {', '.join([str(x) for x in builder_tags])}", "")
        if product:
            grid.add_row(f"[bold]产物:[/bold] {product}", "")

        grid.add_row("\n[bold]所需材料:[/bold]")
        for ing in recipe_data.get("ingredients", []):
            amt = ing.get("amount")
            grid.add_row(f"  • [cyan]{ing.get('item')}[/cyan]", f"[yellow]x{amt}[/yellow]")

        console.print(Panel(grid, title="🛠️  配方详情", border_style="gold1"))

    # ---------- recipe list ----------

    def _render_recipe_list(self, title: str, names):
        names = list(names or [])
        if not names:
            console.print(f"[yellow]无结果: {title}[/yellow]")
            return

        table = Table(title=f"{title} (共 {len(names)})", box=None, show_header=True, header_style="bold dim")
        table.add_column("No.", justify="right", style="dim", width=4)
        table.add_column("Recipe", style="cyan")
        table.add_column("Tab", style="dim")
        table.add_column("Tech", style="dim")

        # 只展示前 80 条，避免刷屏（后续可做交互分页）
        show = names[:80]
        for i, nm in enumerate(show, start=1):
            _, r = self.engine.recipes.get(nm)  # type: ignore[union-attr]
            tab = str((r or {}).get("tab", "UNKNOWN"))
            tech = str((r or {}).get("tech", "UNKNOWN"))
            table.add_row(str(i), nm, tab, tech)

        console.print(Panel(table, border_style="blue"))
        if len(names) > 80:
            console.print(f"[dim]... 其余 {len(names) - 80} 条未显示[/dim]")

    def _list_recipe_by_tab(self, tab):
        names = self.engine.recipes.list_by_tab(tab)  # type: ignore[union-attr]
        self._render_recipe_list(f"🧭 Tab = {tab}", names)

    def _list_recipe_by_filter(self, flt):
        names = self.engine.recipes.list_by_filter(flt)  # type: ignore[union-attr]
        self._render_recipe_list(f"🔎 Filter = {flt}", names)

    def _list_recipe_by_builder_tag(self, tag):
        names = self.engine.recipes.list_by_builder_tag(tag)  # type: ignore[union-attr]
        self._render_recipe_list(f"👤 builder_tag = {tag}", names)

    def _list_recipe_by_tech(self, tech):
        names = self.engine.recipes.list_by_tech(tech)  # type: ignore[union-attr]
        self._render_recipe_list(f"🧪 Tech = {tech}", names)

    def _list_recipe_tabs(self):
        db = self.engine.recipes  # type: ignore[assignment]
        if not db:
            return console.print("[red]recipes DB not loaded[/red]")

        rows = db.tab_order or sorted(db.by_tab.keys())
        table = Table(title=f"Craft Tabs ({len(rows)})", box=None, show_header=True, header_style="bold dim")
        table.add_column("No.", justify="right", style="dim", width=4)
        table.add_column("TAB", style="cyan")
        for i, t in enumerate(rows, start=1):
            table.add_row(str(i), str(t))
        console.print(Panel(table, border_style="blue"))

    def _list_recipe_filters(self):
        db = self.engine.recipes  # type: ignore[assignment]
        if not db:
            return console.print("[red]recipes DB not loaded[/red]")

        defs = db.filter_defs or []
        table = Table(title=f"Craft Filters ({len(defs)})", box=None, show_header=True, header_style="bold dim")
        table.add_column("No.", justify="right", style="dim", width=4)
        table.add_column("Name", style="cyan")
        table.add_column("Image", style="dim")
        table.add_column("Atlas", style="dim")

        for i, d in enumerate(defs, start=1):
            table.add_row(str(i), str(d.get("name")), str(d.get("image")), str(d.get("atlas")))

        console.print(Panel(table, border_style="blue"))

    def _list_recipe_by_ingredient(self, item: str):
        db = self.engine.recipes  # type: ignore[assignment]
        if not db:
            return console.print("[red]recipes DB not loaded[/red]")
        names = db.list_by_ingredient(item)
        self._render_recipe_list(f"🧱 Uses ingredient = {item}", names)

    def _list_recipe_craftable(self, inv: Dict[str, float]):
        db = self.engine.recipes  # type: ignore[assignment]
        if not db:
            return console.print("[red]recipes DB not loaded[/red]")

        names = db.craftable(inv)
        self._render_recipe_list("✅ Craftable recipes", names)

    def _recipe_missing(self, recipe: str, inv: Dict[str, float]):
        db = self.engine.recipes  # type: ignore[assignment]
        if not db:
            return console.print("[red]recipes DB not loaded[/red]")

        missing = db.missing_for(recipe, inv)
        if not missing:
            return console.print("[green]✅ 材料充足（或配方不存在/无材料）[/green]")

        table = Table(title=f"Missing for: {recipe}", box=None, show_header=True, header_style="bold dim")
        table.add_column("Item", style="cyan")
        table.add_column("Need", justify="right")
        table.add_column("Have", justify="right", style="dim")

        for row in missing:
            table.add_row(row["item"], str(row["need"]), str(row["have"]))

        console.print(Panel(table, border_style="red"))

    # ---------- cooking recipes ----------

    def _show_food(self, query: str):
        if not query:
            return console.print("[red]请输入食谱名[/red]")

        db = self.engine.cooking_recipes or {}
        if query not in db:
            # fuzzy contains
            cands = [k for k in db.keys() if query in k]
            if not cands:
                return console.print(f"[red]未找到食谱: {query}[/red]")
            if len(cands) > 1:
                console.print(f"[yellow]可能的匹配: {', '.join(cands[:10])}...[/yellow]")
                return
            query = cands[0]

        r = db.get(query, {})

        grid = Table.grid(expand=True)
        grid.add_column()
        grid.add_column(justify="right")

        grid.add_row(f"[bold gold1]{query.upper()}[/bold gold1]", str(r.get("foodtype", "")))

        for k in ("hunger", "health", "sanity", "perishtime", "cooktime", "priority", "weight"):
            if k in r:
                grid.add_row(f"[bold]{k}:[/bold] {r.get(k)}", "")

        tags = r.get("tags")
        if tags:
            grid.add_row(f"[bold]tags:[/bold] {tags}", "")

        card = r.get("card_ingredients") or []
        if card:
            grid.add_row("\n[bold]card_ingredients (近似用):[/bold]")
            for it, cnt in card:
                grid.add_row(f"  • [cyan]{it}[/cyan]", f"[yellow]x{cnt}[/yellow]")

        console.print(Panel(grid, title="🍲 食谱详情", border_style="gold1"))

    def _list_food_cookable(self, inv: Dict[str, float]):
        # NOTE: This is an approximation: uses card_ingredients as requirements.
        db = self.engine.cooking_recipes or {}
        if not db:
            return console.print("[yellow]未加载 cooking recipes[/yellow]")

        ok: List[str] = []
        for name, rec in db.items():
            req = rec.get("card_ingredients")
            if not req:
                continue
            good = True
            for it, cnt in req:
                try:
                    need = float(cnt)
                except Exception:
                    good = False
                    break
                have = float(inv.get(str(it), 0.0))
                if have + 1e-9 < need:
                    good = False
                    break
            if good:
                ok.append(name)

        ok = sorted(ok)
        table = Table(title=f"Cookable (approx) ({len(ok)})", box=None, show_header=True, header_style="bold dim")
        table.add_column("No.", justify="right", style="dim", width=4)
        table.add_column("Food", style="cyan")
        for i, nm in enumerate(ok[:120], start=1):
            table.add_row(str(i), nm)
        console.print(Panel(table, border_style="blue"))
        if len(ok) > 120:
            console.print(f"[dim]... 其余 {len(ok) - 120} 条未显示[/dim]")

    # ---------- prefab / loot / find (kept) ----------

    def _analyze_prefab(self, query):
        if not query:
            return console.print("[red]请输入名称[/red]")

        filepath = self.engine.find_file(query, fuzzy=True)
        if not filepath:
            return console.print(f"[red]未找到文件: {query}[/red]")

        content = self.engine.read_file(filepath)
        report = LuaAnalyzer(content).get_report()

        tree = Tree(f"🧬 [bold green]实体情报: {os.path.basename(filepath)}[/bold green]")
        tuning = self.engine.tuning

        if report.get("components"):
            comp_branch = tree.add("⚙️ 关键组件")
            for comp in report["components"]:
                c_name = comp["name"]
                has_content = comp.get("properties") or comp.get("methods")

                style = "bold yellow"
                if c_name in ["weapon", "health", "hunger", "sanity", "armor", "lootdropper"]:
                    style = "bold magenta"

                node_text = f"[{style}]{c_name}[/{style}]"

                if not has_content:
                    comp_branch.add(node_text)
                    continue

                comp_node = comp_branch.add(node_text)

                for prop in comp.get("properties", []):
                    val_text = tuning.enrich(prop) if tuning else prop
                    comp_node.add(f"[dim]•[/dim] {val_text}")

                for method in comp.get("methods", []):
                    val_text = tuning.enrich(method) if tuning else method
                    if any(k in method for k in ["SetDamage", "SetMaxHealth", "SetArmor"]):
                        comp_node.add(f"[bold green]ƒ {val_text}[/bold green]")
                    elif "SetChanceLootTable" in method or "SetSharedLootTable" in method:
                        comp_node.add(f"[bold red]ƒ {val_text}[/bold red]")
                    else:
                        comp_node.add(f"[dim]ƒ[/dim] {val_text}")

        console.print(Panel(tree, border_style="green"))
        console.print(
            "\n💡 提示: 若发现 [red]SetChanceLootTable('NAME')[/red]，\n"
            "请运行: [bold cyan]wagstaff wiki loot NAME[/bold cyan] 查看掉落率"
        )

    def _find_loot_table(self, query):
        if not query:
            return console.print("[red]请输入掉落表名称 (例如: krampus)[/red]")

        console.print(f"[dim]正在全库搜索掉落表: '{query}' ...[/dim]")
        pattern = re.compile(r"SetSharedLootTable\s*\(\s*[\'\"]" + re.escape(query) + r"[\'\"]")

        found = False
        for filepath in self.engine.file_list:
            if not filepath.endswith(".lua"):
                continue
            content = self.engine.read_file(filepath)
            if not content:
                continue

            if pattern.search(content):
                self._render_loot_table(filepath, query, content)
                found = True
                break

        if not found:
            console.print(f"[red]未找到掉落表定义: '{query}'[/red]")

    def _render_loot_table(self, filepath, table_name, content):
        console.print(f"[bold green]✅ 找到定义文件: {filepath}[/bold green]")
        parser = LootParser(content)
        data = parser.parse()

        if not data["entries"]:
            console.print("[yellow]解析器未能提取到具体物品项。[/yellow]")
            return

        table = Table(title=f"💰 掉落表: {table_name}", box=None)
        table.add_column("物品 (Prefab)", style="cyan")
        table.add_column("几率 / 权重", style="magenta")
        table.add_column("类型", style="dim")

        for entry in data["entries"]:
            val_str = ""
            if "chance" in entry:
                pct = entry["chance"] * 100
                val_str = f"{pct:.2f}%" if pct < 1 else f"{pct:.0f}%"
            elif "weight" in entry:
                val_str = f"权重 {entry['weight']}"

            table.add_row(entry["item"], val_str, entry["method"])

        console.print(Panel(table, border_style="gold1"))

    def _global_search_interactive(self, query):
        if not query:
            return console.print("[red]请输入搜索关键词[/red]")

        console.print(f"[bold cyan]🔍 正在扫描全库: '{query}' ...[/bold cyan]")

        matches = []
        for f in self.engine.file_list:
            content = self.engine.read_file(f)
            if content and query in content:
                matches.append(f)

        total_count = len(matches)
        if total_count == 0:
            return console.print("[yellow]❌ 无结果[/yellow]")

        page = 1
        per_page = 15
        total_pages = math.ceil(total_count / per_page)

        while True:
            console.clear()
            start_idx = (page - 1) * per_page
            end_idx = start_idx + per_page
            current_batch = matches[start_idx:end_idx]

            console.print(Panel(f"🔍 关键词: [bold green]{query}[/bold green] | 命中: {total_count} 文件", style="blue"))

            table = Table(box=None, show_header=True, header_style="bold dim")
            table.add_column("No.", justify="right", style="dim", width=4)
            table.add_column("文件路径", style="cyan")

            for i, f in enumerate(current_batch):
                idx = start_idx + i + 1
                dir_path, fname = os.path.split(f)
                display_path = f"{dir_path}/[bold white]{fname}[/bold white]"
                table.add_row(str(idx), display_path)

            console.print(table)
            status_color = "green" if page == total_pages else "yellow"
            console.print(f"\n[dim]📄 页码: [{status_color}]{page}/{total_pages}[/{status_color}][/dim]")
            console.print("[dim]操作: n 下一页 | p 上一页 | q 退出[/dim]")

            cmd = input("\n> ").strip().lower()
            if cmd == "q":
                break
            elif cmd == "n" and page < total_pages:
                page += 1
            elif cmd == "p" and page > 1:
                page -= 1


def main(argv=None):
    argv = argv or sys.argv[1:]
    WagstaffWiki().run(argv)


if __name__ == "__main__":
    main()
```

### File: apps/cli/i18n.py
- mode: full
- size_bytes: 3712
- sha256_12: c7ec26928806

```py
# -*- coding: utf-8 -*-
"""CLI i18n helpers (isolated from app/web layers)."""

from __future__ import annotations

import os
from typing import Dict, Optional


DEFAULT_LANG = "zh"
LANG_ALIASES = {
    "zh-cn": "zh",
    "zh_cn": "zh",
    "zh-hans": "zh",
    "en-us": "en",
    "en_us": "en",
}

TEXTS: Dict[str, Dict[str, str]] = {
    "en": {
        "mgmt.doc_label": "Management Doc",
        "mgmt.milestones_title": "Milestones",
        "mgmt.tasks_title": "Active Tasks",
        "mgmt.key": "Key",
        "mgmt.title": "Title",
        "mgmt.milestones_count": "milestones",
        "mgmt.tasks_count": "tasks",
        "mgmt.no_changes": "No changes.",
        "mgmt.pending_update": "Pending TASKS_TODO update (dry-run):",
        "mgmt.tasks_updated": "TASKS_TODO updated.",
        "mgmt.doc_missing": "Management doc not found: {path}",
        "mgmt.devguide_missing": "DEV_GUIDE missing: {path}",
        "mgmt.devguide_check": "DEV_GUIDE Check",
        "mgmt.check": "Check",
        "mgmt.status": "Status",
        "mgmt.details": "Details",
        "mgmt.devguide_meta": "DEV_GUIDE meta",
        "mgmt.devguide_age": "DEV_GUIDE age",
        "mgmt.readme_link": "README mentions DEV_GUIDE",
        "mgmt.mgmt_doc": "PROJECT_MANAGEMENT exists",
        "mgmt.devguide_meta_hint": "DEV_GUIDE_META block",
        "mgmt.devguide_age_hint": "mtime age {age}",
        "mgmt.readme_link_hint": "README mentions DEV_GUIDE",
        "mgmt.mgmt_doc_hint": "{path}",
        "status.done": "done",
        "status.in_progress": "in_progress",
        "status.planned": "planned",
        "status.unknown": "unknown",
        "status.ok": "OK",
        "status.warn": "WARN",
    },
    "zh": {
        "mgmt.doc_label": "管理文档",
        "mgmt.milestones_title": "里程碑",
        "mgmt.tasks_title": "当前任务",
        "mgmt.key": "编号",
        "mgmt.title": "标题",
        "mgmt.milestones_count": "里程碑",
        "mgmt.tasks_count": "任务",
        "mgmt.no_changes": "无变更。",
        "mgmt.pending_update": "待更新 TASKS_TODO（仅预览）：",
        "mgmt.tasks_updated": "TASKS_TODO 已更新。",
        "mgmt.doc_missing": "管理文档缺失：{path}",
        "mgmt.devguide_missing": "DEV_GUIDE 缺失：{path}",
        "mgmt.devguide_check": "DEV_GUIDE 检查",
        "mgmt.check": "检查项",
        "mgmt.status": "状态",
        "mgmt.details": "说明",
        "mgmt.devguide_meta": "DEV_GUIDE 元信息",
        "mgmt.devguide_age": "DEV_GUIDE 更新周期",
        "mgmt.readme_link": "README 引用 DEV_GUIDE",
        "mgmt.mgmt_doc": "PROJECT_MANAGEMENT 存在",
        "mgmt.devguide_meta_hint": "DEV_GUIDE_META 区块",
        "mgmt.devguide_age_hint": "更新时间 {age}",
        "mgmt.readme_link_hint": "README 是否引用 DEV_GUIDE",
        "mgmt.mgmt_doc_hint": "{path}",
        "status.done": "完成",
        "status.in_progress": "进行中",
        "status.planned": "规划中",
        "status.unknown": "未知",
        "status.ok": "正常",
        "status.warn": "注意",
    },
}


def resolve_lang(lang: Optional[str] = None) -> str:
    raw = (lang or os.environ.get("WAGSTAFF_LANG") or "").strip().lower().replace("_", "-")
    if raw in LANG_ALIASES:
        raw = LANG_ALIASES[raw]
    return raw if raw in TEXTS else DEFAULT_LANG


def t(key: str, lang: str, default: Optional[str] = None) -> str:
    if not lang:
        lang = DEFAULT_LANG
    return (
        TEXTS.get(lang, {}).get(key)
        or (default or TEXTS.get(DEFAULT_LANG, {}).get(key))
        or key
    )


def status_label(status: str, lang: str) -> str:
    return t(f"status.{status}", lang, status or "unknown")
```

### File: apps/cli/mgmt_parser.py
- mode: full
- size_bytes: 2233
- sha256_12: d17344d0fdfd

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Shared parser helpers for management docs."""

from __future__ import annotations

import re
from dataclasses import dataclass
from pathlib import Path
from typing import List


@dataclass
class Task:
    key: str
    desc: str


@dataclass
class Milestone:
    key: str
    title: str
    status: str


def read_text(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception:
        return ""


def extract_section(text: str, heading_prefix: str) -> str:
    lines = text.splitlines()
    out: List[str] = []
    in_section = False
    for line in lines:
        if line.startswith(heading_prefix):
            if in_section:
                break
            in_section = True
            continue
        if in_section and line.startswith("## "):
            break
        if in_section:
            out.append(line)
    return "\n".join(out)


def normalize_status(raw: str) -> str:
    r = (raw or "").strip().lower()
    if not r:
        return "unknown"
    if any(x in r for x in ("完成", "done", "complete")):
        return "done"
    if any(x in r for x in ("进行中", "in progress", "ongoing")):
        return "in_progress"
    if any(x in r for x in ("规划中", "planned", "plan")):
        return "planned"
    return r


def parse_tasks(text: str) -> List[Task]:
    section = extract_section(text, "## 4.")
    tasks: List[Task] = []
    for line in section.splitlines():
        m = re.match(r"\s*-\s*\*\*(T-\d+)\*\*[：:]?\s*(.+)$", line.strip())
        if not m:
            continue
        key = m.group(1).strip()
        desc = m.group(2).strip()
        tasks.append(Task(key=key, desc=desc))
    return tasks


def parse_milestones(text: str) -> List[Milestone]:
    section = extract_section(text, "## 2.")
    out: List[Milestone] = []
    for line in section.splitlines():
        m = re.match(r"\s*-\s*\*\*(M[0-9.]+)\s+([^*]+)\*\*（?([^）)]*)", line.strip())
        if not m:
            continue
        key = m.group(1).strip()
        title = m.group(2).strip()
        status = normalize_status(m.group(3).strip())
        out.append(Milestone(key=key, title=title, status=status))
    return out
```

### File: apps/server/__init__.py
- mode: full
- size_bytes: 66
- sha256_12: 00130d1d4140

```py
# -*- coding: utf-8 -*-
"""Server management module (DST ops)."""
```

### File: apps/server/cli.py
- mode: full
- size_bytes: 4691
- sha256_12: 8e414946c537

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Server CLI entrypoint (no data analysis dependency)."""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Iterable, Optional

from apps.server.config import DEFAULT_CONFIG_PATH, resolve_config
from apps.server import manager
from apps.server.ui import run_ui


def _add_common_flags(p: argparse.ArgumentParser) -> None:
    p.add_argument("--config", default=str(DEFAULT_CONFIG_PATH), help="Path to conf/settings.ini")
    p.add_argument("--dst-root", default=None)
    p.add_argument("--steamcmd-dir", default=None)
    p.add_argument("--backup-dir", default=None)
    p.add_argument("--cluster-name", default=None)
    p.add_argument("--klei-home", default=None)


def main(argv: Optional[Iterable[str]] = None) -> int:
    parser = argparse.ArgumentParser(description="DST server management (screen-based)")
    sub = parser.add_subparsers(dest="action", required=True)

    p_status = sub.add_parser("status", help="Show server status")
    _add_common_flags(p_status)

    p_start = sub.add_parser("start", help="Start server")
    _add_common_flags(p_start)
    p_start.add_argument("--no-caves", action="store_true", help="Start Master only")

    p_stop = sub.add_parser("stop", help="Stop server")
    _add_common_flags(p_stop)
    p_stop.add_argument("--timeout", type=float, default=40.0)
    p_stop.add_argument("--force", action="store_true", help="Kill screen sessions if graceful stop times out")

    p_restart = sub.add_parser("restart", help="Restart server")
    _add_common_flags(p_restart)
    p_restart.add_argument("--no-caves", action="store_true")
    p_restart.add_argument("--update", action="store_true")

    p_update = sub.add_parser("update", help="Update DST via SteamCMD")
    _add_common_flags(p_update)

    p_backup = sub.add_parser("backup", help="Create backup tar.gz")
    _add_common_flags(p_backup)
    p_backup.add_argument("--out", default=None, help="Output tar.gz path")

    p_restore = sub.add_parser("restore", help="Restore from backup")
    _add_common_flags(p_restore)
    p_restore.add_argument("--file", default=None, help="Backup tar.gz path")
    p_restore.add_argument("--index", type=int, default=None, help="Backup index (newest=0)")
    p_restore.add_argument("--latest", action="store_true")
    p_restore.add_argument("--yes", action="store_true", help="Confirm destructive overwrite")
    p_restore.add_argument("--start", action="store_true", help="Start after restore")

    p_logs = sub.add_parser("logs", help="Tail server logs")
    _add_common_flags(p_logs)
    p_logs.add_argument("--shard", choices=["master", "caves"], default="master")
    p_logs.add_argument("--follow", action="store_true")
    p_logs.add_argument("--lines", type=int, default=120)

    p_cmd = sub.add_parser("cmd", help="Send console command")
    _add_common_flags(p_cmd)
    p_cmd.add_argument("--shard", choices=["master", "caves"], default="master")
    p_cmd.add_argument("cmd", help="Console command to send")

    p_ui = sub.add_parser("ui", help="Interactive server menu")
    _add_common_flags(p_ui)

    args = parser.parse_args(list(argv) if argv is not None else None)
    cfg = resolve_config(
        config_path=Path(args.config),
        dst_root=args.dst_root,
        steamcmd_dir=args.steamcmd_dir,
        backup_dir=args.backup_dir,
        cluster_name=args.cluster_name,
        klei_home=args.klei_home,
    )

    if args.action == "status":
        return manager.status(cfg)
    if args.action == "start":
        return manager.start(cfg, start_caves=not args.no_caves)
    if args.action == "stop":
        return manager.stop(cfg, timeout=args.timeout, force=args.force)
    if args.action == "restart":
        return manager.restart(cfg, start_caves=not args.no_caves, update=args.update)
    if args.action == "update":
        return manager.update_game(cfg)
    if args.action == "backup":
        out = Path(args.out) if args.out else None
        return manager.backup(cfg, out_path=out)
    if args.action == "restore":
        return manager.restore(
            cfg,
            file_path=Path(args.file) if args.file else None,
            index=args.index,
            latest=args.latest,
            yes=args.yes,
            start_after=args.start,
        )
    if args.action == "logs":
        return manager.logs(cfg, shard=args.shard, follow=args.follow, lines=args.lines)
    if args.action == "cmd":
        return manager.send_cmd(cfg, shard=args.shard, command=args.cmd)
    if args.action == "ui":
        return run_ui(cfg)

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
```

### File: apps/server/config.py
- mode: full
- size_bytes: 2974
- sha256_12: 9ef01d91607b

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Server config loader (isolated from data analysis)."""

from __future__ import annotations

import configparser
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Optional


PROJECT_ROOT = Path(__file__).resolve().parents[2]
DEFAULT_CONFIG_PATH = PROJECT_ROOT / "conf" / "settings.ini"


@dataclass(frozen=True)
class ServerConfig:
    dst_root: Path
    steamcmd_dir: Optional[Path]
    backup_dir: Path
    cluster_name: str
    klei_home: Path

    @property
    def bin_dir(self) -> Path:
        return self.dst_root / "bin"

    @property
    def cluster_dir(self) -> Path:
        return self.klei_home / self.cluster_name

    @property
    def master_log(self) -> Path:
        return self.cluster_dir / "Master" / "server_log.txt"

    @property
    def caves_log(self) -> Path:
        return self.cluster_dir / "Caves" / "server_log.txt"


def _expand(val: Optional[str]) -> Optional[str]:
    if not val:
        return None
    return os.path.expanduser(val.strip())


def _cfg_get(cfg: configparser.ConfigParser, section: str, key: str) -> Optional[str]:
    try:
        val = cfg.get(section, key, fallback="").strip()
    except Exception:
        val = ""
    return _expand(val) if val else None


def load_ini(path: Path) -> configparser.ConfigParser:
    cfg = configparser.ConfigParser()
    if not path.exists():
        raise SystemExit(f"Missing config: {path}")
    cfg.read(path)
    return cfg


def resolve_config(
    *,
    config_path: Path = DEFAULT_CONFIG_PATH,
    dst_root: Optional[str] = None,
    steamcmd_dir: Optional[str] = None,
    backup_dir: Optional[str] = None,
    cluster_name: Optional[str] = None,
    klei_home: Optional[str] = None,
) -> ServerConfig:
    cfg = load_ini(config_path)

    dst_root = dst_root or os.environ.get("DST_ROOT") or _cfg_get(cfg, "PATHS", "DST_ROOT")
    steamcmd_dir = steamcmd_dir or os.environ.get("STEAMCMD_DIR") or _cfg_get(cfg, "PATHS", "STEAMCMD_DIR")
    backup_dir = backup_dir or os.environ.get("BACKUP_DIR") or _cfg_get(cfg, "PATHS", "BACKUP_DIR")
    cluster_name = cluster_name or os.environ.get("CLUSTER_NAME") or _cfg_get(cfg, "SERVER", "CLUSTER_NAME")
    klei_home = klei_home or os.environ.get("KLEI_HOME") or _cfg_get(cfg, "SERVER", "KLEI_HOME")

    if not dst_root:
        raise SystemExit("DST_ROOT missing (conf/settings.ini or --dst-root).")
    if not cluster_name:
        raise SystemExit("CLUSTER_NAME missing (conf/settings.ini or --cluster-name).")
    if not klei_home:
        raise SystemExit("KLEI_HOME missing (conf/settings.ini or --klei-home).")

    if not backup_dir:
        backup_dir = str(Path.home() / "dst_backups")

    return ServerConfig(
        dst_root=Path(dst_root),
        steamcmd_dir=Path(steamcmd_dir) if steamcmd_dir else None,
        backup_dir=Path(backup_dir),
        cluster_name=str(cluster_name),
        klei_home=Path(klei_home),
    )
```

### File: apps/server/manager.py
- mode: full
- size_bytes: 7508
- sha256_12: c7fa4e1b4e45

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Server operations (screen-based)."""

from __future__ import annotations

import os
import shutil
import subprocess
import tarfile
import time
from datetime import datetime
from pathlib import Path
from typing import Optional

from apps.server.config import ServerConfig


def _require_screen() -> None:
    if not shutil.which("screen"):
        raise SystemExit("screen not found (install screen to manage DST sessions).")


def _screen_list() -> str:
    r = subprocess.run(["screen", "-ls"], capture_output=True, text=True)
    return r.stdout or ""


def _screen_has(name: str) -> bool:
    return name in _screen_list()


def _send_screen_cmd(name: str, cmd: str) -> None:
    subprocess.run(["screen", "-S", name, "-p", "0", "-X", "stuff", f"{cmd}\015"], check=False)


def _quit_screen(name: str) -> None:
    subprocess.run(["screen", "-S", name, "-X", "quit"], check=False)


def _dst_env(cfg: ServerConfig) -> dict:
    env = os.environ.copy()
    bin_dir = cfg.bin_dir
    ld = env.get("LD_LIBRARY_PATH", "")
    env["LD_LIBRARY_PATH"] = f"{bin_dir}/lib32:{bin_dir}:{ld}"
    return env


def status(cfg: ServerConfig) -> int:
    master, caves = get_status(cfg)
    print(f"master: {'running' if master else 'stopped'}")
    print(f"caves:  {'running' if caves else 'stopped'}")
    return 0


def get_status(cfg: ServerConfig) -> tuple[bool, bool]:
    _require_screen()
    master = _screen_has("DST_Master")
    caves = _screen_has("DST_Caves")
    return master, caves


def start(cfg: ServerConfig, *, start_caves: bool = True) -> int:
    _require_screen()
    if _screen_has("DST_Master") or _screen_has("DST_Caves"):
        print("Server already running.")
        return 1

    bin_dir = cfg.bin_dir
    exe = bin_dir / "dontstarve_dedicated_server_nullrenderer"
    if not exe.exists():
        raise SystemExit(f"Missing server binary: {exe}")

    env = _dst_env(cfg)
    subprocess.run(
        ["screen", "-dmS", "DST_Master", "./dontstarve_dedicated_server_nullrenderer", "-console", "-cluster", cfg.cluster_name, "-shard", "Master"],
        cwd=bin_dir,
        env=env,
        check=False,
    )
    print("Master started.")

    if start_caves:
        subprocess.run(
            ["screen", "-dmS", "DST_Caves", "./dontstarve_dedicated_server_nullrenderer", "-console", "-cluster", cfg.cluster_name, "-shard", "Caves"],
            cwd=bin_dir,
            env=env,
            check=False,
        )
        print("Caves started.")
    return 0


def stop(cfg: ServerConfig, *, timeout: float = 40.0, force: bool = False) -> int:
    _require_screen()
    if not (_screen_has("DST_Master") or _screen_has("DST_Caves")):
        print("Server not running.")
        return 1

    for shard in ("DST_Master", "DST_Caves"):
        if _screen_has(shard):
            _send_screen_cmd(shard, "c_shutdown(true)")

    end_at = time.time() + timeout
    while time.time() < end_at:
        if not (_screen_has("DST_Master") or _screen_has("DST_Caves")):
            print("Server stopped.")
            return 0
        time.sleep(0.5)

    if force:
        for shard in ("DST_Master", "DST_Caves"):
            if _screen_has(shard):
                _quit_screen(shard)
        print("Server force-stopped.")
        return 0

    print("Timeout waiting for shutdown. Use --force to kill sessions.")
    return 2


def restart(cfg: ServerConfig, *, start_caves: bool = True, update: bool = False) -> int:
    stop(cfg, timeout=40.0, force=True)
    if update:
        update_game(cfg)
    return start(cfg, start_caves=start_caves)


def update_game(cfg: ServerConfig) -> int:
    if not cfg.steamcmd_dir:
        raise SystemExit("STEAMCMD_DIR missing (conf/settings.ini or --steamcmd-dir).")
    steamcmd = cfg.steamcmd_dir / "steamcmd.sh"
    if not steamcmd.exists():
        raise SystemExit(f"steamcmd.sh not found: {steamcmd}")

    subprocess.run(
        [
            str(steamcmd),
            "+force_install_dir",
            str(cfg.dst_root),
            "+login",
            "anonymous",
            "+app_update",
            "343050",
            "validate",
            "+quit",
        ],
        check=False,
    )
    print("Update completed.")
    return 0


def backup(cfg: ServerConfig, *, out_path: Optional[Path] = None) -> int:
    cfg.backup_dir.mkdir(parents=True, exist_ok=True)
    if not cfg.cluster_dir.exists():
        raise SystemExit(f"Cluster dir missing: {cfg.cluster_dir}")

    if out_path is None:
        stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        out_path = cfg.backup_dir / f"backup_{stamp}.tar.gz"

    with tarfile.open(out_path, "w:gz") as tar:
        tar.add(cfg.cluster_dir, arcname=cfg.cluster_name)
    print(f"Backup created: {out_path}")
    return 0


def _safe_delete_cluster(cfg: ServerConfig) -> None:
    target = cfg.cluster_dir.resolve()
    base = cfg.klei_home.resolve()

    if str(target) in ("/", str(Path.home()), str(base)):
        raise SystemExit(f"Refuse to delete unsafe path: {target}")
    if not str(target).startswith(str(base)):
        raise SystemExit("Cluster path is outside KLEI_HOME.")

    shutil.rmtree(target)


def _list_backups(cfg: ServerConfig) -> list[Path]:
    if not cfg.backup_dir.exists():
        return []
    backups = sorted(cfg.backup_dir.glob("*.tar.gz"), key=lambda p: p.stat().st_mtime, reverse=True)
    return backups


def list_backups(cfg: ServerConfig) -> list[Path]:
    return _list_backups(cfg)


def restore(
    cfg: ServerConfig,
    *,
    file_path: Optional[Path] = None,
    index: Optional[int] = None,
    latest: bool = False,
    yes: bool = False,
    start_after: bool = False,
) -> int:
    backups = _list_backups(cfg)
    if not backups:
        raise SystemExit(f"No backups found in {cfg.backup_dir}")

    chosen: Optional[Path] = None
    if file_path:
        chosen = Path(file_path)
    elif index is not None:
        if index < 0 or index >= len(backups):
            raise SystemExit("Backup index out of range.")
        chosen = backups[index]
    elif latest:
        chosen = backups[0]
    else:
        chosen = backups[0]

    if not chosen.exists():
        raise SystemExit(f"Backup not found: {chosen}")
    if not yes:
        raise SystemExit("Restore requires --yes to confirm destructive overwrite.")

    if _screen_has("DST_Master") or _screen_has("DST_Caves"):
        stop(cfg, timeout=40.0, force=True)

    if cfg.cluster_dir.exists():
        _safe_delete_cluster(cfg)

    with tarfile.open(chosen, "r:gz") as tar:
        tar.extractall(cfg.klei_home)
    print(f"Restore completed: {chosen}")

    if start_after:
        return start(cfg, start_caves=True)
    return 0


def logs(cfg: ServerConfig, *, shard: str = "master", follow: bool = False, lines: int = 120) -> int:
    log_path = cfg.master_log if shard == "master" else cfg.caves_log
    if not log_path.exists():
        raise SystemExit(f"Log not found: {log_path}")
    cmd = ["tail", "-n", str(lines), str(log_path)]
    if follow:
        cmd = ["tail", "-f", "-n", str(lines), str(log_path)]
    subprocess.run(cmd, check=False)
    return 0


def send_cmd(cfg: ServerConfig, *, shard: str = "master", command: str = "") -> int:
    _require_screen()
    target = "DST_Master" if shard == "master" else "DST_Caves"
    if not _screen_has(target):
        raise SystemExit(f"{target} is not running.")
    _send_screen_cmd(target, command)
    print("Command sent.")
    return 0
```

### File: apps/server/ui.py
- mode: full
- size_bytes: 39402
- sha256_12: 6786f4b8ce07

```py
# -*- coding: utf-8 -*-
"""Interactive server UI (menu-based)."""

from __future__ import annotations

import os
import re
import shutil
import sys
import unicodedata
from datetime import datetime
from pathlib import Path
from typing import Callable, Optional

from apps.server import manager
from apps.server.config import ServerConfig


LANG_DEFAULT = "zh"


def _supports_color() -> bool:
    if not sys.stdout.isatty():
        return False
    if os.environ.get("NO_COLOR"):
        return False
    term = os.environ.get("TERM", "")
    if term in ("", "dumb"):
        return False
    return True


class _Palette:
    def __init__(self, enabled: bool) -> None:
        if not enabled:
            self.reset = ""
            self.bold = ""
            self.dim = ""
            self.red = ""
            self.green = ""
            self.yellow = ""
            self.blue = ""
            self.cyan = ""
            self.white = ""
            self.black = ""
            self.bg_red = ""
            self.bg_green = ""
            self.bg_yellow = ""
            self.bg_blue = ""
            self.bg_cyan = ""
            return
        self.reset = "\033[0m"
        self.bold = "\033[1m"
        self.dim = "\033[2m"
        self.red = "\033[31m"
        self.green = "\033[32m"
        self.yellow = "\033[33m"
        self.blue = "\033[34m"
        self.cyan = "\033[36m"
        self.white = "\033[97m"
        self.black = "\033[30m"
        self.bg_red = "\033[41m"
        self.bg_green = "\033[42m"
        self.bg_yellow = "\033[43m"
        self.bg_blue = "\033[44m"
        self.bg_cyan = "\033[46m"


PALETTE = _Palette(_supports_color())


TEXT = {
    "title": {"zh": "Wagstaff-Lab 控制台", "en": "Wagstaff-Lab Control Center"},
    "box_status": {"zh": "状态监控", "en": "Status Monitor"},
    "box_paths": {"zh": "路径配置", "en": "Paths & Config"},
    "box_menu": {"zh": "功能菜单", "en": "Menu"},
    "status": {"zh": "状态", "en": "Status"},
    "master": {"zh": "地面", "en": "Master"},
    "caves": {"zh": "洞穴", "en": "Caves"},
    "running": {"zh": "运行中", "en": "RUNNING"},
    "stopped": {"zh": "未运行", "en": "STOPPED"},
    "unknown": {"zh": "未知", "en": "UNKNOWN"},
    "checks": {"zh": "检查", "en": "Checks"},
    "logs": {"zh": "日志", "en": "Logs"},
    "backups": {"zh": "备份", "en": "Backups"},
    "cluster_label": {"zh": "存档簇", "en": "Cluster"},
    "dst_label": {"zh": "DST目录", "en": "DST"},
    "klei_label": {"zh": "Klei目录", "en": "Klei"},
    "steam_label": {"zh": "Steam目录", "en": "Steam"},
    "backup_label": {"zh": "备份目录", "en": "Backup Dir"},
    "section_ops": {"zh": "运维管理", "en": "Server Ops"},
    "section_data": {"zh": "数据与工具", "en": "Data & Tools"},
    "menu_start": {"zh": "🚀 启动服务器", "en": "🚀 Start server"},
    "menu_stop": {"zh": "🛑 停止服务器", "en": "🛑 Stop server"},
    "menu_restart": {"zh": "🔄 重启服务器", "en": "🔄 Restart server"},
    "menu_update": {"zh": "⬇️ 更新版本", "en": "⬇️ Update game"},
    "menu_backup": {"zh": "💾 创建备份", "en": "💾 Backup"},
    "menu_restore": {"zh": "⏪ 恢复存档", "en": "⏪ Restore"},
    "menu_logs": {"zh": "📜 查看日志", "en": "📜 View logs"},
    "menu_console": {"zh": "🎮 控制台指令", "en": "🎮 Console shortcuts"},
    "menu_cmd": {"zh": "📡 发送指令", "en": "📡 Send command"},
    "menu_lang": {"zh": "🌐 切换语言", "en": "🌐 Language"},
    "menu_quit": {"zh": "🚪 退出", "en": "🚪 Quit"},
    "tip": {"zh": "回车刷新，或输入关键字/数字。", "en": "Press Enter to refresh; type keyword or number."},
    "prompt_select": {"zh": "选项", "en": "Select"},
    "prompt_shard": {"zh": "分片 (地面/洞穴)", "en": "Shard (master/caves)"},
    "prompt_timeout": {"zh": "超时时间 (秒)", "en": "Timeout (seconds)"},
    "prompt_force": {"zh": "超时后强制退出", "en": "Force kill if timeout"},
    "prompt_update_continue": {"zh": "服务器似乎在运行，继续更新？", "en": "Server appears running. Continue update"},
    "prompt_update_restart": {"zh": "重启前更新", "en": "Update before restart"},
    "prompt_start_caves": {"zh": "启动洞穴分片", "en": "Start caves shard"},
    "prompt_follow": {"zh": "持续跟随", "en": "Follow"},
    "prompt_lines": {"zh": "行数", "en": "Lines"},
    "prompt_backup_out": {"zh": "备份输出路径 (留空自动)", "en": "Backup output path (blank for auto)"},
    "prompt_restore_choose": {"zh": "选择备份序号/路径 (空取消, L=最新)", "en": "Choose backup index/path (blank to cancel, L=latest)"},
    "prompt_restore_start": {"zh": "恢复后启动服务器", "en": "Start server after restore"},
    "prompt_send_cmd": {"zh": "控制台指令", "en": "Console command"},
    "prompt_announce": {"zh": "公告内容", "en": "Announcement"},
    "prompt_regen_confirm": {"zh": "输入 YES 确认", "en": "Type YES to confirm"},
    "prompt_rollback_days": {"zh": "回滚天数", "en": "Rollback days"},
    "console_title": {"zh": "控制台指令中心", "en": "Console Command Center"},
    "console_save": {"zh": "💾 立即保存 (c_save)", "en": "💾 Save now (c_save)"},
    "console_rollback": {"zh": "⏪ 回滚 (c_rollback)", "en": "⏪ Rollback (c_rollback)"},
    "console_announce": {"zh": "📢 发送公告 (c_announce)", "en": "📢 Announce (c_announce)"},
    "console_regen": {"zh": "☠️  重置世界 (c_regenerateworld)", "en": "☠️  Regenerate world (c_regenerateworld)"},
    "console_players": {"zh": "👥 列出玩家 (c_listallplayers)", "en": "👥 List players (c_listallplayers)"},
    "console_back": {"zh": "🔙 返回", "en": "🔙 Back"},
    "msg_screen_missing": {"zh": "未找到 screen，请先安装。", "en": "screen not found. Install screen to manage DST sessions."},
    "msg_server_running": {"zh": "服务器已在运行。", "en": "Server already running."},
    "msg_server_not_running": {"zh": "服务器未运行。", "en": "Server not running."},
    "msg_steam_missing": {"zh": "未配置 SteamCMD (设置 STEAMCMD_DIR)。", "en": "SteamCMD not configured (set STEAMCMD_DIR)."},
    "msg_cluster_missing": {"zh": "存档目录不存在", "en": "Cluster dir missing"},
    "msg_backup_none": {"zh": "暂无备份。", "en": "No backups found."},
    "msg_backup_index_oob": {"zh": "备份序号超出范围。", "en": "Backup index out of range."},
    "msg_backup_not_found": {"zh": "备份不存在", "en": "Backup not found"},
    "msg_log_not_found": {"zh": "日志不存在", "en": "Log not found"},
    "msg_unknown": {"zh": "无效选项。", "en": "Unknown option."},
    "msg_enter_continue": {"zh": "按回车继续...", "en": "Press Enter to continue..."},
    "msg_enter_number": {"zh": "请输入数字。", "en": "Please enter a number."},
    "msg_enter_yn": {"zh": "请输入 y 或 n。", "en": "Please enter y or n."},
    "msg_restore_overwrite": {"zh": "恢复将覆盖以下目录：", "en": "Restore will overwrite:"},
    "msg_restore_confirm": {"zh": "输入 DELETE <path> 确认", "en": "Type DELETE <path> to confirm"},
    "backups_header": {"zh": "备份列表 (最新在前):", "en": "Backups (newest first):"},
    "backups_more": {"zh": "还有 {count} 个未显示", "en": "{count} more not shown"},
    "backup_latest": {"zh": "最新", "en": "latest"},
    "flag_ok": {"zh": "OK", "en": "OK"},
    "flag_missing": {"zh": "缺失", "en": "MISSING"},
    "flag_na": {"zh": "N/A", "en": "N/A"},
    "file_missing": {"zh": "缺失", "en": "missing"},
    "file_unknown": {"zh": "未知", "en": "unknown"},
    "result_ok": {"zh": "结果: 成功", "en": "Result: OK"},
    "result_exit": {"zh": "结果: exit={code}", "en": "Result: exit={code}"},
    "msg_interrupted": {"zh": "已中断。", "en": "Interrupted."},
}


def _t(key: str, lang: str) -> str:
    entry = TEXT.get(key) or {}
    if lang in entry:
        return entry[lang]
    if "en" in entry:
        return entry["en"]
    return key


_ANSI_RE = re.compile(r"\x1b\[[0-9;]*m")


def _strip_ansi(text: str) -> str:
    return _ANSI_RE.sub("", text or "")


def _display_width(text: str) -> int:
    width = 0
    for ch in _strip_ansi(text):
        if unicodedata.combining(ch):
            continue
        if unicodedata.east_asian_width(ch) in ("W", "F"):
            width += 2
        else:
            width += 1
    return width


def _pad(text: str, width: int) -> str:
    padding = width - _display_width(text)
    if padding <= 0:
        return text
    return text + (" " * padding)


def _two_col(
    items: list[str],
    width: int,
    *,
    sep_text: str = "  │  ",
    min_col_width: int = 18,
) -> list[str]:
    sep = _paint(sep_text, PALETTE.dim)
    col_width = max(min_col_width, (width - _display_width(sep_text)) // 2)
    lines: list[str] = []
    for i in range(0, len(items), 2):
        left = items[i]
        right = items[i + 1] if i + 1 < len(items) else ""
        if right:
            line = _pad(left, col_width) + sep + right
        else:
            line = left
        lines.append(line.rstrip())
    return lines


def _lang_label(lang: str) -> str:
    return "中文" if lang == "zh" else "EN"


def _compose_title_line(width: int, lang: str, *, context: str = "") -> str:
    left = f"🦅 {_t('title', lang)}"
    ctx = str(context or "").strip()
    if ctx:
        left = f"{left} {_paint('·', PALETTE.dim)} {_paint(ctx, PALETTE.dim)}"
    right = f"🌐 {_lang_label(lang)}"
    gap = width - _display_width(left) - _display_width(right)
    if gap < 2:
        return left
    return left + (" " * gap) + right


def _kv_line(label: str, value: str, label_width: int) -> str:
    return f"{_pad(label, label_width)}: {value}"


def _truncate(text: str, max_width: int) -> str:
    if max_width <= 0:
        return ""
    if _display_width(text) <= max_width:
        return text
    if max_width == 1:
        return "…"

    limit = max_width - 1
    out: list[str] = []
    w = 0
    i = 0
    n = len(text)
    while i < n and w < limit:
        if text[i] == "\x1b" and i + 1 < n and text[i + 1] == "[":
            end = text.find("m", i)
            if end == -1:
                break
            out.append(text[i : end + 1])
            i = end + 1
            continue

        ch = text[i]
        i += 1
        if unicodedata.combining(ch):
            out.append(ch)
            continue
        cw = 2 if unicodedata.east_asian_width(ch) in ("W", "F") else 1
        if w + cw > limit:
            break
        out.append(ch)
        w += cw

    out.append("…")
    if PALETTE.reset:
        out.append(PALETTE.reset)
    return "".join(out)


def _divider_line(inner_width: int) -> str:
    return "─" * max(0, inner_width)


def _shorten_path(text: str, max_width: int) -> str:
    if max_width <= 0:
        return ""
    if _display_width(text) <= max_width:
        return text
    if max_width == 1:
        return "…"

    home = str(Path.home())
    if home and text.startswith(home):
        suffix = text[len(home) :]
        if not suffix or suffix.startswith(("/", "\\")):
            text = "~" + suffix

    sep = "/" if "/" in text else ("\\" if "\\" in text else os.sep)
    parts = [part for part in re.split(r"[\\/]+", text) if part]
    if not parts:
        return _truncate(text, max_width)

    has_tilde = text.startswith("~")
    tail = parts[-1]
    idx = len(parts) - 2
    while idx >= 0:
        candidate = parts[idx] + sep + tail
        if _display_width(f"…{sep}{candidate}") > max_width:
            break
        tail = candidate
        idx -= 1

    out = f"…{sep}{tail}"
    if has_tilde and not tail.startswith("~"):
        prefixed = f"~{sep}{out}"
        if _display_width(prefixed) <= max_width:
            out = prefixed
    if _display_width(out) > max_width:
        return _truncate(out, max_width)
    return out


def _fit_value(value: str, max_width: int) -> str:
    if max_width <= 0:
        return ""
    raw = str(value or "")
    if "/" in raw or "\\" in raw:
        return _shorten_path(raw, max_width)
    return _truncate(raw, max_width)


def _box_lines(title: str, lines: list[str], width: int, *, dim_body: bool = False) -> list[str]:
    width = max(40, width)
    inner = width - 2

    title_text = f" {title} "
    title_text = _truncate(title_text, inner)
    title_w = _display_width(title_text)
    top = "┌" + title_text + ("─" * max(0, inner - title_w)) + "┐"
    bottom = "└" + ("─" * inner) + "┘"

    border_style = (PALETTE.cyan + PALETTE.dim) if PALETTE.cyan else ""
    title_style = PALETTE.bold
    body_style = PALETTE.dim if dim_body else ""

    out: list[str] = []
    out.append(_paint(top, border_style, title_style))
    for raw in lines:
        line = _truncate(raw, inner)
        line = _pad(line, inner)
        if body_style:
            line = _paint(line, body_style)
        out.append(_paint("│", border_style) + line + _paint("│", border_style))
    out.append(_paint(bottom, border_style))
    return out


def _box(title: str, lines: list[str], width: int, *, dim_body: bool = False) -> None:
    for line in _box_lines(title, lines, width, dim_body=dim_body):
        print(line)


def _pill(text: str, *, fg: str, bg: str, fallback: str) -> str:
    if not PALETTE.reset:
        return fallback
    return _paint(f" {text} ", PALETTE.bold, fg, bg)


def _paint(text: str, *styles: str) -> str:
    if not any(styles):
        return text
    return "".join(styles) + text + PALETTE.reset


def _term_width(default: int = 80) -> int:
    try:
        width = shutil.get_terminal_size((default, 20)).columns
    except Exception:
        return default
    return max(60, min(120, width))


def _vstack(blocks: list[list[str]], *, gap: int = 1) -> list[str]:
    out: list[str] = []
    for idx, block in enumerate(blocks):
        if idx and gap:
            out.extend([""] * gap)
        out.extend(block)
    return out


def _hstack(
    left: list[str],
    right: list[str],
    *,
    left_width: int,
    right_width: int,
    gap: int = 2,
) -> list[str]:
    gap_text = " " * max(0, int(gap))
    blank_left = " " * max(0, left_width)
    blank_right = " " * max(0, right_width)
    height = max(len(left), len(right))
    out: list[str] = []
    for i in range(height):
        l = left[i] if i < len(left) else blank_left
        r = right[i] if i < len(right) else blank_right
        out.append(_pad(l, left_width) + gap_text + r)
    return out


def _sidebar_layout(width: int) -> Optional[tuple[int, int, int]]:
    gap = 2
    min_sidebar = 32
    max_sidebar = 48
    sidebar = min(max_sidebar, max(min_sidebar, width // 3))
    main_min = 58
    if width >= (main_min + gap + sidebar):
        return (width - gap - sidebar, sidebar, gap)
    return None


def _clear() -> None:
    if not sys.stdout.isatty():
        return
    try:
        os.system("cls" if os.name == "nt" else "clear")
    except Exception:
        pass


def _prompt(text: str, *, default: Optional[str] = None) -> str:
    suffix = f" [{default}]" if default else ""
    try:
        raw = input(f"{text}{suffix}: ").strip()
    except (KeyboardInterrupt, EOFError):
        print("")
        return default or ""
    return raw if raw else (default or "")


def _prompt_bool(text: str, *, default: bool = False, lang: str = LANG_DEFAULT) -> bool:
    hint = "Y/n" if default else "y/N"
    while True:
        raw = input(f"{text} [{hint}]: ").strip().lower()
        if not raw:
            return default
        if raw in ("y", "yes"):
            return True
        if raw in ("n", "no"):
            return False
        print(_t("msg_enter_yn", lang))


def _prompt_int(text: str, *, default: int, lang: str = LANG_DEFAULT) -> int:
    while True:
        raw = _prompt(text, default=str(default))
        try:
            return int(raw)
        except ValueError:
            print(_t("msg_enter_number", lang))


def _prompt_float(text: str, *, default: float, lang: str = LANG_DEFAULT) -> float:
    while True:
        raw = _prompt(text, default=str(default))
        try:
            return float(raw)
        except ValueError:
            print(_t("msg_enter_number", lang))


def _prompt_path(text: str, *, default: Optional[str] = None) -> Optional[Path]:
    raw = _prompt(text, default=default)
    if not raw:
        return None
    return Path(raw).expanduser()


def _pause(lang: str = LANG_DEFAULT) -> None:
    try:
        input(f"\n{_t('msg_enter_continue', lang)}")
    except (KeyboardInterrupt, EOFError):
        print("")


def _format_bytes(size: int) -> str:
    value = float(size)
    for unit in ("B", "KB", "MB", "GB", "TB"):
        if value < 1024.0 or unit == "TB":
            if unit == "B":
                return f"{int(value)}{unit}"
            return f"{value:.1f}{unit}"
        value /= 1024.0
    return f"{value:.1f}PB"


def _format_time(ts: float) -> str:
    return datetime.fromtimestamp(ts).strftime("%Y-%m-%d %H:%M")


def _path_exists(path: Optional[Path]) -> Optional[bool]:
    if path is None:
        return None
    try:
        return path.exists()
    except Exception:
        return None


def _flag(ok: Optional[bool], lang: str) -> str:
    if ok is True:
        return _paint(_t("flag_ok", lang), PALETTE.dim)
    if ok is False:
        return _paint(_t("flag_missing", lang), PALETTE.red)
    return _paint(_t("flag_na", lang), PALETTE.dim)


def _check_item(label: str, ok: Optional[bool], lang: str) -> str:
    return f"{_paint(label, PALETTE.dim)} {_flag(ok, lang)}"


def _file_brief(path: Path, lang: str) -> str:
    try:
        if not path.exists():
            return _paint(_t("file_missing", lang), PALETTE.red)
        stat = path.stat()
        return f"{_format_bytes(stat.st_size)} @ {_format_time(stat.st_mtime)}"
    except Exception:
        return _paint(_t("file_unknown", lang), PALETTE.dim)


def _backup_brief(cfg: ServerConfig, lang: str) -> str:
    backups = manager.list_backups(cfg)
    if not backups:
        return _t("msg_backup_none", lang)
    latest = backups[0]
    size = "-"
    stamp = "-"
    try:
        stat = latest.stat()
        size = _format_bytes(stat.st_size)
        stamp = _format_time(stat.st_mtime)
    except Exception:
        pass
    return f"{len(backups)} ({_t('backup_latest', lang)} {latest.name} {size} {stamp})"


def _ensure_screen(lang: str) -> bool:
    if shutil.which("screen"):
        return True
    print(_paint(_t("msg_screen_missing", lang), PALETTE.red))
    return False


def _run_action(label: str, fn: Callable[[], int], lang: str) -> None:
    print(f"\n[{label}]")
    try:
        code = fn()
    except KeyboardInterrupt:
        print(f"\n{_t('msg_interrupted', lang)}")
        return
    except SystemExit as exc:
        msg = str(exc)
        if msg:
            print(msg)
        return
    except Exception as exc:
        print(f"Error: {exc}")
        return
    if isinstance(code, int):
        if code == 0:
            print(_paint(_t("result_ok", lang), PALETTE.green))
        else:
            msg = _t("result_exit", lang).format(code=code)
            print(_paint(msg, PALETTE.yellow))


def _choose_shard(lang: str) -> str:
    default = "地面" if lang == "zh" else "master"
    raw = _prompt(_t("prompt_shard", lang), default=default).strip().lower()
    if raw in ("c", "cave", "caves", "洞", "洞穴"):
        return "caves"
    if raw in ("m", "master", "地面", "主"):
        return "master"
    return "caves" if raw.startswith("c") or "洞" in raw else "master"


def _get_status(cfg: ServerConfig) -> tuple[Optional[bool], Optional[bool], Optional[str]]:
    try:
        master, caves = manager.get_status(cfg)
        return master, caves, None
    except SystemExit as exc:
        msg = str(exc) or "screen not available"
        return None, None, msg
    except Exception as exc:
        return None, None, f"status error: {exc}"


def _status_label(running: Optional[bool], lang: str) -> str:
    if running is True:
        label = _t("running", lang)
        return _pill(label, fg=PALETTE.black, bg=PALETTE.bg_green, fallback=f"🟢 {label}")
    if running is False:
        label = _t("stopped", lang)
        return _pill(label, fg=PALETTE.white, bg=PALETTE.bg_red, fallback=f"🔴 {label}")
    label = _t("unknown", lang)
    return _pill(label, fg=PALETTE.black, bg=PALETTE.bg_yellow, fallback=f"🟡 {label}")


def _status_line(cfg: ServerConfig, lang: str) -> str:
    master, caves, err = _get_status(cfg)
    if err:
        return _paint(err, PALETTE.yellow)
    return (
        f"{_t('master', lang)}: {_status_label(master, lang)} | "
        f"{_t('caves', lang)}: {_status_label(caves, lang)}"
    )


def _is_running(cfg: ServerConfig) -> Optional[bool]:
    master, caves, err = _get_status(cfg)
    if err:
        return None
    return bool(master or caves)


def _build_status_box(cfg: ServerConfig, lang: str, width: int) -> list[str]:
    screen_ok = shutil.which("screen") is not None
    bin_ok = _path_exists(cfg.bin_dir / "dontstarve_dedicated_server_nullrenderer")
    cluster_ok = _path_exists(cfg.cluster_dir)
    steam_path = cfg.steamcmd_dir / "steamcmd.sh" if cfg.steamcmd_dir else None
    steam_ok = _path_exists(steam_path) if steam_path else None

    master, caves, err = _get_status(cfg)

    inner = max(40, width) - 2
    check_items = [
        _check_item("Screen", screen_ok, lang),
        _check_item("Binary", bin_ok, lang),
        _check_item("Cluster", cluster_ok, lang),
        _check_item("SteamCMD", steam_ok, lang),
    ]
    checks = _two_col(check_items, inner, sep_text="  ·  ", min_col_width=16)

    status_rows = [
        (_t("master", lang), _status_label(master, lang)),
        (_t("caves", lang), _status_label(caves, lang)),
        (f"📜 {_t('logs', lang)}", f"master={_file_brief(cfg.master_log, lang)} · caves={_file_brief(cfg.caves_log, lang)}"),
        (f"💾 {_t('backups', lang)}", _backup_brief(cfg, lang)),
    ]
    s_label_w = max(_display_width(label) for label, _ in status_rows)
    status_lines = [
        _kv_line(status_rows[0][0], status_rows[0][1], s_label_w),
        _kv_line(status_rows[1][0], status_rows[1][1], s_label_w),
        _paint(_divider_line(inner), PALETTE.dim),
        _paint(f"🔎 {_t('checks', lang)}", PALETTE.bold),
        *checks,
        _paint(_divider_line(inner), PALETTE.dim),
        _kv_line(status_rows[2][0], status_rows[2][1], s_label_w),
        _kv_line(status_rows[3][0], status_rows[3][1], s_label_w),
    ]
    if err:
        status_lines.append(_paint(f"! {err}", PALETTE.red))

    return _box_lines(f"📡 {_t('box_status', lang)}", status_lines, width)


def _build_paths_box(cfg: ServerConfig, lang: str, width: int) -> list[str]:
    path_rows = [
        (_t("cluster_label", lang), str(cfg.cluster_name)),
        (_t("dst_label", lang), str(cfg.dst_root)),
        (_t("klei_label", lang), str(cfg.klei_home)),
        (_t("steam_label", lang), str(cfg.steamcmd_dir or "(not set)")),
        (_t("backup_label", lang), str(cfg.backup_dir)),
    ]
    inner = max(40, width) - 2
    p_label_w = max(_display_width(label) for label, _ in path_rows)
    value_max = inner - p_label_w - 2
    path_lines = [
        _kv_line(label, _fit_value(value, value_max), p_label_w)
        for label, value in path_rows
    ]
    return _box_lines(f"🧭 {_t('box_paths', lang)}", path_lines, width, dim_body=True)


def _format_entry(key: str, label: str) -> str:
    k = str(key or "").strip()
    if not k:
        return str(label)
    if not k.startswith("["):
        k = f"[{k}]"
    return f"{k} {label}"


def _build_main_menu_box(lang: str, width: int) -> list[str]:
    inner = max(10, width - 2)
    ops = [
        _format_entry("1", _t("menu_start", lang)),
        _format_entry("2", _t("menu_stop", lang)),
        _format_entry("3", _t("menu_restart", lang)),
        _format_entry("4", _t("menu_update", lang)),
    ]
    data = [
        _format_entry("5", _t("menu_backup", lang)),
        _format_entry("6", _t("menu_restore", lang)),
        _format_entry("7", _t("menu_logs", lang)),
        _format_entry("8", _t("menu_console", lang)),
        _format_entry("9", _t("menu_cmd", lang)),
        _format_entry("L", _t("menu_lang", lang)),
        _format_entry("0", _t("menu_quit", lang)),
    ]

    lines: list[str] = []
    lines.append(_paint(_t("section_ops", lang), PALETTE.bold))
    lines.append(_paint(_divider_line(inner), PALETTE.dim))
    lines.extend(_two_col(ops, inner))
    lines.append("")
    lines.append(_paint(_t("section_data", lang), PALETTE.bold))
    lines.append(_paint(_divider_line(inner), PALETTE.dim))
    lines.extend(_two_col(data, inner))
    lines.append(_paint(_divider_line(inner), PALETTE.dim))
    lines.append(_paint(_t("tip", lang), PALETTE.dim))
    return _box_lines(f"🧰 {_t('box_menu', lang)}", lines, width)


def _render_main_screen(cfg: ServerConfig, lang: str) -> None:
    _clear()
    width = _term_width()
    print(_paint(_compose_title_line(width, lang), PALETTE.bold))
    print("")

    layout = _sidebar_layout(width)
    if layout:
        main_w, side_w, gap = layout
        status_box = _build_status_box(cfg, lang, main_w)
        menu_box = _build_main_menu_box(lang, main_w)
        paths_box = _build_paths_box(cfg, lang, side_w)
        left_col = _vstack([status_box, menu_box], gap=1)
        for line in _hstack(left_col, paths_box, left_width=main_w, right_width=side_w, gap=gap):
            print(line)
        return

    status_box = _build_status_box(cfg, lang, width)
    menu_box = _build_main_menu_box(lang, width)
    paths_box = _build_paths_box(cfg, lang, width)
    for line in _vstack([status_box, menu_box, paths_box], gap=1):
        print(line)


def _build_console_menu_box(lang: str, width: int) -> list[str]:
    inner = max(10, width - 2)
    options = [
        _format_entry("1", _t("console_save", lang)),
        _format_entry("2", _t("console_rollback", lang)),
        _format_entry("3", _t("console_announce", lang)),
        _format_entry("4", _t("console_regen", lang)),
        _format_entry("5", _t("console_players", lang)),
        _format_entry("L", _t("menu_lang", lang)),
        _format_entry("0", _t("console_back", lang)),
    ]
    lines: list[str] = []
    lines.extend(_two_col(options, inner))
    lines.append(_paint(_divider_line(inner), PALETTE.dim))
    lines.append(_paint(_t("tip", lang), PALETTE.dim))
    return _box_lines(f"🎮 {_t('console_title', lang)}", lines, width)


def _render_console_screen(cfg: ServerConfig, lang: str) -> None:
    _clear()
    width = _term_width()
    print(_paint(_compose_title_line(width, lang, context=_t("menu_console", lang)), PALETTE.bold))
    print("")

    layout = _sidebar_layout(width)
    if layout:
        main_w, side_w, gap = layout
        status_box = _build_status_box(cfg, lang, main_w)
        menu_box = _build_console_menu_box(lang, main_w)
        paths_box = _build_paths_box(cfg, lang, side_w)
        left_col = _vstack([status_box, menu_box], gap=1)
        for line in _hstack(left_col, paths_box, left_width=main_w, right_width=side_w, gap=gap):
            print(line)
        return

    status_box = _build_status_box(cfg, lang, width)
    menu_box = _build_console_menu_box(lang, width)
    paths_box = _build_paths_box(cfg, lang, width)
    for line in _vstack([status_box, menu_box, paths_box], gap=1):
        print(line)


def _console_menu(cfg: ServerConfig, lang: str) -> str:
    cur = lang
    while True:
        if not _ensure_screen(cur):
            _pause(cur)
            return cur
        _render_console_screen(cfg, cur)

        choice = _prompt(_t("prompt_select", cur), default="").strip().lower()
        if not choice:
            continue

        if choice in ("0", "back", "b", "返回"):
            return cur
        if choice in ("l", "lang", "language", "语言", "切换"):
            cur = "en" if cur == "zh" else "zh"
            continue

        if choice == "1":
            _run_action("Save", lambda: manager.send_cmd(cfg, shard="master", command="c_save()"), cur)
            _pause(cur)
            continue
        if choice == "2":
            days = _prompt_int(_t("prompt_rollback_days", cur), default=1, lang=cur)
            _run_action("Rollback", lambda: manager.send_cmd(cfg, shard="master", command=f"c_rollback({days})"), cur)
            _pause(cur)
            continue
        if choice == "3":
            msg = _prompt(_t("prompt_announce", cur), default="")
            if not msg:
                continue
            msg = msg.replace('"', '\\"')
            _run_action("Announce", lambda: manager.send_cmd(cfg, shard="master", command=f'c_announce("{msg}")'), cur)
            _pause(cur)
            continue
        if choice == "4":
            confirm = _prompt(_t("prompt_regen_confirm", cur), default="")
            if confirm != "YES":
                continue
            _run_action("Regenerate", lambda: manager.send_cmd(cfg, shard="master", command="c_regenerateworld()"), cur)
            _pause(cur)
            continue
        if choice == "5":
            _run_action("Players", lambda: manager.send_cmd(cfg, shard="master", command="c_listallplayers()"), cur)
            _pause(cur)
            continue

        print(_t("msg_unknown", cur))
        _pause(cur)


def _build_restore_box(backups: list[Path], lang: str, width: int, *, limit: int = 10) -> list[str]:
    inner = max(40, width) - 2
    lines: list[str] = []
    lines.append(_paint(_t("backups_header", lang), PALETTE.bold))
    lines.append(_paint(_divider_line(inner), PALETTE.dim))
    for idx, path in enumerate(backups[:limit]):
        size = "-"
        stamp = "-"
        try:
            stat = path.stat()
            size = _format_bytes(stat.st_size)
            stamp = _format_time(stat.st_mtime)
        except Exception:
            pass
        lines.append(f"[{idx}] {path.name}  {size}  {stamp}")
    if len(backups) > limit:
        msg = _t("backups_more", lang).format(count=len(backups) - limit)
        lines.append(_paint(f"... {msg}", PALETTE.dim))
    lines.append(_paint(_divider_line(inner), PALETTE.dim))
    lines.append(_paint(_t("prompt_restore_choose", lang), PALETTE.dim))
    return _box_lines(_t("menu_restore", lang), lines, width)


def _render_restore_screen(cfg: ServerConfig, lang: str, backups: list[Path]) -> None:
    _clear()
    width = _term_width()
    print(_paint(_compose_title_line(width, lang, context=_t("menu_restore", lang)), PALETTE.bold))
    print("")

    layout = _sidebar_layout(width)
    if layout:
        main_w, side_w, gap = layout
        main_box = _build_restore_box(backups, lang, main_w)
        side_col = _vstack([_build_status_box(cfg, lang, side_w), _build_paths_box(cfg, lang, side_w)], gap=1)
        for line in _hstack(main_box, side_col, left_width=main_w, right_width=side_w, gap=gap):
            print(line)
        return

    status_box = _build_status_box(cfg, lang, width)
    restore_box = _build_restore_box(backups, lang, width)
    paths_box = _build_paths_box(cfg, lang, width)
    for line in _vstack([status_box, restore_box, paths_box], gap=1):
        print(line)


def _confirm_restore_target(cfg: ServerConfig, lang: str, *, selected_backup: Optional[Path] = None) -> bool:
    try:
        target = cfg.cluster_dir.resolve()
    except Exception:
        target = cfg.cluster_dir

    lines: list[str] = []
    lines.append(_paint(_t("msg_restore_overwrite", lang), PALETTE.bold))
    lines.append(f"  {target}")
    if selected_backup is not None:
        lines.append("")
        lines.append(_paint(f"Backup: {selected_backup.name}", PALETTE.dim))
    lines.append("")
    lines.append(_paint(_t("msg_restore_confirm", lang), PALETTE.dim))

    _clear()
    width = _term_width()
    print(_paint(_compose_title_line(width, lang, context=_t("menu_restore", lang)), PALETTE.bold))
    print("")

    layout = _sidebar_layout(width)
    if layout:
        main_w, side_w, gap = layout
        confirm_box = _box_lines("⚠️ Confirm", lines, main_w)
        side_col = _vstack([_build_status_box(cfg, lang, side_w), _build_paths_box(cfg, lang, side_w)], gap=1)
        for line in _hstack(confirm_box, side_col, left_width=main_w, right_width=side_w, gap=gap):
            print(line)
    else:
        status_box = _build_status_box(cfg, lang, width)
        confirm_box = _box_lines("⚠️ Confirm", lines, width)
        paths_box = _build_paths_box(cfg, lang, width)
        for line in _vstack([status_box, confirm_box, paths_box], gap=1):
            print(line)

    confirm = _prompt(_t("msg_restore_confirm", lang), default="")
    return confirm == f"DELETE {target}"


def run_ui(cfg: ServerConfig) -> int:
    lang = LANG_DEFAULT
    while True:
        _render_main_screen(cfg, lang)
        choice = _prompt(_t("prompt_select", lang), default="").strip().lower()
        if not choice:
            continue

        if choice in ("r", "refresh", "刷新"):
            continue

        if choice in ("0", "q", "quit", "exit", "退出"):
            return 0

        if choice in ("l", "lang", "language", "语言", "切换"):
            lang = "en" if lang == "zh" else "zh"
            continue

        if choice in ("1", "start", "启动", "开服"):
            if not _ensure_screen(lang):
                _pause(lang)
                continue
            if _is_running(cfg) is True:
                print(_t("msg_server_running", lang))
                _pause(lang)
                continue
            start_caves = _prompt_bool(_t("prompt_start_caves", lang), default=True, lang=lang)
            _run_action("Start", lambda: manager.start(cfg, start_caves=start_caves), lang)
            _pause(lang)
            continue

        if choice in ("2", "stop", "停止", "停服"):
            if not _ensure_screen(lang):
                _pause(lang)
                continue
            if _is_running(cfg) is False:
                print(_t("msg_server_not_running", lang))
                _pause(lang)
                continue
            timeout = _prompt_float(_t("prompt_timeout", lang), default=40.0, lang=lang)
            force = _prompt_bool(_t("prompt_force", lang), default=True, lang=lang)
            _run_action("Stop", lambda: manager.stop(cfg, timeout=timeout, force=force), lang)
            _pause(lang)
            continue

        if choice in ("3", "restart", "重启"):
            if not _ensure_screen(lang):
                _pause(lang)
                continue
            update = _prompt_bool(_t("prompt_update_restart", lang), default=False, lang=lang)
            start_caves = _prompt_bool(_t("prompt_start_caves", lang), default=True, lang=lang)
            _run_action("Restart", lambda: manager.restart(cfg, start_caves=start_caves, update=update), lang)
            _pause(lang)
            continue

        if choice in ("4", "update", "更新"):
            steam_path = cfg.steamcmd_dir / "steamcmd.sh" if cfg.steamcmd_dir else None
            if steam_path is None or not steam_path.exists():
                print(_t("msg_steam_missing", lang))
                _pause(lang)
                continue
            running = _is_running(cfg)
            if running:
                proceed = _prompt_bool(_t("prompt_update_continue", lang), default=False, lang=lang)
                if not proceed:
                    continue
            _run_action("Update", lambda: manager.update_game(cfg), lang)
            _pause(lang)
            continue

        if choice in ("5", "backup", "备份"):
            if not cfg.cluster_dir.exists():
                print(f"{_t('msg_cluster_missing', lang)}: {cfg.cluster_dir}")
                _pause(lang)
                continue
            out_path = _prompt_path(_t("prompt_backup_out", lang), default="")
            _run_action("Backup", lambda: manager.backup(cfg, out_path=out_path), lang)
            _pause(lang)
            continue

        if choice in ("6", "restore", "恢复"):
            backups = manager.list_backups(cfg)
            if not backups:
                print(_t("msg_backup_none", lang))
                _pause(lang)
                continue
            _render_restore_screen(cfg, lang, backups)
            raw = _prompt(_t("prompt_restore_choose", lang), default="")
            if not raw:
                continue
            file_path = None
            index = None
            latest = False
            selected_backup = None
            raw_lower = raw.lower()
            if raw_lower in ("l", "latest", "最新"):
                latest = True
                selected_backup = backups[0]
            elif raw.isdigit():
                index = int(raw)
                if index < 0 or index >= len(backups):
                    print(_t("msg_backup_index_oob", lang))
                    _pause(lang)
                    continue
                selected_backup = backups[index]
            else:
                file_path = Path(raw).expanduser()
                if not file_path.exists():
                    print(f"{_t('msg_backup_not_found', lang)}: {file_path}")
                    _pause(lang)
                    continue
                selected_backup = file_path

            if not _confirm_restore_target(cfg, lang, selected_backup=selected_backup):
                continue
            start_after = _prompt_bool(_t("prompt_restore_start", lang), default=True, lang=lang)
            _run_action(
                "Restore",
                lambda: manager.restore(
                    cfg,
                    file_path=file_path,
                    index=index,
                    latest=latest,
                    yes=True,
                    start_after=start_after,
                ),
                lang,
            )
            _pause(lang)
            continue

        if choice in ("7", "logs", "日志"):
            shard = _choose_shard(lang)
            log_path = cfg.master_log if shard == "master" else cfg.caves_log
            if not log_path.exists():
                print(f"{_t('msg_log_not_found', lang)}: {log_path}")
                _pause(lang)
                continue
            lines = _prompt_int(_t("prompt_lines", lang), default=120, lang=lang)
            follow = _prompt_bool(_t("prompt_follow", lang), default=True, lang=lang)
            _run_action("Logs", lambda: manager.logs(cfg, shard=shard, follow=follow, lines=lines), lang)
            _pause(lang)
            continue

        if choice in ("8", "console", "控制台"):
            lang = _console_menu(cfg, lang)
            continue

        if choice in ("9", "cmd", "command", "指令"):
            if not _ensure_screen(lang):
                _pause(lang)
                continue
            shard = _choose_shard(lang)
            cmd = _prompt(_t("prompt_send_cmd", lang), default="")
            if not cmd:
                continue
            _run_action("Send command", lambda: manager.send_cmd(cfg, shard=shard, command=cmd), lang)
            _pause(lang)
            continue

        print(_t("msg_unknown", lang))
        _pause(lang)
```

### File: apps/webcraft/__init__.py
- mode: full
- size_bytes: 48
- sha256_12: 43210b35acae

```py
# -*- coding: utf-8 -*-
"""WebCraft package."""
```

### File: apps/webcraft/api.py
- mode: full
- size_bytes: 35980
- sha256_12: 92c2d565d4de

```py
# -*- coding: utf-8 -*-
from __future__ import annotations

from typing import Any, Dict, List, Optional

import hashlib

from fastapi import APIRouter, Request, Depends, HTTPException, Query
from fastapi.responses import FileResponse, JSONResponse
from pydantic import BaseModel, Field

from .catalog_store import CatalogStore, CraftRecipe, CookingRecipe
from .mechanism_store import MechanismStore
from .planner import craftable_recipes, missing_for, normalize_inventory
from .cooking_planner import explore_cookpot, find_cookable, simulate_cookpot, normalize_counts


def get_store(request: Request) -> CatalogStore:
    """Resolve the catalog store from app state (with optional auto-reload)."""

    store: CatalogStore = request.app.state.store  # type: ignore[attr-defined]
    auto = bool(getattr(request.app.state, "auto_reload_catalog", False))
    if auto:
        try:
            store.load(force=False)
        except Exception:
            # do not break requests on reload errors
            pass
    return store


def get_mechanism_store(request: Request) -> Optional[MechanismStore]:
    """Resolve mechanism index store from app state (with optional auto-reload)."""

    store = getattr(request.app.state, "mechanism_store", None)
    if store is None:
        path = getattr(request.app.state, "mechanism_path", None)
        if path:
            try:
                from pathlib import Path

                p = Path(path)
                if p.exists():
                    store = MechanismStore(p)
                    request.app.state.mechanism_store = store
            except Exception:
                store = None
    auto = bool(getattr(request.app.state, "auto_reload_mechanism", False))
    if store is not None and auto:
        try:
            store.load(force=False)
        except Exception:
            pass
    return store


def _cache_headers(
    request: Request,
    *,
    max_age: int,
    etag: Optional[str] = None,
    auto_reload_attr: str = "auto_reload_catalog",
) -> Dict[str, str]:
    if max_age <= 0:
        return {}
    if bool(getattr(request.app.state, auto_reload_attr, False)):
        return {}
    headers = {"Cache-Control": f"public, max-age={int(max_age)}"}
    if etag:
        headers["ETag"] = str(etag)
    return headers


def _json(data: Dict[str, Any], *, headers: Optional[Dict[str, str]] = None) -> JSONResponse:
    return JSONResponse(content=data, headers=headers or {})


router = APIRouter(prefix="/api/v1")


def _has_cjk(text: str) -> bool:
    return any("\u4e00" <= ch <= "\u9fff" for ch in str(text or ""))


# ----------------- optional tuning traces (requires analyzer engine) -----------------


def _get_engine(request: Request):
    return getattr(request.app.state, "engine", None)


def _get_tuning_trace_store(request: Request):
    store = getattr(request.app.state, "tuning_trace_store", None)
    if store is None:
        path = getattr(request.app.state, "tuning_trace_path", None)
        if path:
            try:
                from pathlib import Path
                from .tuning_trace import TuningTraceStore

                p = Path(path)
                if p.exists():
                    store = TuningTraceStore(p)
                    request.app.state.tuning_trace_store = store
            except Exception:
                store = None
    auto = bool(getattr(request.app.state, "auto_reload_tuning_trace", False))
    if store is not None and auto:
        try:
            store.load(force=False)
        except Exception:
            pass
    return store


def _get_i18n_index_store(request: Request):
    store = getattr(request.app.state, "i18n_index_store", None)
    if store is None:
        path = getattr(request.app.state, "i18n_index_path", None)
        if path:
            try:
                from pathlib import Path
                from .i18n_index import I18nIndexStore

                p = Path(path)
                if p.exists():
                    store = I18nIndexStore(p)
                    request.app.state.i18n_index_store = store
            except Exception:
                store = None
    auto = bool(getattr(request.app.state, "auto_reload_i18n_index", False))
    if store is not None and auto:
        try:
            store.load(force=False)
        except Exception:
            pass
    return store


def _ensure_tuning(engine, store_meta: Optional[Dict[str, Any]] = None) -> Any:
    """Best-effort: ensure tuning.lua is parsed (even without analyzer engine)."""

    tuning = getattr(engine, "tuning", None) if engine is not None else None
    if tuning is not None:
        return tuning

    try:
        from core.parsers import TuningResolver  # type: ignore
        content = ""
        if engine is not None:
            content = engine.read_file("scripts/tuning.lua") or engine.read_file("tuning.lua") or ""
        if (not content) and store_meta:
            hint = str(store_meta.get("scripts_zip") or "").strip()
            if hint:
                try:
                    import zipfile
                    with zipfile.ZipFile(hint, "r") as z:
                        content = z.read("scripts/tuning.lua").decode("utf-8", errors="replace")
                except Exception:
                    content = ""
        if not content:
            return None
        tuning = TuningResolver(content)
        if engine is not None:
            try:
                engine.tuning = tuning
            except Exception:
                pass
        return tuning
    except Exception:
        return None


def _enrich_cooking_recipe(raw: Dict[str, Any], *, name: Optional[str] = None, tuning: Any = None) -> Dict[str, Any]:
    """Return a safe copy of cooking recipe dict with optional tuning traces.

    Adds:
      - name (if missing)
      - _tuning: { field -> tuning.trace_expr(raw_string) } for any string that contains "TUNING"
    """
    out: Dict[str, Any] = dict(raw or {})
    if name and not out.get("name"):
        out["name"] = name

    if tuning is None:
        return out

    traces: Dict[str, Any] = {}
    for k, v in list(out.items()):
        if isinstance(v, str) and ("TUNING" in v):
            try:
                tr = tuning.trace_expr(v)
                traces[str(k)] = tr
                # If resolvable, inline final value so UI can show both value + expr.
                if isinstance(tr.get("value"), (int, float)):
                    out[k] = tr.get("value")
            except Exception:
                traces[str(k)] = {"expr": v, "value": None, "expr_resolved": v, "refs": {}}

    if traces:
        out["_tuning"] = traces
    return out



class PlanRequest(BaseModel):
    inventory: Dict[str, float] = Field(default_factory=dict)
    builder_tag: Optional[str] = None
    strict: bool = False
    limit: int = 200


class MissingRequest(BaseModel):
    name: str
    inventory: Dict[str, float] = Field(default_factory=dict)


class CookingFindRequest(BaseModel):
    inventory: Dict[str, float] = Field(default_factory=dict)
    limit: int = 200


class CookingSimulateRequest(BaseModel):
    slots: Dict[str, float] = Field(default_factory=dict)
    return_top: int = 25


class CookingExploreRequest(BaseModel):
    slots: Dict[str, float] = Field(default_factory=dict)
    available: List[str] = Field(default_factory=list)
    limit: int = 200


@router.get("/meta")
def meta(request: Request, store: CatalogStore = Depends(get_store)):
    m = store.meta()
    m.update({"schema_version": store.schema_version()})

    # icon config (public)
    svc = getattr(request.app.state, "icon_service", None)
    if svc is not None:
        try:
            m.update({"icon": svc.cfg.to_public_dict()})
        except Exception:
            pass


    # runtime analyzer availability (optional)
    eng = _get_engine(request)
    m.update({"analyzer_enabled": bool(eng)})
    if eng is not None:
        try:
            m.update(
                {
                    "engine_mode": str(getattr(eng, "mode", "") or ""),
                    "scripts_file_count": len(getattr(eng, "file_list", []) or []),
                }
            )
        except Exception:
            pass

    mstore = get_mechanism_store(request)
    if mstore is not None:
        try:
            m.update(
                {
                    "mechanism_enabled": True,
                    "mechanism_schema_version": mstore.schema_version(),
                    "mechanism_counts": mstore.counts(),
                }
            )
        except Exception:
            m.update({"mechanism_enabled": True})
    else:
        m.update({"mechanism_enabled": False})

    analysis_mode = "mechanism" if mstore else ("runtime" if eng else "none")
    m.update({"analysis_enabled": analysis_mode != "none", "analysis_mode": analysis_mode})

    # tuning traces for UI (optional; requires scripts mounted)
    tuning = _ensure_tuning(eng, store.meta())
    if tuning is not None:
        try:
            m.update({"tuning_enabled": True, "tuning_count": len(getattr(tuning, "raw_map", {}) or {})})
        except Exception:
            m.update({"tuning_enabled": True})
    else:
        m.update({"tuning_enabled": False})

    # tuning trace index (optional; built offline)
    tstore = _get_tuning_trace_store(request)
    if tstore is not None:
        try:
            m.update({"tuning_trace_enabled": True, "tuning_trace_count": tstore.count()})
        except Exception:
            m.update({"tuning_trace_enabled": True})
    else:
        m.update({"tuning_trace_enabled": False})

    # i18n (optional)
    iindex = _get_i18n_index_store(request)
    if iindex is not None:
        try:
            m.update({"i18n": iindex.public_meta()})
        except Exception:
            pass
    else:
        isvc = getattr(request.app.state, "i18n_service", None)
        if isvc is not None:
            try:
                scripts_zip_hint = str((m or {}).get("scripts_zip") or "").strip() or None
                m.update({"i18n": isvc.public_meta(engine=eng, scripts_zip_hint=scripts_zip_hint)})
            except Exception:
                pass

    etag = f'W/"meta-{int(store.mtime())}"'
    headers = _cache_headers(request, max_age=60, etag=etag)
    return _json(m, headers=headers)


# ----------------- mechanism index -----------------


@router.get("/mechanism/meta")
def mechanism_meta(request: Request):
    store = get_mechanism_store(request)
    if store is None:
        return _json({"enabled": False, "schema_version": 0, "meta": {}, "counts": {}})

    out = {
        "enabled": True,
        "schema_version": store.schema_version(),
        "meta": store.meta(),
        "counts": store.counts(),
    }
    etag = f'W/"mechanism-meta-{int(store.mtime())}"'
    headers = _cache_headers(request, max_age=60, etag=etag, auto_reload_attr="auto_reload_mechanism")
    return _json(out, headers=headers)


@router.get("/mechanism/components")
def mechanism_components(
    request: Request,
    q: Optional[str] = None,
    offset: int = Query(0, ge=0),
    limit: int = Query(200, ge=1, le=2000),
):
    store = get_mechanism_store(request)
    if store is None:
        return {"enabled": False, "items": [], "count": 0, "total": 0, "offset": int(offset), "limit": int(limit)}

    ids = store.search_components(q or "")
    total = len(ids)
    start = int(offset)
    end = start + int(limit)
    page = ids[start:end]

    items: List[Dict[str, Any]] = []
    for cid in page:
        comp = store.get_component(cid) or {}
        used_by = store.component_usage(cid)
        items.append(
            {
                "id": cid,
                "class_name": comp.get("class_name"),
                "path": comp.get("path"),
                "aliases": comp.get("aliases") or [],
                "methods_count": len(comp.get("methods") or []),
                "fields_count": len(comp.get("fields") or []),
                "events_count": len(comp.get("events") or []),
                "prefabs_count": len(used_by),
            }
        )

    q_sig = hashlib.sha1(str(q).encode("utf-8")).hexdigest()[:8] if q else "all"
    etag = f'W/"mechanism-components-{int(store.mtime())}-{q_sig}-{total}-{start}-{end}"'
    headers = _cache_headers(request, max_age=120, etag=etag, auto_reload_attr="auto_reload_mechanism")
    return _json(
        {"enabled": True, "items": items, "count": len(items), "total": total, "offset": start, "limit": int(limit)},
        headers=headers,
    )


@router.get("/mechanism/components/{component_id}")
def mechanism_component_detail(component_id: str, request: Request):
    store = get_mechanism_store(request)
    if store is None:
        raise HTTPException(status_code=404, detail="mechanism index not available")
    comp = store.get_component(component_id)
    if not comp:
        raise HTTPException(status_code=404, detail=f"component not found: {component_id}")
    used_by = store.component_usage(component_id)
    return {"component": comp, "used_by": used_by, "prefabs_count": len(used_by)}


@router.get("/mechanism/prefabs")
def mechanism_prefabs(
    request: Request,
    q: Optional[str] = None,
    component: Optional[str] = None,
    offset: int = Query(0, ge=0),
    limit: int = Query(200, ge=1, le=2000),
):
    store = get_mechanism_store(request)
    if store is None:
        return {"enabled": False, "items": [], "count": 0, "total": 0, "offset": int(offset), "limit": int(limit)}

    if component:
        ids = store.prefabs_for_component(component)
    else:
        ids = store.search_prefabs(q or "")
    total = len(ids)
    start = int(offset)
    end = start + int(limit)
    page = ids[start:end]

    items: List[Dict[str, Any]] = []
    for pid in page:
        row = store.get_prefab(pid) or {}
        items.append(
            {
                "id": pid,
                "components": row.get("components") or [],
                "tags": row.get("tags") or [],
                "brains": row.get("brains") or [],
                "stategraphs": row.get("stategraphs") or [],
                "helpers": row.get("helpers") or [],
                "files": row.get("files") or [],
            }
        )

    q_sig = hashlib.sha1(str(component or q or "").encode("utf-8")).hexdigest()[:8] if (component or q) else "all"
    etag = f'W/"mechanism-prefabs-{int(store.mtime())}-{q_sig}-{total}-{start}-{end}"'
    headers = _cache_headers(request, max_age=120, etag=etag, auto_reload_attr="auto_reload_mechanism")
    return _json(
        {
            "enabled": True,
            "items": items,
            "count": len(items),
            "total": total,
            "offset": start,
            "limit": int(limit),
            "component": component,
            "q": q,
        },
        headers=headers,
    )


@router.get("/mechanism/prefabs/{prefab_id}")
def mechanism_prefab_detail(prefab_id: str, request: Request):
    store = get_mechanism_store(request)
    if store is None:
        raise HTTPException(status_code=404, detail="mechanism index not available")
    row = store.get_prefab(prefab_id)
    if not row:
        raise HTTPException(status_code=404, detail=f"prefab not found: {prefab_id}")
    return {"prefab": row}


@router.get("/tuning/trace")
def tuning_trace(
    request: Request,
    key: Optional[str] = None,
    prefix: Optional[str] = None,
    limit: int = Query(2000, ge=1, le=10000),
):
    """Return tuning trace entries by key or prefix (from tuning trace index)."""
    store = _get_tuning_trace_store(request)
    if store is None:
        return {"enabled": False, "trace": None, "traces": {}, "count": 0}

    if key:
        trace = store.get(str(key))
        return {"enabled": True, "key": str(key), "trace": trace}

    pref = str(prefix or "").strip()
    traces = store.get_prefix(pref, limit=int(limit))
    return {"enabled": True, "prefix": pref, "traces": traces, "count": len(traces)}


@router.get("/assets")
def assets(request: Request, store: CatalogStore = Depends(get_store)):
    mp = store.assets(include_icon_only=True)

    svc = getattr(request.app.state, "icon_service", None)
    icon_cfg = None
    if svc is not None:
        try:
            icon_cfg = svc.cfg.to_public_dict()
        except Exception:
            icon_cfg = None

    etag = f'W/"assets-{int(store.mtime())}-{len(mp)}"'
    headers = _cache_headers(request, max_age=300, etag=etag)
    return _json(
        {"assets": mp, "count": len(mp), "icon": icon_cfg or {"mode": "off", "static_base": "/static/data/icons", "api_base": "/api/v1/icon"}},
        headers=headers,
    )


@router.get("/catalog/index")
def catalog_index(
    request: Request,
    store: CatalogStore = Depends(get_store),
    offset: int = Query(0, ge=0),
    limit: int = Query(200, ge=1, le=2000),
):
    items, total = store.catalog_index_page(offset=int(offset), limit=int(limit))

    svc = getattr(request.app.state, "icon_service", None)
    icon_cfg = None
    if svc is not None:
        try:
            icon_cfg = svc.cfg.to_public_dict()
        except Exception:
            icon_cfg = None

    etag = f'W/"catalog-{int(store.mtime())}-{total}"'
    headers = _cache_headers(request, max_age=300, etag=etag)
    return _json(
        {
            "items": items,
            "count": len(items),
            "total": total,
            "offset": int(offset),
            "limit": int(limit),
            "icon": icon_cfg or {"mode": "off", "static_base": "/static/data/icons", "api_base": "/api/v1/icon"},
        },
        headers=headers,
    )


@router.get("/catalog/search")
def catalog_search(
    request: Request,
    q: str = Query(..., min_length=1),
    store: CatalogStore = Depends(get_store),
    offset: int = Query(0, ge=0),
    limit: int = Query(200, ge=1, le=2000),
):
    name_lookup: Optional[Dict[str, str]] = None
    iindex = _get_i18n_index_store(request)
    i18n_stamp = ""
    if iindex is not None and _has_cjk(q):
        try:
            name_lookup = iindex.names("zh")
            i18n_stamp = f"-i18n{int(getattr(iindex, 'mtime', lambda: 0)())}"
        except Exception:
            name_lookup = None
    items, total = store.catalog_search(q, offset=int(offset), limit=int(limit), name_lookup=name_lookup)
    q_sig = hashlib.sha1(str(q).encode("utf-8")).hexdigest()[:8]
    etag = f'W/"catalog-search-{int(store.mtime())}-{q_sig}-{total}{i18n_stamp}"'
    headers = _cache_headers(request, max_age=60, etag=etag)
    return _json({"q": q, "items": items, "count": len(items), "total": total, "offset": int(offset), "limit": int(limit)}, headers=headers)

@router.get("/i18n")
def i18n_meta(request: Request, store: CatalogStore = Depends(get_store)):
    """Return i18n capability + available languages."""
    iindex = _get_i18n_index_store(request)
    if iindex is not None:
        try:
            meta = iindex.public_meta()
            etag = f'W/"i18n-meta-{int(getattr(iindex, "mtime", lambda: 0)())}-{meta.get("langs")}"'
            headers = _cache_headers(request, max_age=300, etag=etag)
            return _json(meta, headers=headers)
        except Exception:
            return _json({"enabled": False, "langs": [], "ui_langs": [], "modes": ["en", "zh", "id"], "default_mode": "en"})
    return _json({"enabled": False, "langs": [], "ui_langs": [], "modes": ["en", "zh", "id"], "default_mode": "en"})


@router.get("/i18n/ui/{lang}")
def i18n_ui(lang: str, request: Request):
    """Return UI strings for the given language."""
    store = _get_i18n_index_store(request)
    if store is None:
        return _json({"lang": str(lang), "strings": {}, "count": 0, "enabled": False})
    try:
        mp = store.ui_strings(str(lang))
    except Exception:
        mp = {}
    etag = f'W/"i18n-ui-{int(getattr(store, "mtime", lambda: 0)())}-{lang}-{len(mp)}"'
    headers = _cache_headers(request, max_age=600, etag=etag)
    return _json({"lang": str(lang), "strings": mp, "count": len(mp or {}), "enabled": bool(mp)}, headers=headers)


@router.get("/i18n/names/{lang}")
def i18n_names(lang: str, request: Request, store: CatalogStore = Depends(get_store)):
    """Return id->localized name mapping for items in the current catalog."""
    iindex = _get_i18n_index_store(request)
    if iindex is not None:
        try:
            mp = iindex.names(str(lang))
        except Exception:
            mp = {}
        if mp:
            etag = f'W/"i18n-names-{int(getattr(iindex, "mtime", lambda: 0)())}-{lang}-{len(mp)}"'
            headers = _cache_headers(request, max_age=600, etag=etag)
            return _json({"lang": str(lang), "names": mp, "count": len(mp or {})}, headers=headers)
    return _json({"lang": str(lang), "names": {}, "count": 0})


@router.get("/i18n/tags/{lang}")
def i18n_tags(lang: str, request: Request):
    """Return tag->localized label mapping (with optional source meta)."""
    store = _get_i18n_index_store(request)
    if store is None:
        return _json({"lang": str(lang), "tags": {}, "meta": {}, "count": 0})
    try:
        mp = store.tags(str(lang))
        meta = store.tags_meta(str(lang))
    except Exception:
        mp, meta = {}, {}
    etag = f'W/"i18n-tags-{int(getattr(store, "mtime", lambda: 0)())}-{lang}-{len(mp)}"'
    headers = _cache_headers(request, max_age=600, etag=etag)
    return _json({"lang": str(lang), "tags": mp, "meta": meta, "count": len(mp or {})}, headers=headers)


@router.get("/items/{item_id}")
def item_detail(item_id: str, request: Request, store: CatalogStore = Depends(get_store)):
    """Return best-effort item-centric view.

    This endpoint is used by the /catalog UI.
    """
    q = (item_id or "").strip()
    if not q:
        raise HTTPException(status_code=400, detail="empty item_id")

    # presentation asset + item metadata (if any)
    asset = store.get_asset(q) or store.get_asset(q.lower())
    item = store.get_item(q) or store.get_item(q.lower())

    # craft references
    craft_used_in = store.list_by_ingredient(q) or store.list_by_ingredient(q.lower())
    craft_produced_by = store.list_by_product(q) or store.list_by_product(q.lower())

    # cooking references
    cook_used_in = store.list_cooking_by_ingredient(q) or store.list_cooking_by_ingredient(q.lower())
    cook_rec = store.get_cooking_recipe(q) or store.get_cooking_recipe(q.lower())

    return {
        "item_id": q,
        "item": item,
        "asset": asset,
        "craft": {"used_in": craft_used_in, "produced_by": craft_produced_by},
        "cooking": {
            "as_recipe": (
                _enrich_cooking_recipe(
                    cook_rec.raw,
                    name=cook_rec.name,
                    tuning=_ensure_tuning(_get_engine(request), store.meta()),
                )
                if cook_rec else None
            ),
            "used_in": cook_used_in,
        },
    }



@router.get("/icon/{item_id}.png")
def icon_png(item_id: str, request: Request, store: CatalogStore = Depends(get_store)):
    """Return an item icon as PNG.

    This endpoint supports dynamic generation (atlas+xml + tex) when enabled.
    In all modes, it caches to the static icons directory as <id>.png.
    """

    svc = getattr(request.app.state, "icon_service", None)
    if svc is None:
        raise HTTPException(status_code=503, detail="Icon service not configured")

    asset = store.get_asset(item_id)
    p = svc.ensure_icon(item_id, asset)
    if not p:
        raise HTTPException(status_code=404, detail=f"Icon not found: {item_id}")

    return FileResponse(path=str(p), media_type="image/png")


# ----------------- craft browse -----------------


@router.get("/craft/filters")
def craft_filters(store: CatalogStore = Depends(get_store)):
    order, defs = store.list_filters()
    return {"order": order, "defs": defs}


@router.get("/craft/tabs")
def craft_tabs(store: CatalogStore = Depends(get_store)):
    return {"tabs": store.list_tabs()}


@router.get("/craft/tags")
def craft_tags(store: CatalogStore = Depends(get_store)):
    return {"tags": store.list_tags()}


@router.get("/craft/filters/{filter_name}/recipes")
def craft_filter_recipes(filter_name: str, store: CatalogStore = Depends(get_store)):
    names = store.list_by_filter(filter_name)
    return {"filter": filter_name, "recipes": names}


@router.get("/craft/tabs/{tab}/recipes")
def craft_tab_recipes(tab: str, store: CatalogStore = Depends(get_store)):
    names = store.list_by_tab(tab)
    return {"tab": tab, "recipes": names}


@router.get("/craft/tags/{tag}/recipes")
def craft_tag_recipes(tag: str, store: CatalogStore = Depends(get_store)):
    names = store.list_by_tag(tag)
    return {"tag": tag, "recipes": names}


@router.get("/craft/ingredients/{item}/recipes")
def craft_ingredient_recipes(item: str, store: CatalogStore = Depends(get_store)):
    names = store.list_by_ingredient(item)
    return {"ingredient": item, "recipes": names}


# ----------------- craft recipe -----------------

@router.get("/craft/products/{item}/recipes")
def craft_product_recipes(item: str, store: CatalogStore = Depends(get_store)):
    names = store.list_by_product(item)
    return {"product": item, "recipes": names}



@router.get("/craft/recipes/search")
def craft_search(
    request: Request,
    q: str = Query(..., min_length=1),
    limit: int = Query(50, ge=1, le=500),
    store: CatalogStore = Depends(get_store),
):
    name_lookup: Optional[Dict[str, str]] = None
    iindex = _get_i18n_index_store(request)
    if iindex is not None and _has_cjk(q):
        try:
            name_lookup = iindex.names("zh")
        except Exception:
            name_lookup = None
    return {"q": q, "results": store.search(q, limit=limit, name_lookup=name_lookup)}


@router.get("/craft/recipes/{name}")
def craft_recipe(name: str, store: CatalogStore = Depends(get_store)):
    rec = store.get_recipe(name)
    if not rec:
        raise HTTPException(status_code=404, detail=f"Recipe not found: {name}")
    return {"recipe": rec.raw}


# ----------------- craft planning -----------------


@router.post("/craft/plan")
def craft_plan(req: PlanRequest, store: CatalogStore = Depends(get_store)):
    inv = normalize_inventory(req.inventory)
    limit = int(req.limit or 200)
    limit = max(1, min(limit, 2000))

    recipes: List[CraftRecipe] = store.iter_recipes()
    recipes.sort(key=lambda x: x.name)

    ok, blocked = craftable_recipes(recipes, inv, builder_tag=req.builder_tag, strict=bool(req.strict))
    ok_names = [r.name for r in ok[:limit]]
    return {"craftable": ok_names, "blocked": blocked[:limit], "count": len(ok_names)}


@router.post("/craft/missing")
def craft_missing(req: MissingRequest, store: CatalogStore = Depends(get_store)):
    rec = store.get_recipe(req.name)
    if not rec:
        raise HTTPException(status_code=404, detail=f"Recipe not found: {req.name}")
    inv = normalize_inventory(req.inventory)
    miss = missing_for(rec, inv)
    return {"name": rec.name, "missing": [m.__dict__ for m in miss]}


# ----------------- cooking browse -----------------


@router.get("/cooking/recipes")
def cooking_all(store: CatalogStore = Depends(get_store)):
    recipes: List[CookingRecipe] = store.iter_cooking_recipes()
    names = sorted([r.name for r in recipes])
    return {"recipes": names, "count": len(names)}


@router.get("/cooking/foodtypes")
def cooking_foodtypes(store: CatalogStore = Depends(get_store)):
    return {"foodtypes": store.list_cooking_foodtypes()}


@router.get("/cooking/tags")
def cooking_tags(store: CatalogStore = Depends(get_store)):
    return {"tags": store.list_cooking_tags()}


@router.get("/cooking/ingredients")
def cooking_ingredients(store: CatalogStore = Depends(get_store)):
    raw, source = store.cooking_ingredients_with_fallback()
    items: List[Dict[str, Any]] = []
    virtual_ids = {
        "batnose_dried",
        "cutlichen_cooked",
        "egg",
        "egg_cooked",
        "honey_cooked",
        "honeycomb_cooked",
        "mandrake_cooked",
        "meat_cooked",
        "monstermeat_cooked",
        "pondeel_cooked",
        "royal_jelly_cooked",
        "smallmeat_cooked",
    }

    if raw:
        for iid, data in raw.items():
            tags = data.get("tags")
            if not isinstance(tags, (dict, list)):
                tags = {}
            items.append(
                {
                    "id": iid,
                    "tags": tags,
                    "foodtype": data.get("foodtype"),
                    "uses": len(store.list_cooking_by_ingredient(iid)),
                    "virtual": iid in virtual_ids,
                }
            )
    elif source == "card_ingredients":
        for iid in store.list_cooking_ingredients():
            items.append(
                {
                    "id": iid,
                    "tags": {},
                    "foodtype": None,
                    "uses": len(store.list_cooking_by_ingredient(iid)),
                    "virtual": iid in virtual_ids,
                }
            )

    items.sort(key=lambda x: (-int(x.get("uses") or 0), str(x.get("id") or "")))
    return {"ingredients": items, "count": len(items), "source": source}


@router.get("/cooking/foodtypes/{foodtype}/recipes")
def cooking_foodtype_recipes(foodtype: str, store: CatalogStore = Depends(get_store)):
    return {"foodtype": foodtype, "recipes": store.list_cooking_by_foodtype(foodtype)}


@router.get("/cooking/tags/{tag}/recipes")
def cooking_tag_recipes(tag: str, store: CatalogStore = Depends(get_store)):
    return {"tag": tag, "recipes": store.list_cooking_by_tag(tag)}


@router.get("/cooking/ingredients/{item}/recipes")
def cooking_ingredient_recipes(item: str, store: CatalogStore = Depends(get_store)):
    return {"ingredient": item, "recipes": store.list_cooking_by_ingredient(item)}


# ----------------- cooking recipe -----------------


@router.get("/cooking/recipes/search")
def cooking_search(
    request: Request,
    q: str = Query(..., min_length=1),
    limit: int = Query(50, ge=1, le=500),
    store: CatalogStore = Depends(get_store),
):
    name_lookup: Optional[Dict[str, str]] = None
    iindex = _get_i18n_index_store(request)
    if iindex is not None and _has_cjk(q):
        try:
            name_lookup = iindex.names("zh")
        except Exception:
            name_lookup = None
    return {"q": q, "results": store.search_cooking(q, limit=limit, name_lookup=name_lookup)}


@router.get("/cooking/recipes/{name}")
def cooking_recipe(name: str, request: Request, store: CatalogStore = Depends(get_store)):
    rec = store.get_cooking_recipe(name)
    if not rec:
        raise HTTPException(status_code=404, detail=f"Cooking recipe not found: {name}")
    eng = _get_engine(request)
    tuning = _ensure_tuning(eng, store.meta())
    return {"recipe": _enrich_cooking_recipe(rec.raw, name=rec.name, tuning=tuning)}


# ----------------- cooking helpers -----------------


@router.post("/cooking/find")
def cooking_find(req: CookingFindRequest, store: CatalogStore = Depends(get_store)):
    inv = normalize_counts(req.inventory)
    limit = max(1, min(int(req.limit or 200), 2000))

    recipes = store.iter_cooking_recipes()
    cookable = find_cookable(recipes, inv, limit=limit)
    names = [r.name for r in cookable]

    return {
        "cookable": names,
        "count": len(names),
        "note": "only recipes with card_ingredients are searchable/simulatable",
    }


@router.post("/cooking/explore")
def cooking_explore(req: CookingExploreRequest, store: CatalogStore = Depends(get_store)):
    recipes = store.iter_cooking_recipes()
    ingredients, _ = store.cooking_ingredients_with_fallback()
    return explore_cookpot(
        recipes,
        req.slots,
        ingredients=ingredients,
        available=req.available,
        limit=int(req.limit or 200),
    )


@router.post("/cooking/simulate")
def cooking_simulate(req: CookingSimulateRequest, request: Request, store: CatalogStore = Depends(get_store)):
    recipes = store.iter_cooking_recipes()
    ingredients, _ = store.cooking_ingredients_with_fallback()
    out = simulate_cookpot(recipes, req.slots, return_top=int(req.return_top or 25), ingredients=ingredients)

    # Attach result recipe details if available.
    if out.get("ok") and out.get("result"):
        rec = store.get_cooking_recipe(str(out.get("result")))
        if rec:
            eng = _get_engine(request)
            tuning = _ensure_tuning(eng, store.meta())
            out["recipe"] = _enrich_cooking_recipe(rec.raw, name=rec.name, tuning=tuning)

    return out


def _prefab_report_from_mechanism(prefab_id: str, row: Dict[str, Any]) -> Dict[str, Any]:
    comps = sorted({str(x) for x in (row.get("components") or []) if x})
    tags = sorted({str(x) for x in (row.get("tags") or []) if x})
    brains = sorted({str(x) for x in (row.get("brains") or []) if x})
    stategraphs = sorted({str(x) for x in (row.get("stategraphs") or []) if x})
    helpers = sorted({str(x) for x in (row.get("helpers") or []) if x})
    files = sorted({str(x) for x in (row.get("files") or []) if x})
    return {
        "type": "prefab",
        "prefab_name": prefab_id,
        "components": comps,
        "tags": tags,
        "brain": brains[0] if brains else None,
        "stategraph": stategraphs[0] if stategraphs else None,
        "brains": brains,
        "stategraphs": stategraphs,
        "helpers": helpers,
        "files": files,
        "events": [],
        "assets": [],
        "source": "mechanism_index",
    }


@router.get("/analyze/prefab/{name}")
def analyze_prefab(name: str, request: Request, mode: Optional[str] = None):
    """Return a prefab analysis report (mechanism index preferred, runtime optional)."""
    q = (name or "").strip()
    if not q:
        raise HTTPException(status_code=400, detail="empty name")

    mode_norm = str(mode or "").strip().lower()
    eng = getattr(request.app.state, "engine", None)
    mstore = get_mechanism_store(request)

    def _from_mechanism() -> Optional[Dict[str, Any]]:
        if mstore is None:
            return None
        pid = q
        row = mstore.get_prefab(pid)
        if row is None and pid.lower() != pid:
            pid = pid.lower()
            row = mstore.get_prefab(pid)
        if not row:
            return None
        report = _prefab_report_from_mechanism(str(row.get("id") or pid), row)
        files = report.get("files") or []
        path = files[0] if files else None
        return {"query": q, "path": path, "report": report, "mode": "mechanism"}

    def _from_engine() -> Dict[str, Any]:
        if eng is None:
            raise HTTPException(status_code=400, detail="analyzer disabled (no scripts source mounted)")
        try:
            path = eng.find_file(q, fuzzy=True)
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))
        if not path:
            raise HTTPException(status_code=404, detail=f"file not found for: {q}")

        content = eng.read_file(path)
        if content is None:
            raise HTTPException(status_code=404, detail=f"cannot read: {path}")

        from core.parsers import LuaAnalyzer

        rep = LuaAnalyzer(content, path=path).get_report()
        return {"query": q, "path": path, "report": rep, "mode": "runtime"}

    if mode_norm in {"mechanism", "index"}:
        rep = _from_mechanism()
        if rep is not None:
            return rep
        if mstore is None:
            raise HTTPException(status_code=400, detail="mechanism index not available")
        raise HTTPException(status_code=404, detail=f"prefab not found: {q}")

    if mode_norm in {"runtime", "engine", "analyzer"}:
        return _from_engine()

    rep = _from_mechanism()
    if rep is not None:
        return rep
    if eng is not None:
        return _from_engine()
    raise HTTPException(status_code=400, detail="prefab analysis unavailable (no mechanism index or analyzer)")
```

### File: apps/webcraft/app.py
- mode: full
- size_bytes: 9336
- sha256_12: 0419bf41fdfd

```py
# -*- coding: utf-8 -*-
from __future__ import annotations

import sqlite3
from pathlib import Path
from typing import Optional, Sequence

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles

from .api import router as api_router
from .catalog_store import CatalogStore
from .icon_service import IconConfig, IconService
from .i18n_index import I18nIndexStore
from .mechanism_store import MechanismStore
from .settings import WebCraftSettings
from .tuning_trace import TuningTraceStore
from .ui import render_index_html, render_cooking_html, render_cooking_tools_html, render_catalog_html


def create_app(
    catalog_path: Path,
    *,
    root_path: str = "",
    cors_allow_origins: Optional[Sequence[str]] = None,
    gzip_minimum_size: int = 800,
    auto_reload_catalog: bool = False,
    # icons
    icons_mode: str = "auto",
    game_data_dir: Optional[Path] = None,
    icons_unpremultiply: bool = True,
    static_root_dir: Optional[Path] = None,
    # analyzer (optional)
    enable_analyzer: bool = False,
    analyzer_load_db: bool = False,
    dst_root: Optional[Path] = None,
    scripts_zip: Optional[Path] = None,
    scripts_dir: Optional[Path] = None,
    # tuning trace (optional)
    tuning_trace_path: Optional[Path] = None,
    auto_reload_tuning_trace: bool = False,
    # i18n index (optional)
    i18n_index_path: Optional[Path] = None,
    auto_reload_i18n_index: bool = False,
    # mechanism index (optional)
    mechanism_path: Optional[Path] = None,
    auto_reload_mechanism: bool = False,
) -> FastAPI:
    """FastAPI app factory."""

    def _is_sqlite_path(path: Path) -> bool:
        return path.suffix.lower() in (".sqlite", ".sqlite3", ".db")

    def _has_tuning_trace_table(path: Path) -> bool:
        if not _is_sqlite_path(path) or not path.exists():
            return False
        try:
            conn = sqlite3.connect(str(path))
            cur = conn.cursor()
            tables = {row[0] for row in cur.execute("SELECT name FROM sqlite_master WHERE type='table'")}
            return "tuning_trace" in tables
        except Exception:
            return False
        finally:
            try:
                conn.close()
            except Exception:
                pass

    rp = WebCraftSettings.normalize_root_path(root_path)

    app = FastAPI(
        title="Wagstaff WebCraft API",
        version="1.0",
        root_path=rp,
        docs_url="/docs",
        redoc_url=None,
    )

    # static: app assets vs data outputs
    project_root = Path(__file__).resolve().parents[2]
    app_static_root = project_root / "apps" / "webcraft" / "static"
    data_static_root = Path(static_root_dir) if static_root_dir else (project_root / "data" / "static")
    try:
        app_static_root.mkdir(parents=True, exist_ok=True)
        data_static_root.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass
    app.mount("/static/app", StaticFiles(directory=str(app_static_root), check_dir=False), name="static-app")
    app.mount("/static/data", StaticFiles(directory=str(data_static_root), check_dir=False), name="static-data")

    # state
    app.state.store = CatalogStore(Path(catalog_path))

    # tuning trace (separate from catalog)
    ttp = Path(tuning_trace_path) if tuning_trace_path else None
    if ttp is None:
        catalog_sqlite = app.state.store.path if _is_sqlite_path(app.state.store.path) else None
        if catalog_sqlite and _has_tuning_trace_table(catalog_sqlite):
            ttp = catalog_sqlite
        else:
            ttp = Path(catalog_path).parent / "wagstaff_tuning_trace_v1.json"
    app.state.tuning_trace_path = ttp
    if ttp.exists():
        app.state.tuning_trace_store = TuningTraceStore(ttp)
    else:
        app.state.tuning_trace_store = None
    app.state.auto_reload_tuning_trace = bool(auto_reload_tuning_trace or auto_reload_catalog)

    # i18n index (separate from catalog)
    iip = Path(i18n_index_path) if i18n_index_path else (Path(catalog_path).parent / "wagstaff_i18n_v1.json")
    app.state.i18n_index_path = iip
    if iip.exists():
        app.state.i18n_index_store = I18nIndexStore(iip)
    else:
        app.state.i18n_index_store = None
    app.state.auto_reload_i18n_index = bool(auto_reload_i18n_index or auto_reload_catalog)

    # mechanism index (separate from catalog)
    mp = Path(mechanism_path) if mechanism_path else (Path(catalog_path).parent / "wagstaff_mechanism_index_v1.json")
    mp = MechanismStore.resolve_path(mp)
    app.state.mechanism_path = mp
    if mp.exists():
        app.state.mechanism_store = MechanismStore(mp)
    else:
        app.state.mechanism_store = None
    app.state.auto_reload_mechanism = bool(auto_reload_mechanism or auto_reload_catalog)

    # analyzer (auto-on if scripts_zip hint is available)
    scripts_zip_hint = None
    scripts_dir_hint = None
    try:
        scripts_zip_hint = str((app.state.store.meta() or {}).get("scripts_zip") or "").strip() or None
    except Exception:
        scripts_zip_hint = None
    try:
        scripts_dir_hint = str((app.state.store.meta() or {}).get("scripts_dir") or "").strip() or None
    except Exception:
        scripts_dir_hint = None

    scripts_zip_arg = scripts_zip
    scripts_dir_arg = scripts_dir
    dst_root_arg = dst_root
    enable_analyzer_arg = bool(enable_analyzer)

    if (not enable_analyzer_arg) and (not dst_root_arg) and (not scripts_zip_arg) and (not scripts_dir_arg):
        if scripts_zip_hint and Path(scripts_zip_hint).exists():
            scripts_zip_arg = Path(scripts_zip_hint)
            enable_analyzer_arg = True
        elif scripts_dir_hint and Path(scripts_dir_hint).exists():
            scripts_dir_arg = Path(scripts_dir_hint)
            enable_analyzer_arg = True

    # optional live analyzer (mount scripts source for on-demand parsing)
    app.state.engine = None
    if enable_analyzer_arg or dst_root_arg or scripts_zip_arg or scripts_dir_arg:
        try:
            from core.engine import WagstaffEngine
            app.state.engine = WagstaffEngine(
                load_db=bool(analyzer_load_db),
                silent=True,
                dst_root=str(dst_root_arg) if dst_root_arg else None,
                scripts_zip=str(scripts_zip_arg) if scripts_zip_arg else None,
                scripts_dir=str(scripts_dir_arg) if scripts_dir_arg else None,
            )
        except Exception:
            app.state.engine = None
    app.state.auto_reload_catalog = bool(auto_reload_catalog)

    # i18n index only (runtime PO parsing disabled)
    app.state.i18n_service = None

    icons_dir = data_static_root / "icons"
    try:
        icons_dir.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass

    cfg = IconConfig(
        mode=str(icons_mode or "auto"),
        static_dir=icons_dir,
        game_data_dir=(Path(game_data_dir).expanduser().resolve() if game_data_dir else None),
        unpremultiply=bool(icons_unpremultiply),
    )
    app.state.icon_service = IconService(cfg)

    # middleware
    if gzip_minimum_size and gzip_minimum_size > 0:
        app.add_middleware(GZipMiddleware, minimum_size=int(gzip_minimum_size))

    if cors_allow_origins:
        app.add_middleware(
            CORSMiddleware,
            allow_origins=list(cors_allow_origins),
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )

    # routes
    app.include_router(api_router)

    @app.on_event("shutdown")
    def _shutdown() -> None:
        eng = getattr(app.state, "engine", None)
        if eng is not None:
            try:
                eng.close()
            except Exception:
                pass

    @app.get("/healthz")
    def healthz():
        return {"ok": True}

    @app.get("/", response_class=HTMLResponse)
    def index(request: Request):
        # root_path is already applied by FastAPI; still need it for frontend URL prefixing
        root = request.scope.get("root_path") or ""
        return HTMLResponse(render_cooking_tools_html(app_root=str(root)))

    @app.get("/craft", response_class=HTMLResponse)
    def craft(request: Request):
        root = request.scope.get("root_path") or ""
        return HTMLResponse(render_index_html(app_root=str(root)))

    @app.get("/cooking", response_class=HTMLResponse)
    def cooking(request: Request):
        root = request.scope.get("root_path") or ""
        return HTMLResponse(render_cooking_html(app_root=str(root)))

    @app.get("/cooking/explore", response_class=HTMLResponse)
    def cooking_explore(request: Request):
        root = request.scope.get("root_path") or ""
        return HTMLResponse(render_cooking_tools_html(app_root=str(root)))

    @app.get("/cooking/simulate", response_class=HTMLResponse)
    def cooking_simulate(request: Request):
        root = request.scope.get("root_path") or ""
        return HTMLResponse(render_cooking_tools_html(app_root=str(root)))

    @app.get("/catalog", response_class=HTMLResponse)
    def catalog(request: Request):
        root = request.scope.get("root_path") or ""
        return HTMLResponse(render_catalog_html(app_root=str(root)))

    return app
```

### File: apps/webcraft/catalog_store.py
- mode: full
- size_bytes: 59435
- sha256_12: c4146e6d324a

```py
# -*- coding: utf-8 -*-
from __future__ import annotations

import json
import sqlite3
import threading
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple


def _dedup_preserve_order(items: Iterable[str]) -> List[str]:
    out: List[str] = []
    seen = set()
    for x in items:
        if not x:
            continue
        if x in seen:
            continue
        out.append(x)
        seen.add(x)
    return out


_SQLITE_SUFFIXES = (".sqlite", ".sqlite3", ".db")


def _is_sqlite_path(path: Path) -> bool:
    return path.suffix.lower() in _SQLITE_SUFFIXES


def _find_sqlite_peer(path: Path) -> Optional[Path]:
    if path.suffix.lower() != ".json":
        return None
    for ext in _SQLITE_SUFFIXES:
        candidate = path.with_suffix(ext)
        if candidate.exists():
            return candidate
    return None


@dataclass
class CraftRecipe:
    name: str
    product: Optional[str]
    tab: str
    tech: str
    filters: List[str]
    builder_tags: List[str]
    builder_skill: Optional[str]
    station_tag: Optional[str]
    ingredients: List[Dict[str, Any]]
    ingredients_unresolved: List[str]
    raw: Dict[str, Any]


@dataclass
class CookingRecipe:
    """Cookpot cooking recipe (preparedfoods).

    Notes
    - Currently backed by catalog['cooking'] entries.
    - `card_ingredients` may be missing for many recipes (legacy limitation).
    """

    name: str
    priority: float
    weight: float
    foodtype: Optional[str]
    hunger: Any
    health: Any
    sanity: Any
    perishtime: Any
    cooktime: Any
    tags: List[str]
    card_ingredients: List[Tuple[str, float]]
    raw: Dict[str, Any]


class CatalogError(RuntimeError):
    pass


COOKING_TAG_KEYS = {
    "meat",
    "monster",
    "fish",
    "egg",
    "dairy",
    "sweetener",
    "fruit",
    "veggie",
    "vegetable",
    "inedible",
    "fungus",
    "mushroom",
    "frozen",
    "seed",
    "fat",
    "magic",
}

COOKING_TAG_HINTS = {
    "meat": ["meat", "leafymeat"],
    "monster": ["monstermeat", "durian"],
    "fish": ["fish", "eel", "salmon", "tuna", "perch", "trout", "barnacle"],
    "egg": ["bird_egg", "tallbirdegg", "egg"],
    "dairy": ["goatmilk", "milk", "butter", "cheese"],
    "sweetener": ["honey", "sugar", "nectar", "syrup", "maplesyrup"],
    "fruit": ["berries", "berry", "banana", "pomegranate", "watermelon", "dragonfruit", "durian", "fig", "cave_banana"],
    "veggie": ["carrot", "corn", "pumpkin", "eggplant", "pepper", "potato", "tomato", "onion", "garlic", "asparagus", "cactus", "kelp"],
    "fungus": ["mushroom", "cap"],
    "inedible": ["twigs"],
    "frozen": ["ice"],
    "seed": ["seed"],
    "fat": ["butter", "goatmilk", "milk", "cheese"],
    "magic": ["mandrake", "nightmarefuel", "glommerfuel"],
}

COOKING_SMALL_MEAT = ["morsel", "smallmeat", "drumstick", "froglegs", "batwing"]


def normalize_cooking_tags(raw: Any) -> Dict[str, float]:
    out: Dict[str, float] = {}
    if isinstance(raw, dict):
        for k, v in raw.items():
            key = str(k or "").strip().lower()
            if not key:
                continue
            if key not in COOKING_TAG_KEYS:
                continue
            try:
                out[key] = float(v)
            except Exception:
                continue
        return out
    if isinstance(raw, (list, tuple, set)):
        for k in raw:
            key = str(k or "").strip().lower()
            if not key or key not in COOKING_TAG_KEYS:
                continue
            out[key] = 1.0
    return out


def guess_cooking_tags(item_id: str, item: Optional[Dict[str, Any]] = None) -> Dict[str, float]:
    iid = str(item_id or "").strip().lower()
    if not iid:
        return {}
    tags = normalize_cooking_tags((item or {}).get("tags") if item else None)

    if "eggplant" in iid:
        tags.setdefault("veggie", 1.0)
    for key in COOKING_SMALL_MEAT:
        if key in iid:
            tags["meat"] = min(tags.get("meat", 1.0), 0.5)
            break

    for tag, hints in COOKING_TAG_HINTS.items():
        if tag == "egg" and "eggplant" in iid:
            continue
        if any(h in iid for h in hints):
            tags.setdefault(tag, 1.0)

    return tags


class CatalogStore:
    """Load + index wagstaff catalog for fast queries (thread-safe).

    Data source:
      - data/index/wagstaff_catalog_v2.json
      - data/index/wagstaff_catalog_v2.sqlite

    This layer is intentionally independent from wiki/cli layers. It should be safe
    to reuse from:
      - CLI (wiki)
      - Web (webcraft)
      - future GUI (desktop)
    """

    def __init__(self, catalog_path: Path):
        resolved = self._resolve_catalog_path(Path(catalog_path))
        self._path = resolved
        self._use_sqlite = _is_sqlite_path(self._path)
        self._lock = threading.RLock()
        self._mtime: float = -1.0
        self._catalog_fts_available: Optional[bool] = None
        self._catalog_fts_mtime: float = -1.0
        self._icon_index_mtime: float = -1.0
        self._icon_index: Dict[str, str] = {}
        self._icon_index_path: Path = self._path.parent / "wagstaff_icon_index_v1.json"
        self._catalog_index_path: Path = self._path.parent / "wagstaff_catalog_index_v1.json"
        self._catalog_index_mtime: float = -1.0
        self._catalog_index_items: List[Dict[str, Any]] = []
        self._catalog_index_total: int = 0

        self._doc: Dict[str, Any] = {}
        self._meta: Dict[str, Any] = {}

        # presentation mapping (id -> {name, atlas, image, ...})
        self._assets: Dict[str, Any] = {}
        self._items: Dict[str, Dict[str, Any]] = {}
        self._item_ids: List[str] = []
        self._by_kind: Dict[str, List[str]] = {}
        self._by_category: Dict[str, List[str]] = {}
        self._by_behavior: Dict[str, List[str]] = {}
        self._by_source: Dict[str, List[str]] = {}
        self._by_component: Dict[str, List[str]] = {}
        self._by_tag_item: Dict[str, List[str]] = {}
        self._by_slot: Dict[str, List[str]] = {}

        # craft
        self._recipes: Dict[str, CraftRecipe] = {}
        self._aliases: Dict[str, str] = {}
        self._filter_defs: List[Dict[str, Any]] = []
        self._filter_order: List[str] = []

        # indexes (craft)
        self._by_filter: Dict[str, List[str]] = {}
        self._by_tab: Dict[str, List[str]] = {}
        self._by_tag: Dict[str, List[str]] = {}
        self._by_ingredient: Dict[str, List[str]] = {}
        self._by_product: Dict[str, List[str]] = {}

        # cooking
        self._cooking: Dict[str, CookingRecipe] = {}
        self._cook_by_tag: Dict[str, List[str]] = {}
        self._cook_by_foodtype: Dict[str, List[str]] = {}
        self._cook_by_ingredient: Dict[str, List[str]] = {}
        self._cooking_ingredients: Dict[str, Dict[str, Any]] = {}

        self.load(force=True)

    @staticmethod
    def _resolve_catalog_path(catalog_path: Path) -> Path:
        if _is_sqlite_path(catalog_path):
            return catalog_path
        peer = _find_sqlite_peer(catalog_path)
        return peer if peer else catalog_path

    @property
    def path(self) -> Path:
        return self._path

    def meta(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._meta)

    def schema_version(self) -> int:
        with self._lock:
            return int(self._doc.get("schema_version") or (self._meta or {}).get("schema") or 0)

    def mtime(self) -> float:
        with self._lock:
            return float(self._mtime or 0)

    def item_ids(self, include_icon_only: bool = False) -> List[str]:
        """Return known item ids (optionally including icon-only ids)."""
        with self._lock:
            ids = list(self._item_ids)
            if include_icon_only:
                self._ensure_icon_index()
                ids = _dedup_preserve_order(ids + list((self._icon_index or {}).keys()))
            return ids

    def get_item(self, item_id: str) -> Optional[Dict[str, Any]]:
        if not item_id:
            return None
        with self._lock:
            v = (self._items or {}).get(str(item_id))
            return dict(v) if isinstance(v, dict) else None

    # ----------------- presentation assets -----------------

    def assets(self, include_icon_only: bool = False) -> Dict[str, Any]:
        """Return presentation assets mapping (shallow copy).

        If include_icon_only=True, merge in icon-index-only entries (name=id) so the
        catalog page can list/search more ids even when catalog assets are sparse.
        """
        with self._lock:
            base = dict(self._assets or {})
            if include_icon_only:
                self._ensure_icon_index()
                for k, png in (self._icon_index or {}).items():
                    if k in base:
                        if png and isinstance(base.get(k), dict):
                            base[k].setdefault("icon", png)
                            base[k].setdefault("image", png)
                        continue
                    base[k] = {"name": k, "icon": png, "image": png, "icon_only": True}
            return base

    def get_asset(self, item_id: str) -> Optional[Dict[str, Any]]:
        if not item_id:
            return None
        with self._lock:
            v = (self._assets or {}).get(str(item_id))
            if not v:
                v = (self._items or {}).get(str(item_id), {}).get("assets")
            return dict(v) if isinstance(v, dict) else None

    # ----------------- load / reload -----------------

    def load(self, force: bool = False) -> bool:
        """Load catalog if changed.

        Returns True if reload occurred.
        """
        with self._lock:
            try:
                mtime = self._path.stat().st_mtime
            except FileNotFoundError as e:
                raise CatalogError(f"Catalog file not found: {self._path}") from e

            if (not force) and self._doc and self._mtime == mtime:
                return False

            doc = self._load_doc()
            self._validate(doc)

            self._doc = doc
            self._meta = doc.get("meta") or {}
            self._mtime = mtime

            self._build_indexes(doc)
            self._load_icon_index_if_stale(force=force)
            self._load_catalog_index_if_stale(force=force)
            return True

    def _load_doc(self) -> Dict[str, Any]:
        if self._use_sqlite:
            return self._load_doc_from_sqlite(self._path)
        return json.loads(self._path.read_text(encoding="utf-8"))

    def _load_doc_from_sqlite(self, path: Path) -> Dict[str, Any]:
        try:
            conn = sqlite3.connect(str(path))
            conn.row_factory = sqlite3.Row
        except Exception as exc:
            raise CatalogError(f"Failed to open SQLite catalog: {path}") from exc
        try:
            cur = conn.cursor()
            tables = {row["name"] for row in cur.execute("SELECT name FROM sqlite_master WHERE type='table'")}
            if "meta" not in tables or "items" not in tables:
                raise CatalogError("SQLite catalog missing required tables")

            meta_rows = {row["key"]: row["value"] for row in cur.execute("SELECT key, value FROM meta")}

            def _table_columns(name: str) -> set[str]:
                try:
                    return {row["name"] for row in cur.execute(f"PRAGMA table_info({name})")}
                except Exception:
                    return set()

            def _pick_payload(table: str) -> str:
                cols = _table_columns(table)
                if "raw_json" in cols:
                    return "raw_json"
                if "data" in cols:
                    return "data"
                return ""

            items_payload = _pick_payload("items")
            assets_payload = _pick_payload("assets")
            if not items_payload or not assets_payload:
                raise CatalogError("SQLite catalog missing payload columns")

            items_rows = cur.execute(f"SELECT id, {items_payload} AS payload FROM items").fetchall()
            assets_rows = cur.execute(f"SELECT id, {assets_payload} AS payload FROM assets").fetchall()

            craft: Dict[str, Any] = {}
            craft_recipes: Dict[str, Any] = {}
            if "craft_meta" in tables:
                craft_meta_rows = cur.execute("SELECT key, value_json FROM craft_meta").fetchall()
                craft = {str(row["key"]): row["value_json"] for row in craft_meta_rows}
            if "craft_recipes" in tables:
                craft_payload = _pick_payload("craft_recipes")
                if craft_payload:
                    craft_rows = cur.execute(
                        f"SELECT name, {craft_payload} AS payload FROM craft_recipes"
                    ).fetchall()
                    craft_recipes = {str(row["name"]): row["payload"] for row in craft_rows}
            if craft or craft_recipes:
                craft.setdefault("recipes", craft_recipes)
            elif "craft" in tables:
                craft_rows = cur.execute("SELECT key, data FROM craft").fetchall()
                craft = {str(row["key"]): row["data"] for row in craft_rows}

            cooking: Dict[str, Any] = {}
            if "cooking_recipes" in tables:
                cooking_payload = _pick_payload("cooking_recipes")
                if cooking_payload:
                    cooking_rows = cur.execute(
                        f"SELECT name, {cooking_payload} AS payload FROM cooking_recipes"
                    ).fetchall()
                    cooking = {str(row["name"]): row["payload"] for row in cooking_rows}
            elif "cooking" in tables:
                cooking_rows = cur.execute("SELECT name, data FROM cooking").fetchall()
                cooking = {str(row["name"]): row["data"] for row in cooking_rows}

            cooking_ingredients: Dict[str, Any] = {}
            if "cooking_ingredients" in tables:
                cooking_payload = _pick_payload("cooking_ingredients")
                if cooking_payload:
                    cooking_rows = cur.execute(
                        f"SELECT item_id, {cooking_payload} AS payload FROM cooking_ingredients"
                    ).fetchall()
                    cooking_ingredients = {str(row["item_id"]): row["payload"] for row in cooking_rows}
        except Exception as exc:
            raise CatalogError(f"SQLite catalog missing tables: {path}") from exc
        finally:
            conn.close()

        def _load_json(value: Any) -> Any:
            if value is None:
                return None
            try:
                return json.loads(value)
            except Exception:
                return value

        meta_obj = _load_json(meta_rows.get("meta")) or {}
        stats_obj = _load_json(meta_rows.get("stats")) or {}
        schema_version = _load_json(meta_rows.get("schema_version"))
        if schema_version is None:
            schema_version = (meta_obj or {}).get("schema") or 0

        items = {str(row["id"]): _load_json(row["payload"]) for row in items_rows}
        assets = {str(row["id"]): _load_json(row["payload"]) for row in assets_rows}

        if craft:
            craft = {str(k): _load_json(v) for k, v in craft.items()}
        if craft_recipes:
            craft["recipes"] = {str(k): _load_json(v) for k, v in craft_recipes.items()}
        if cooking:
            cooking = {str(k): _load_json(v) for k, v in cooking.items()}
        if cooking_ingredients:
            cooking_ingredients = {str(k): _load_json(v) for k, v in cooking_ingredients.items()}

        return {
            "schema_version": schema_version,
            "meta": meta_obj,
            "items": items,
            "assets": assets,
            "craft": craft,
            "cooking": cooking,
            "cooking_ingredients": cooking_ingredients,
            "stats": stats_obj,
        }

    def _validate(self, doc: Dict[str, Any]) -> None:
        if not isinstance(doc, dict):
            raise CatalogError("Catalog root must be a JSON object")
        if "meta" not in doc:
            raise CatalogError("Catalog missing key: meta")

        if "assets" in doc and not isinstance(doc.get("assets"), dict):
            raise CatalogError("Catalog assets must be an object")

        schema = int(doc.get("schema_version") or (doc.get("meta") or {}).get("schema") or 0)
        if schema < 2:
            raise CatalogError("Catalog schema must be >=2")
        if "items" not in doc or not isinstance(doc.get("items"), dict):
            raise CatalogError("Catalog items must be an object")

        if "craft" not in doc:
            raise CatalogError("Catalog missing key: craft")
        craft = doc.get("craft") or {}
        if "recipes" not in craft or not isinstance(craft.get("recipes"), dict):
            raise CatalogError("Catalog craft.recipes must be an object")

        # cooking is optional, but if present it must be an object.
        if "cooking" in doc and not isinstance(doc.get("cooking"), dict):
            raise CatalogError("Catalog cooking must be an object")
        if "cooking_ingredients" in doc and not isinstance(doc.get("cooking_ingredients"), dict):
            raise CatalogError("Catalog cooking_ingredients must be an object")

    def _build_indexes(self, doc: Dict[str, Any]) -> None:
        assets_obj = doc.get("assets") or {}
        items_obj = doc.get("items") or {}

        items_out: Dict[str, Dict[str, Any]] = {}
        assets_out: Dict[str, Dict[str, Any]] = {}

        if isinstance(items_obj, dict) and items_obj:
            for iid, raw in items_obj.items():
                if not iid:
                    continue
                if isinstance(raw, dict):
                    item = dict(raw)
                else:
                    item = {"id": iid}
                item_id = str(item.get("id") or iid).strip()
                if not item_id:
                    continue
                item["id"] = item_id
                items_out[item_id] = item

        if isinstance(assets_obj, dict):
            for iid, raw in assets_obj.items():
                if not iid or not isinstance(raw, dict):
                    continue
                assets_out[str(iid)] = dict(raw)

        # merge per-item assets into assets_out
        for iid, item in items_out.items():
            a = item.get("assets")
            if not isinstance(a, dict):
                continue
            merged = dict(assets_out.get(iid) or {})
            for k, v in a.items():
                if k not in merged or merged.get(k) in (None, "", []):
                    merged[k] = v
            if merged:
                assets_out[iid] = merged

        self._items = items_out
        self._assets = assets_out

        # ---- item indexes ----
        by_kind: Dict[str, List[str]] = {}
        by_category: Dict[str, List[str]] = {}
        by_behavior: Dict[str, List[str]] = {}
        by_source: Dict[str, List[str]] = {}
        by_component: Dict[str, List[str]] = {}
        by_tag: Dict[str, List[str]] = {}
        by_slot: Dict[str, List[str]] = {}

        def _as_list(val: Any) -> List[str]:
            if isinstance(val, str):
                return [val]
            if isinstance(val, (list, tuple, set)):
                return [str(x) for x in val if x]
            return []

        def _push(bucket: Dict[str, List[str]], key: Optional[str], iid: str) -> None:
            if not key:
                return
            bucket.setdefault(str(key), []).append(iid)

        for iid, item in items_out.items():
            kind = item.get("kind")
            if kind:
                _push(by_kind, str(kind), iid)
            for cat in _as_list(item.get("categories")):
                _push(by_category, cat, iid)
            for beh in _as_list(item.get("behaviors")):
                _push(by_behavior, beh, iid)
            for src in _as_list(item.get("sources")):
                _push(by_source, src, iid)
            for comp in _as_list(item.get("components")):
                _push(by_component, comp, iid)
            for tag in _as_list(item.get("tags")):
                _push(by_tag, tag, iid)
            for slot in _as_list(item.get("slots")):
                _push(by_slot, slot, iid)

        for bucket in (by_kind, by_category, by_behavior, by_source, by_component, by_tag, by_slot):
            for k in list(bucket.keys()):
                bucket[k] = sorted(_dedup_preserve_order(bucket[k]))

        self._item_ids = sorted(items_out.keys())
        self._by_kind = by_kind
        self._by_category = by_category
        self._by_behavior = by_behavior
        self._by_source = by_source
        self._by_component = by_component
        self._by_tag_item = by_tag
        self._by_slot = by_slot

        # ---- craft ----
        craft = doc.get("craft") or {}
        recipes_obj: Dict[str, Any] = craft.get("recipes") or {}
        aliases: Dict[str, str] = craft.get("aliases") or {}
        filter_defs: List[Dict[str, Any]] = craft.get("filter_defs") or []
        filter_order: List[str] = craft.get("filter_order") or []

        recipes: Dict[str, CraftRecipe] = {}
        for name, raw in recipes_obj.items():
            if not isinstance(raw, dict):
                continue

            btags = raw.get("builder_tags") or []
            if isinstance(btags, str):
                btags = [btags]
            btags = [str(x) for x in btags if x]

            rec = CraftRecipe(
                name=str(raw.get("name") or name),
                product=(raw.get("product") or None),
                tab=str(raw.get("tab") or "UNKNOWN"),
                tech=str(raw.get("tech") or "UNKNOWN"),
                filters=[str(x) for x in (raw.get("filters") or []) if x],
                builder_tags=btags,
                builder_skill=(raw.get("builder_skill") or None),
                station_tag=(raw.get("station_tag") or None),
                ingredients=list(raw.get("ingredients") or []),
                ingredients_unresolved=list(raw.get("ingredients_unresolved") or []),
                raw=raw,
            )
            recipes[rec.name] = rec

        # indexes
        by_filter: Dict[str, List[str]] = {}
        by_tab: Dict[str, List[str]] = {}
        by_tag: Dict[str, List[str]] = {}
        by_ing: Dict[str, List[str]] = {}
        by_product: Dict[str, List[str]] = {}

        for rec in recipes.values():
            if rec.product:
                by_product.setdefault(str(rec.product), []).append(rec.name)

            for f in rec.filters:
                by_filter.setdefault(f, []).append(rec.name)

            if rec.tab:
                by_tab.setdefault(rec.tab, []).append(rec.name)

            for t in rec.builder_tags:
                by_tag.setdefault(t, []).append(rec.name)

            for ing in rec.ingredients:
                item = str(ing.get("item") or "").strip()
                if not item:
                    continue
                by_ing.setdefault(item, []).append(rec.name)

        for bucket in (by_filter, by_tab, by_tag, by_ing, by_product):
            for k in list(bucket.keys()):
                bucket[k] = sorted(_dedup_preserve_order(bucket[k]))

        self._recipes = recipes
        self._aliases = {str(k): str(v) for k, v in aliases.items() if k and v}
        self._filter_defs = list(filter_defs)
        self._filter_order = list(filter_order)

        self._by_filter = by_filter
        self._by_tab = by_tab
        self._by_tag = by_tag
        self._by_ingredient = by_ing
        self._by_product = by_product

        # ---- cooking ----
        self._build_cooking_indexes(doc.get("cooking") or {})
        self._build_cooking_ingredient_indexes(doc.get("cooking_ingredients") or {})

    def _build_cooking_indexes(self, cooking_obj: Dict[str, Any]) -> None:
        recipes: Dict[str, CookingRecipe] = {}
        by_tag: Dict[str, List[str]] = {}
        by_ft: Dict[str, List[str]] = {}
        by_ing: Dict[str, List[str]] = {}

        if not isinstance(cooking_obj, dict):
            cooking_obj = {}

        for name, raw in cooking_obj.items():
            if not isinstance(raw, dict):
                continue

            tags = raw.get("tags") or []
            if isinstance(tags, str):
                tags = [tags]
            tags = [str(x) for x in tags if x]

            ci_raw = raw.get("card_ingredients") or []
            card_ings: List[Tuple[str, float]] = []
            if isinstance(ci_raw, list):
                for row in ci_raw:
                    if not isinstance(row, (list, tuple)) or len(row) < 2:
                        continue
                    item = str(row[0]).strip()
                    if not item:
                        continue
                    try:
                        cnt = float(row[1])
                    except Exception:
                        continue
                    if cnt <= 0:
                        continue
                    card_ings.append((item, cnt))

            try:
                priority = float(raw.get("priority", 0))
            except Exception:
                priority = 0.0
            try:
                weight = float(raw.get("weight", 1))
            except Exception:
                weight = 1.0

            ft = raw.get("foodtype")
            foodtype = str(ft).strip() if ft else None

            rec = CookingRecipe(
                name=str(name),
                priority=priority,
                weight=weight,
                foodtype=foodtype,
                hunger=raw.get("hunger"),
                health=raw.get("health"),
                sanity=raw.get("sanity"),
                perishtime=raw.get("perishtime"),
                cooktime=raw.get("cooktime"),
                tags=tags,
                card_ingredients=card_ings,
                raw=raw,
            )
            recipes[rec.name] = rec

            if rec.foodtype:
                by_ft.setdefault(rec.foodtype, []).append(rec.name)

            for t in rec.tags:
                by_tag.setdefault(t, []).append(rec.name)

            for item, _ in rec.card_ingredients:
                by_ing.setdefault(item, []).append(rec.name)

        for bucket in (by_tag, by_ft, by_ing):
            for k in list(bucket.keys()):
                bucket[k] = sorted(_dedup_preserve_order(bucket[k]))

        self._cooking = recipes
        self._cook_by_tag = by_tag
        self._cook_by_foodtype = by_ft
        self._cook_by_ingredient = by_ing

    def _build_cooking_ingredient_indexes(self, cooking_obj: Dict[str, Any]) -> None:
        items: Dict[str, Dict[str, Any]] = {}
        if not isinstance(cooking_obj, dict):
            cooking_obj = {}
        for item_id, raw in cooking_obj.items():
            if not item_id or not isinstance(raw, dict):
                continue
            iid = str(item_id).strip()
            if not iid:
                continue
            item = dict(raw)
            item.setdefault("id", iid)
            items[iid] = item
        self._cooking_ingredients = items

    # ----------------- helpers -----------------

    def _load_icon_index_if_stale(self, force: bool = False) -> None:
        path = self._icon_index_path
        try:
            mtime = path.stat().st_mtime
        except Exception:
            return
        if (not force) and self._icon_index and self._icon_index_mtime == mtime:
            return
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
            icons = data.get("icons") or {}
            mp: Dict[str, str] = {}
            for k, v in icons.items():
                if not k or not isinstance(k, str):
                    continue
                if isinstance(v, dict) and v.get("png"):
                    mp[k] = str(v.get("png"))
            self._icon_index = mp
            self._icon_index_mtime = mtime
        except Exception:
            return

    def _load_catalog_index_if_stale(self, force: bool = False) -> None:
        if self._use_sqlite:
            self._load_catalog_index_from_sqlite(force=force)
            return

        path = self._catalog_index_path
        try:
            mtime = path.stat().st_mtime
        except Exception:
            self._catalog_index_items = []
            self._catalog_index_total = 0
            self._catalog_index_mtime = -1.0
            return
        if (not force) and self._catalog_index_items and self._catalog_index_mtime == mtime:
            return
        try:
            data = json.loads(path.read_text(encoding="utf-8"))
            items = data.get("items") if isinstance(data, dict) else None
            if isinstance(items, list):
                out: List[Dict[str, Any]] = []
                for row in items:
                    if not isinstance(row, dict):
                        continue
                    iid = str(row.get("id") or "").strip()
                    if not iid:
                        continue
                    out.append(dict(row))
                out.sort(key=lambda x: x.get("id") or "")
                self._catalog_index_items = out
                self._catalog_index_total = len(out)
            else:
                self._catalog_index_items = []
                self._catalog_index_total = 0
            self._catalog_index_mtime = mtime
        except Exception:
            return

    def _load_catalog_index_from_sqlite(self, force: bool = False) -> None:
        try:
            mtime = self._path.stat().st_mtime
        except Exception:
            self._catalog_index_items = []
            self._catalog_index_total = 0
            self._catalog_index_mtime = -1.0
            return
        if (not force) and self._catalog_index_items and self._catalog_index_mtime == mtime:
            return
        try:
            conn = sqlite3.connect(str(self._path))
            conn.row_factory = sqlite3.Row
        except Exception:
            return
        try:
            cur = conn.cursor()
            cols = {row["name"] for row in cur.execute("PRAGMA table_info(catalog_index)")}
            cat_col = "categories_json" if "categories_json" in cols else "categories"
            beh_col = "behaviors_json" if "behaviors_json" in cols else "behaviors"
            src_col = "sources_json" if "sources_json" in cols else "sources"
            tag_col = "tags_json" if "tags_json" in cols else "tags"
            comp_col = "components_json" if "components_json" in cols else "components"
            slot_col = "slots_json" if "slots_json" in cols else "slots"
            rows = cur.execute(
                f"""
                SELECT id, name, icon, image, has_icon, icon_only, kind,
                       {cat_col} AS categories,
                       {beh_col} AS behaviors,
                       {src_col} AS sources,
                       {tag_col} AS tags,
                       {comp_col} AS components,
                       {slot_col} AS slots
                FROM catalog_index
                ORDER BY id
                """
            ).fetchall()
        except Exception:
            self._catalog_index_items = []
            self._catalog_index_total = 0
            self._catalog_index_mtime = mtime
            conn.close()
            return
        finally:
            conn.close()

        def _load_list(value: Any) -> List[str]:
            if value is None:
                return []
            if isinstance(value, list):
                return [str(x) for x in value if x]
            try:
                out = json.loads(value)
                if isinstance(out, list):
                    return [str(x) for x in out if x]
            except Exception:
                pass
            return []

        out: List[Dict[str, Any]] = []
        for row in rows:
            iid = str(row["id"] or "").strip()
            if not iid:
                continue
            out.append(
                {
                    "id": iid,
                    "name": row["name"],
                    "icon": row["icon"],
                    "image": row["image"],
                    "has_icon": bool(row["has_icon"]),
                    "icon_only": bool(row["icon_only"]),
                    "kind": row["kind"],
                    "categories": _load_list(row["categories"]),
                    "behaviors": _load_list(row["behaviors"]),
                    "sources": _load_list(row["sources"]),
                    "tags": _load_list(row["tags"]),
                    "components": _load_list(row["components"]),
                    "slots": _load_list(row["slots"]),
                }
            )
        self._catalog_index_items = out
        self._catalog_index_total = len(out)
        self._catalog_index_mtime = mtime

    def _has_catalog_fts(self) -> bool:
        if not self._use_sqlite:
            return False
        try:
            mtime = self._path.stat().st_mtime
        except Exception:
            mtime = -1.0
        if self._catalog_fts_available is not None and self._catalog_fts_mtime == mtime:
            return bool(self._catalog_fts_available)
        available = False
        try:
            conn = sqlite3.connect(str(self._path))
            cur = conn.cursor()
            row = cur.execute(
                "SELECT 1 FROM sqlite_master WHERE type='table' AND name='catalog_index_fts' LIMIT 1"
            ).fetchone()
            available = bool(row)
        except Exception:
            available = False
        finally:
            try:
                conn.close()
            except Exception:
                pass
        self._catalog_fts_available = available
        self._catalog_fts_mtime = mtime
        return available

    def _fetch_catalog_index_from_fts(self, words: List[str], *, limit: int) -> Optional[List[Dict[str, Any]]]:
        if not words:
            return None
        if not self._has_catalog_fts():
            return None

        terms = []
        for w in words:
            w = str(w or "").strip()
            if not w:
                continue
            w = w.replace('"', '""')
            terms.append(f'"{w}"')
        if not terms:
            return None
        query = " OR ".join(terms)

        try:
            conn = sqlite3.connect(str(self._path))
            conn.row_factory = sqlite3.Row
        except Exception:
            return None
        try:
            cur = conn.cursor()
            cols = {row["name"] for row in cur.execute("PRAGMA table_info(catalog_index)")}
            cat_col = "categories_json" if "categories_json" in cols else "categories"
            beh_col = "behaviors_json" if "behaviors_json" in cols else "behaviors"
            src_col = "sources_json" if "sources_json" in cols else "sources"
            tag_col = "tags_json" if "tags_json" in cols else "tags"
            comp_col = "components_json" if "components_json" in cols else "components"
            slot_col = "slots_json" if "slots_json" in cols else "slots"
            rows = cur.execute(
                f"""
                SELECT c.id, c.name, c.icon, c.image, c.has_icon, c.icon_only, c.kind,
                       c.{cat_col} AS categories,
                       c.{beh_col} AS behaviors,
                       c.{src_col} AS sources,
                       c.{tag_col} AS tags,
                       c.{comp_col} AS components,
                       c.{slot_col} AS slots
                FROM catalog_index_fts fts
                JOIN catalog_index c ON c.rowid = fts.rowid
                WHERE catalog_index_fts MATCH ?
                LIMIT ?
                """,
                (query, int(limit)),
            ).fetchall()
        except Exception:
            return None
        finally:
            conn.close()

        def _load_list(value: Any) -> List[str]:
            if value is None:
                return []
            if isinstance(value, list):
                return [str(x) for x in value if x]
            try:
                out = json.loads(value)
                if isinstance(out, list):
                    return [str(x) for x in out if x]
            except Exception:
                pass
            return []

        out: List[Dict[str, Any]] = []
        for row in rows:
            iid = str(row["id"] or "").strip()
            if not iid:
                continue
            out.append(
                {
                    "id": iid,
                    "name": row["name"],
                    "icon": row["icon"],
                    "image": row["image"],
                    "has_icon": bool(row["has_icon"]),
                    "icon_only": bool(row["icon_only"]),
                    "kind": row["kind"],
                    "categories": _load_list(row["categories"]),
                    "behaviors": _load_list(row["behaviors"]),
                    "sources": _load_list(row["sources"]),
                    "tags": _load_list(row["tags"]),
                    "components": _load_list(row["components"]),
                    "slots": _load_list(row["slots"]),
                }
            )
        return out

    def _ensure_icon_index(self) -> None:
        self._load_icon_index_if_stale()

    def catalog_index(self) -> List[Dict[str, Any]]:
        """Compact catalog index for search/listing."""
        items: List[Dict[str, Any]] = []
        with self._lock:
            self._load_catalog_index_if_stale()
            if self._catalog_index_items:
                return list(self._catalog_index_items)
            self._ensure_icon_index()
            ids = list(self._item_ids)
            if not ids and self._assets:
                ids = list(self._assets.keys())
            if self._icon_index:
                ids = _dedup_preserve_order(ids + list(self._icon_index.keys()))

            for iid in ids:
                if not iid:
                    continue
                item = self._items.get(iid) or {}
                asset = (self._assets or {}).get(iid) or item.get("assets") or {}
                name = asset.get("name") or item.get("name") or iid
                icon = asset.get("icon") or asset.get("image") or (self._icon_index or {}).get(iid)
                items.append(
                    {
                        "id": iid,
                        "name": name,
                        "image": asset.get("image") or icon,
                        "icon": icon,
                        "has_icon": bool(icon),
                        "icon_only": bool(iid not in self._items),
                        "kind": item.get("kind"),
                        "categories": item.get("categories") or [],
                        "behaviors": item.get("behaviors") or [],
                        "sources": item.get("sources") or [],
                        "tags": item.get("tags") or [],
                        "components": item.get("components") or [],
                        "slots": item.get("slots") or [],
                    }
                )
        items.sort(key=lambda x: x["id"])
        with self._lock:
            self._catalog_index_items = items
            self._catalog_index_total = len(items)
        return list(items)

    def catalog_index_page(self, *, offset: int = 0, limit: int = 200) -> Tuple[List[Dict[str, Any]], int]:
        """Return a page of catalog index entries and total count."""
        off = max(0, int(offset or 0))
        lim = max(1, min(int(limit or 200), 2000))
        with self._lock:
            self._load_catalog_index_if_stale()
            items = list(self._catalog_index_items) if self._catalog_index_items else self.catalog_index()
            total = self._catalog_index_total or len(items)
        return items[off : off + lim], total

    def catalog_search(
        self,
        q: str,
        *,
        offset: int = 0,
        limit: int = 200,
        name_lookup: Optional[Dict[str, str]] = None,
    ) -> Tuple[List[Dict[str, Any]], int]:
        """Search catalog index entries (id/name/tags/etc).

        name_lookup:
          Optional external name mapping (id -> name) used for extra matching.
        """
        query = str(q or "").strip().lower()
        if not query:
            return [], 0

        def _split_query(text: str) -> Tuple[List[Tuple[str, str]], List[str]]:
            tokens = [t for t in text.split() if t]
            filters: List[Tuple[str, str]] = []
            words: List[str] = []
            for tok in tokens:
                if ":" in tok:
                    k, v = tok.split(":", 1)
                    k = k.strip()
                    v = v.strip()
                    if k and v:
                        filters.append((k, v))
                        continue
                words.append(tok)
            return filters, words

        filters, words = _split_query(query)

        def _as_list(val: Any) -> List[str]:
            if isinstance(val, str):
                return [val]
            if isinstance(val, (list, tuple, set)):
                return [str(x) for x in val if x]
            return []

        def _match_filters(item: Dict[str, Any]) -> bool:
            if not filters:
                return True
            kind = str(item.get("kind") or "").lower()
            cats = [v.lower() for v in _as_list(item.get("categories"))]
            behs = [v.lower() for v in _as_list(item.get("behaviors"))]
            srcs = [v.lower() for v in _as_list(item.get("sources"))]
            tags = [v.lower() for v in _as_list(item.get("tags"))]
            comps = [v.lower() for v in _as_list(item.get("components"))]
            slots = [v.lower() for v in _as_list(item.get("slots"))]

            def _hit(arr: List[str], vals: List[str]) -> bool:
                return any(v in arr for v in vals)

            for key_raw, val_raw in filters:
                key = key_raw.lower()
                vals = [v.strip().lower() for v in val_raw.split(",") if v.strip()]
                if not vals:
                    continue
                if key in ("kind", "type"):
                    if kind not in vals:
                        return False
                elif key in ("cat", "category"):
                    if not _hit(cats, vals):
                        return False
                elif key in ("beh", "behavior"):
                    if not _hit(behs, vals):
                        return False
                elif key in ("src", "source"):
                    if not _hit(srcs, vals):
                        return False
                elif key == "tag":
                    if not _hit(tags, vals):
                        return False
                elif key in ("comp", "component"):
                    if not _hit(comps, vals):
                        return False
                elif key == "slot":
                    if not _hit(slots, vals):
                        return False
            return True

        items: List[Dict[str, Any]]
        use_fts = bool(words) and self._use_sqlite and not name_lookup
        if use_fts:
            candidate_limit = min(max(int(offset or 0) + int(limit or 200), 2000), 10000)
            fts_items = self._fetch_catalog_index_from_fts(words, limit=candidate_limit)
            if fts_items is not None:
                items = fts_items
            else:
                use_fts = False

        if not use_fts:
            with self._lock:
                self._load_catalog_index_if_stale()
                items = list(self._catalog_index_items) if self._catalog_index_items else self.catalog_index()

        extra_names: Dict[str, str] = {}
        if name_lookup:
            extra_names = {
                str(k).lower(): str(v).lower()
                for k, v in name_lookup.items()
                if k and v
            }

        scored: List[Tuple[int, str, Dict[str, Any]]] = []
        for item in items:
            if not isinstance(item, dict):
                continue
            if not _match_filters(item):
                continue
            iid_raw = str(item.get("id") or "")
            iid = iid_raw.lower()
            name = str(item.get("name") or "").lower()
            alt = extra_names.get(iid) or extra_names.get(iid_raw.lower(), "")
            score = 0
            if not words:
                score = 1
            else:
                for w in words:
                    if not w:
                        continue
                    if iid == w:
                        score += 1000
                    if iid.startswith(w):
                        score += 200
                    if w in iid:
                        score += 80
                    if w in name:
                        score += 40
                    if alt and w in alt:
                        score += 60
            if score > 0:
                scored.append((score, iid, item))

        scored.sort(key=lambda x: (-x[0], x[1]))
        total = len(scored)
        off = max(0, int(offset or 0))
        lim = max(1, min(int(limit or 200), 2000))
        sliced = [row[2] for row in scored[off : off + lim]]
        return sliced, total

    # ----------------- craft queries -----------------

    def resolve_recipe_name(self, q: str) -> Optional[str]:
        """Resolve query -> canonical recipe name via aliases & exact match (case-insensitive)."""
        if not q:
            return None
        q0 = str(q).strip()
        if not q0:
            return None

        with self._lock:
            if q0 in self._recipes:
                return q0

            if q0 in self._aliases:
                return self._aliases[q0]

            ql = q0.lower()
            for nm in self._recipes.keys():
                if nm.lower() == ql:
                    return nm
            for a, nm in self._aliases.items():
                if a.lower() == ql:
                    return nm

        return None

    def get_recipe(self, q: str) -> Optional[CraftRecipe]:
        name = self.resolve_recipe_name(q)
        if not name:
            return None
        with self._lock:
            return self._recipes.get(name)

    def iter_recipes(self) -> List[CraftRecipe]:
        """Return a snapshot list of all recipes."""
        with self._lock:
            return list(self._recipes.values())

    def list_filters(self) -> Tuple[List[str], List[Dict[str, Any]]]:
        with self._lock:
            return list(self._filter_order), list(self._filter_defs)

    def list_tabs(self) -> List[Dict[str, Any]]:
        """Return ordered tabs with counts.

        Order heuristic:
          - follow filter_order for any matching tab name
          - then append remaining tabs alphabetically
        """
        with self._lock:
            tab_names = set(self._by_tab.keys())
            ordered: List[str] = []
            for f in self._filter_order:
                if f in tab_names and f not in ordered:
                    ordered.append(f)
            ordered += sorted([t for t in tab_names if t not in ordered])
            return [{"name": t, "count": len(self._by_tab.get(t, []))} for t in ordered]

    def list_tags(self) -> List[Dict[str, Any]]:
        with self._lock:
            tags = [{"name": t, "count": len(v)} for t, v in self._by_tag.items()]
        tags.sort(key=lambda x: (-x["count"], x["name"]))
        return tags

    def list_by_filter(self, filter_name: str) -> List[str]:
        with self._lock:
            return list(self._by_filter.get(filter_name, []))

    def list_by_tab(self, tab: str) -> List[str]:
        with self._lock:
            return list(self._by_tab.get(tab, []))

    def list_by_tag(self, tag: str) -> List[str]:
        with self._lock:
            return list(self._by_tag.get(tag, []))

    def list_by_ingredient(self, item: str) -> List[str]:
        with self._lock:
            return list(self._by_ingredient.get(item, []))

    def list_by_product(self, item: str) -> List[str]:
        """List craft recipes whose product equals `item`."""
        key = (item or "").strip()
        if not key:
            return []
        with self._lock:
            return list(self._by_product.get(key, []))

    def search(
        self,
        q: str,
        limit: int = 50,
        name_lookup: Optional[Dict[str, str]] = None,
    ) -> List[Dict[str, Any]]:
        """Search craft recipes.

        Supported prefixes:
          - ing:<item>
          - tag:<builder_tag>
          - filter:<FILTER>
          - tab:<TAB>

        Otherwise:
          - substring match on recipe name or product
        """
        q = (q or "").strip()
        if not q:
            return []

        limit = max(1, min(int(limit or 50), 500))
        ql = q.lower()

        extra_names: Dict[str, str] = {}
        if name_lookup:
            extra_names = {
                str(k).lower(): str(v).lower()
                for k, v in name_lookup.items()
                if k and v
            }

        with self._lock:
            for prefix in ("ing:", "tag:", "filter:", "tab:"):
                if ql.startswith(prefix):
                    val = q[len(prefix) :].strip()
                    if not val:
                        return []
                    if prefix == "ing:":
                        names = self._by_ingredient.get(val, [])
                    elif prefix == "tag:":
                        names = self._by_tag.get(val, [])
                    elif prefix == "filter:":
                        names = self._by_filter.get(val, [])
                    else:
                        names = self._by_tab.get(val, [])
                    return [self._recipe_brief(nm) for nm in names[:limit]]

            nm = self.resolve_recipe_name(q)
            if nm:
                return [self._recipe_brief(nm)]

            if not extra_names:
                hits: List[str] = []
                for nm2, rec in self._recipes.items():
                    if ql in nm2.lower() or (rec.product and ql in str(rec.product).lower()):
                        hits.append(nm2)
                        if len(hits) >= limit:
                            break
                return [self._recipe_brief(nm2) for nm2 in hits]

            scored: List[Tuple[int, str]] = []
            for nm2, rec in self._recipes.items():
                nm2l = nm2.lower()
                prod = str(rec.product or "").lower()
                alt_nm = extra_names.get(nm2l) or ""
                alt_prod = extra_names.get(prod) if prod else ""
                score = 0
                if nm2l == ql:
                    score += 400
                elif nm2l.startswith(ql):
                    score += 200
                elif ql in nm2l:
                    score += 80
                if prod:
                    if prod == ql:
                        score += 120
                    elif prod.startswith(ql):
                        score += 60
                    elif ql in prod:
                        score += 20
                if alt_nm:
                    idx = alt_nm.find(ql)
                    if idx >= 0:
                        score += 120
                        if idx == 0:
                            score += 60
                        if len(alt_nm) == len(ql):
                            score += 40
                if alt_prod:
                    idx = alt_prod.find(ql)
                    if idx >= 0:
                        score += 60
                        if idx == 0:
                            score += 30
                        if len(alt_prod) == len(ql):
                            score += 20
                if score > 0:
                    scored.append((score, nm2))

            scored.sort(key=lambda x: (-x[0], x[1]))
            return [self._recipe_brief(nm2) for _, nm2 in scored[:limit]]

    def _recipe_brief(self, name: str) -> Dict[str, Any]:
        rec = self._recipes.get(name)
        if not rec:
            return {"name": name}
        return {
            "name": rec.name,
            "product": rec.product,
            "tab": rec.tab,
            "tech": rec.tech,
            "filters": rec.filters,
            "builder_tags": rec.builder_tags,
            "builder_skill": rec.builder_skill,
            "station_tag": rec.station_tag,
        }

    # ----------------- cooking queries -----------------

    def resolve_cooking_name(self, q: str) -> Optional[str]:
        if not q:
            return None
        q0 = str(q).strip()
        if not q0:
            return None

        with self._lock:
            if q0 in self._cooking:
                return q0

            ql = q0.lower()
            for nm in self._cooking.keys():
                if nm.lower() == ql:
                    return nm

        return None

    def get_cooking_recipe(self, q: str) -> Optional[CookingRecipe]:
        nm = self.resolve_cooking_name(q)
        if not nm:
            return None
        with self._lock:
            return self._cooking.get(nm)

    def iter_cooking_recipes(self) -> List[CookingRecipe]:
        with self._lock:
            return list(self._cooking.values())

    def cooking_ingredients(self) -> Dict[str, Dict[str, Any]]:
        with self._lock:
            return dict(self._cooking_ingredients)

    def cooking_ingredients_with_fallback(self) -> Tuple[Dict[str, Dict[str, Any]], str]:
        with self._lock:
            if self._cooking_ingredients:
                return dict(self._cooking_ingredients), "cooking_ingredients"

            items: Dict[str, Dict[str, Any]] = {}

            def _is_foodish(item: Dict[str, Any]) -> bool:
                comps = item.get("components") or []
                if isinstance(comps, (list, tuple, set)) and "edible" in comps:
                    return True
                beh = item.get("behaviors") or []
                if isinstance(beh, (list, tuple, set)) and "edible" in beh:
                    return True
                cats = item.get("categories") or []
                if isinstance(cats, (list, tuple, set)) and "food" in cats:
                    return True
                return False

            cooking_ids = set(self._cooking.keys())
            for iid, item in self._items.items():
                foodish = _is_foodish(item)
                tags = guess_cooking_tags(iid, item if foodish else None)
                if not tags:
                    continue
                kind = str(item.get("kind") or "").strip().lower()
                if not foodish and set(tags.keys()) <= {"magic"}:
                    continue
                if kind in ("creature", "structure", "character", "world") and not foodish:
                    continue
                if not foodish and iid in cooking_ids and iid not in self._cook_by_ingredient:
                    continue
                items[iid] = {"id": iid, "tags": tags, "foodtype": None}

            for iid in self._cook_by_ingredient.keys():
                if iid in items:
                    continue
                tags = guess_cooking_tags(iid, self._items.get(iid))
                items[iid] = {"id": iid, "tags": tags, "foodtype": None}

            source = "items_fallback" if items else "card_ingredients"
            return items, source

    def list_cooking_ingredients(self) -> List[str]:
        with self._lock:
            if self._cooking_ingredients:
                items = self._cooking_ingredients.keys()
            else:
                items = self._cook_by_ingredient.keys()
        return sorted(items)

    def list_cooking_foodtypes(self) -> List[Dict[str, Any]]:
        with self._lock:
            items = [{"name": ft, "count": len(v)} for ft, v in self._cook_by_foodtype.items()]
        items.sort(key=lambda x: (-x["count"], x["name"]))
        return items

    def list_cooking_tags(self) -> List[Dict[str, Any]]:
        with self._lock:
            items = [{"name": t, "count": len(v)} for t, v in self._cook_by_tag.items()]
        items.sort(key=lambda x: (-x["count"], x["name"]))
        return items

    def list_cooking_by_foodtype(self, foodtype: str) -> List[str]:
        with self._lock:
            return list(self._cook_by_foodtype.get(foodtype, []))

    def list_cooking_by_tag(self, tag: str) -> List[str]:
        with self._lock:
            return list(self._cook_by_tag.get(tag, []))

    def list_cooking_by_ingredient(self, item: str) -> List[str]:
        with self._lock:
            return list(self._cook_by_ingredient.get(item, []))

    def search_cooking(
        self,
        q: str,
        limit: int = 50,
        name_lookup: Optional[Dict[str, str]] = None,
    ) -> List[Dict[str, Any]]:
        """Search cooking recipes.

        Supported prefixes:
          - ing:<item>      (uses card_ingredients index; may be incomplete)
          - tag:<tag>      (food tags)
          - type:<FOODTYPE.*> or foodtype:<FOODTYPE.*>

        Otherwise: substring match on recipe name.
        """
        q = (q or "").strip()
        if not q:
            return []

        limit = max(1, min(int(limit or 50), 500))
        ql = q.lower()

        extra_names: Dict[str, str] = {}
        if name_lookup:
            extra_names = {
                str(k).lower(): str(v).lower()
                for k, v in name_lookup.items()
                if k and v
            }

        with self._lock:
            for prefix in ("ing:", "tag:", "type:", "foodtype:"):
                if ql.startswith(prefix):
                    val = q[len(prefix) :].strip()
                    if not val:
                        return []
                    if prefix == "ing:":
                        names = self._cook_by_ingredient.get(val, [])
                    elif prefix == "tag:":
                        names = self._cook_by_tag.get(val, [])
                    else:
                        names = self._cook_by_foodtype.get(val, [])
                    return [{"name": nm} for nm in names[:limit]]

            nm = self.resolve_cooking_name(q)
            if nm:
                return [{"name": nm}]

            if not extra_names:
                hits: List[str] = []
                for nm2 in self._cooking.keys():
                    if ql in nm2.lower():
                        hits.append(nm2)
                        if len(hits) >= limit:
                            break
                return [{"name": nm2} for nm2 in hits]

            scored: List[Tuple[int, str]] = []
            for nm2 in self._cooking.keys():
                nm2l = nm2.lower()
                alt = extra_names.get(nm2l) or ""
                score = 0
                if nm2l == ql:
                    score += 200
                elif nm2l.startswith(ql):
                    score += 120
                elif ql in nm2l:
                    score += 60
                if alt:
                    idx = alt.find(ql)
                    if idx >= 0:
                        score += 120
                        if idx == 0:
                            score += 60
                        if len(alt) == len(ql):
                            score += 40
                if score > 0:
                    scored.append((score, nm2))

            scored.sort(key=lambda x: (-x[0], x[1]))
            return [{"name": nm2} for _, nm2 in scored[:limit]]
```

### File: apps/webcraft/cooking_planner.py
- mode: full
- size_bytes: 41532
- sha256_12: db5056ac693c

```py
# -*- coding: utf-8 -*-
from __future__ import annotations

import re
from dataclasses import dataclass
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple

from .catalog_store import CookingRecipe, guess_cooking_tags, normalize_cooking_tags

TAG_PENALTY = 10.0
NAME_PENALTY = 50.0
MAX_AVAILABLE_COMBOS = 15000
FILLER_TAGS = {"inedible", "frozen", "dried"}
FILLER_NAMES = {"twigs", "ice", "lightninggoathorn", "boneshard"}
NEAR_TIER_ORDER = {"primary": 0, "secondary": 1, "filler": 2}

def normalize_counts(inv: Dict[str, Any]) -> Dict[str, float]:
    """Normalize item->count mapping.

    - Keys are stripped strings.
    - Values must be numeric and > 0.

    This helper is shared by:
      - cookable query (inventory)
      - simulator (cookpot slots)
    """

    out: Dict[str, float] = {}
    for k, v in (inv or {}).items():
        key = str(k).strip().lower()
        if not key:
            continue
        try:
            num = float(v)
        except Exception:
            continue
        if num <= 0:
            continue
        out[key] = num
    return out


def _normalize_slots(slots: Dict[str, Any]) -> Dict[str, int]:
    out: Dict[str, int] = {}
    for k, v in normalize_counts(slots).items():
        n = int(round(v))
        if n <= 0:
            continue
        out[k] = out.get(k, 0) + n
    return out


def _normalize_available(items: Optional[Iterable[str]]) -> List[str]:
    out: List[str] = []
    seen: Set[str] = set()
    for item in items or []:
        iid = str(item or "").strip().lower()
        if not iid or iid in seen:
            continue
        seen.add(iid)
        out.append(iid)
    return out


def _combo_count(n: int, k: int) -> int:
    if k <= 0:
        return 1
    if n <= 0:
        return 0
    num = 1
    den = 1
    for i in range(1, k + 1):
        num *= n + i - 1
        den *= i
    return num // den


def _build_slot_combos(items: List[str], remaining: int, *, max_count: int) -> Optional[List[Dict[str, int]]]:
    if remaining <= 0:
        return [dict()]
    if not items:
        return []
    if _combo_count(len(items), remaining) > max_count:
        return None

    combos: List[Dict[str, int]] = []

    def _walk(start: int, rem: int, cur: Dict[str, int]) -> None:
        if rem <= 0:
            combos.append(dict(cur))
            return
        for idx in range(start, len(items)):
            iid = items[idx]
            cur[iid] = cur.get(iid, 0) + 1
            _walk(idx, rem - 1, cur)
            nxt = cur.get(iid, 0) - 1
            if nxt <= 0:
                cur.pop(iid, None)
            else:
                cur[iid] = nxt

    _walk(0, remaining, {})
    return combos


def _merge_slots(base: Dict[str, int], extra: Dict[str, int]) -> Dict[str, int]:
    if not extra:
        return dict(base)
    out = dict(base)
    for k, v in extra.items():
        out[k] = out.get(k, 0) + int(v)
    return out


def _collect_pool(
    items: Iterable[str],
    tags_by_item: Dict[str, Dict[str, float]],
) -> Tuple[Set[str], Set[str]]:
    pool_names: Set[str] = set()
    pool_tags: Set[str] = set()
    for item in items or []:
        iid = str(item or "").strip().lower()
        if not iid:
            continue
        pool_names.add(iid)
        for tag in (tags_by_item.get(iid) or {}).keys():
            pool_tags.add(str(tag).strip().lower())
    return pool_names, pool_tags


def _is_filler_name(name: str, tags_by_item: Dict[str, Dict[str, float]]) -> bool:
    key = str(name or "").strip().lower()
    if not key:
        return False
    if key in FILLER_NAMES:
        return True
    tags = tags_by_item.get(key) or {}
    if tags and all(str(t).strip().lower() in FILLER_TAGS for t in tags.keys()):
        return True
    return False


def _missing_is_filler(entry: Dict[str, Any], tags_by_item: Dict[str, Dict[str, float]]) -> bool:
    mtype = str(entry.get("type") or "").strip().lower()
    key = str(entry.get("key") or "").strip().lower()
    if mtype == "tag":
        return key in FILLER_TAGS
    if mtype == "name":
        return _is_filler_name(key, tags_by_item)
    if mtype == "name_any":
        options = entry.get("options") or []
        opts = [str(o).strip().lower() for o in options if str(o).strip()]
        return bool(opts) and all(_is_filler_name(opt, tags_by_item) for opt in opts)
    return False


def _classify_near_miss(
    row: Dict[str, Any],
    *,
    pool_names: Set[str],
    pool_tags: Set[str],
    tags_by_item: Dict[str, Dict[str, float]],
) -> Tuple[str, int, int, int]:
    missing = row.get("missing") or []
    if row.get("rule_mode") == "none":
        non_filler = sum(1 for m in missing if not _missing_is_filler(m, tags_by_item))
        return "filler", 0, 0, non_filler

    req_names = {str(n).strip().lower() for n in (row.get("req_names") or []) if str(n).strip()}
    req_groups = row.get("req_name_groups") or []
    req_tags = {str(t).strip().lower() for t in (row.get("req_tags") or []) if str(t).strip()}

    name_hits = sum(1 for name in req_names if name in pool_names and not _is_filler_name(name, tags_by_item))
    group_hits = 0
    for group in req_groups:
        if not isinstance(group, list):
            continue
        opts = [str(o).strip().lower() for o in group if str(o).strip()]
        if not opts:
            continue
        if any(opt in pool_names and not _is_filler_name(opt, tags_by_item) for opt in opts):
            group_hits += 1

    tag_hits = sum(1 for tag in req_tags if tag in pool_tags and tag not in FILLER_TAGS)
    feature_hits = name_hits + group_hits

    non_filler = sum(1 for m in missing if not _missing_is_filler(m, tags_by_item))

    if feature_hits > 0:
        tier = "primary"
    elif tag_hits > 0:
        tier = "secondary"
    else:
        tier = "filler"
    return tier, feature_hits, tag_hits, non_filler


def _rank_near_miss(
    rows: List[Dict[str, Any]],
    *,
    pool_names: Set[str],
    pool_tags: Set[str],
    tags_by_item: Dict[str, Dict[str, float]],
    limit: int,
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    enriched: List[Dict[str, Any]] = []
    for row in rows:
        tier, feature_hits, tag_hits, non_filler = _classify_near_miss(
            row,
            pool_names=pool_names,
            pool_tags=pool_tags,
            tags_by_item=tags_by_item,
        )
        row["near_tier"] = tier
        row["near_feature_hits"] = feature_hits
        row["near_tag_hits"] = tag_hits
        row["near_missing_non_filler"] = non_filler
        enriched.append(row)

    def _key(r: Dict[str, Any]) -> Tuple[Any, ...]:
        tier = str(r.get("near_tier") or "secondary")
        return (
            NEAR_TIER_ORDER.get(tier, 9),
            -int(r.get("near_feature_hits") or 0),
            -int(r.get("near_tag_hits") or 0),
            int(r.get("near_missing_non_filler") or 0),
            -float(r.get("score") or 0.0),
            str(r.get("name") or ""),
        )

    enriched.sort(key=_key)
    tiers: Dict[str, List[Dict[str, Any]]] = {"primary": [], "secondary": [], "filler": []}
    limited: List[Dict[str, Any]] = []
    for row in enriched:
        if limit and len(limited) >= limit:
            break
        limited.append(row)
        tier = str(row.get("near_tier") or "secondary")
        tiers.setdefault(tier, []).append(row)

    tier_list = []
    for key in ("primary", "secondary", "filler"):
        if tiers.get(key):
            tier_list.append({"key": key, "count": len(tiers[key]), "items": tiers[key]})
    return limited, tier_list


def _requirements_satisfied(req: List[Tuple[str, float]], available: Dict[str, float]) -> bool:
    """Check if `available` contains all `req` items with required counts."""
    for item, need in (req or []):
        if not item:
            continue
        have = float(available.get(item, 0.0))
        if have + 1e-9 < float(need):
            return False
    return True


def build_ingredient_index(
    ingredients: Dict[str, Any],
    *,
    extra_items: Optional[Iterable[str]] = None,
) -> Tuple[Dict[str, Dict[str, float]], Dict[str, float]]:
    tags_by_item: Dict[str, Dict[str, float]] = {}
    max_by_tag: Dict[str, float] = {}

    def _merge(iid: str, tags: Dict[str, float]) -> None:
        if not tags:
            return
        out = tags_by_item.get(iid, {})
        for k, v in tags.items():
            key = str(k).strip().lower()
            if not key:
                continue
            try:
                num = float(v)
            except Exception:
                continue
            out[key] = num
            if key not in max_by_tag or num > max_by_tag[key]:
                max_by_tag[key] = num
        if out:
            tags_by_item[iid] = out

    for item_id, raw in (ingredients or {}).items():
        if not item_id or not isinstance(raw, dict):
            continue
        iid = str(item_id).strip().lower()
        if not iid:
            continue
        tags = normalize_cooking_tags(raw.get("tags"))
        if not tags and not raw.get("tags_expr"):
            tags = guess_cooking_tags(iid)
        _merge(iid, tags)

    for item_id in (extra_items or []):
        iid = str(item_id).strip().lower()
        if not iid or iid in tags_by_item:
            continue
        _merge(iid, guess_cooking_tags(iid))

    return tags_by_item, max_by_tag


def _sum_tags(slots: Dict[str, int], tags_by_item: Dict[str, Dict[str, float]]) -> Dict[str, float]:
    totals: Dict[str, float] = {}
    for item_id, count in (slots or {}).items():
        tags = tags_by_item.get(str(item_id).strip().lower()) or {}
        if not tags:
            continue
        for tag, val in tags.items():
            totals[tag] = totals.get(tag, 0.0) + float(val) * float(count)
    return totals


def _sum_names(slots: Dict[str, int]) -> Dict[str, int]:
    out: Dict[str, int] = {}
    for item_id, count in (slots or {}).items():
        iid = str(item_id).strip().lower()
        if not iid:
            continue
        out[iid] = out.get(iid, 0) + int(count)
    return out


def _compare(lhs: float, op: str, rhs: float) -> bool:
    if op == "==":
        return abs(lhs - rhs) <= 1e-9
    if op == "~=":
        return abs(lhs - rhs) > 1e-9
    if op == ">":
        return lhs > rhs + 1e-9
    if op == ">=":
        return lhs + 1e-9 >= rhs
    if op == "<":
        return lhs + 1e-9 < rhs
    if op == "<=":
        return lhs <= rhs + 1e-9
    return True


def _constraint_delta(lhs: float, op: str, rhs: float) -> Tuple[float, str]:
    if op in (">", ">="):
        delta = max(0.0, rhs - lhs)
        return delta, "under"
    if op in ("<", "<="):
        delta = max(0.0, lhs - rhs)
        return delta, "over"
    if op == "==":
        delta = abs(lhs - rhs)
        return delta, "mismatch"
    if op == "~=":
        delta = 0.0 if abs(lhs - rhs) > 1e-9 else 1.0
        return delta, "equal"
    return 0.0, "unknown"


def _coerce_constraint_value(v: Any) -> Optional[float]:
    if v is None:
        return 0.0
    if isinstance(v, (int, float)):
        return float(v)
    try:
        return float(str(v))
    except Exception:
        return None


def _get_rule_constraints(recipe: CookingRecipe) -> Dict[str, List[Dict[str, Any]]]:
    rule = recipe.raw.get("rule") if isinstance(recipe.raw, dict) else None
    if not isinstance(rule, dict):
        return {}
    cons = rule.get("constraints")
    if not isinstance(cons, dict):
        return {}
    out: Dict[str, List[Dict[str, Any]]] = {}
    for key in ("tags", "names", "names_any", "names_sum", "unparsed"):
        rows = cons.get(key)
        if isinstance(rows, list):
            out[key] = [r for r in rows if isinstance(r, dict)]

    sum_keys: Set[str] = set()
    for g in out.get("names_sum") or []:
        keys_raw = g.get("keys") if isinstance(g, dict) else None
        if not isinstance(keys_raw, list):
            continue
        for k in keys_raw:
            key = str(k).strip().lower()
            if key:
                sum_keys.add(key)

    tags = out.get("tags") or []
    if tags:
        not_keys: Set[str] = set()
        for c in tags:
            text = str(c.get("text") or "").strip().lower()
            key = str(c.get("key") or "").strip().lower()
            if key and text.startswith("not "):
                not_keys.add(key)
        if not_keys:
            filtered: List[Dict[str, Any]] = []
            for c in tags:
                key = str(c.get("key") or "").strip().lower()
                text = str(c.get("text") or "").strip().lower()
                op = str(c.get("op") or "").strip()
                if key in not_keys and not text.startswith("not ") and op in (">", ">="):
                    continue
                filtered.append(c)
            out["tags"] = filtered

    if sum_keys and out.get("names"):
        filtered: List[Dict[str, Any]] = []
        for c in out.get("names") or []:
            key = str(c.get("key") or "").strip().lower()
            op = str(c.get("op") or "").strip()
            rhs = _coerce_constraint_value(c.get("value"))
            if key in sum_keys and _is_positive_requirement(op, rhs):
                continue
            filtered.append(c)
        out["names"] = filtered
    return out


def _is_positive_requirement(op: str, rhs: Optional[float]) -> bool:
    if rhs is None:
        return False
    if op in (">", ">="):
        return rhs >= 0
    if op == "==":
        return rhs > 0
    return False


def _extract_recipe_requirements(recipe: CookingRecipe) -> Dict[str, Any]:
    req_names: Set[str] = set()
    req_tags: Set[str] = set()
    req_groups: List[List[str]] = []

    for item, need in (recipe.card_ingredients or []):
        try:
            if float(need) <= 0:
                continue
        except Exception:
            continue
        iid = str(item or "").strip().lower()
        if iid:
            req_names.add(iid)

    cons = _get_rule_constraints(recipe)
    for c in cons.get("names") or []:
        key = str(c.get("key") or "").strip().lower()
        op = str(c.get("op") or "").strip()
        rhs = _coerce_constraint_value(c.get("value"))
        if key and _is_positive_requirement(op, rhs):
            req_names.add(key)
    for g in cons.get("names_any") or []:
        keys_raw = g.get("keys") if isinstance(g, dict) else None
        if not isinstance(keys_raw, list):
            continue
        keys = [str(k).strip().lower() for k in keys_raw if str(k).strip()]
        if keys:
            req_groups.append(keys)
    for g in cons.get("names_sum") or []:
        keys_raw = g.get("keys") if isinstance(g, dict) else None
        if not isinstance(keys_raw, list):
            continue
        keys = [str(k).strip().lower() for k in keys_raw if str(k).strip()]
        if keys:
            req_groups.append(keys)
    for c in cons.get("tags") or []:
        key = str(c.get("key") or "").strip().lower()
        op = str(c.get("op") or "").strip()
        rhs = _coerce_constraint_value(c.get("value"))
        if key and _is_positive_requirement(op, rhs):
            req_tags.add(key)

    return {
        "req_names": sorted(req_names),
        "req_name_groups": req_groups,
        "req_tags": sorted(req_tags),
    }


def _stat_value(value: Any) -> Any:
    if isinstance(value, dict):
        if "value" in value:
            return value.get("value")
        if "expr" in value:
            return value.get("expr")
    return value


def _recipe_attrs(recipe: CookingRecipe) -> Dict[str, Any]:
    return {
        "foodtype": recipe.foodtype,
        "hunger": _stat_value(recipe.hunger),
        "health": _stat_value(recipe.health),
        "sanity": _stat_value(recipe.sanity),
        "perishtime": _stat_value(recipe.perishtime),
        "cooktime": _stat_value(recipe.cooktime),
    }


def _evaluate_constraints(
    constraints: Dict[str, List[Dict[str, Any]]],
    *,
    tags_total: Dict[str, float],
    names_total: Dict[str, int],
) -> Tuple[bool, List[Dict[str, Any]], List[str]]:
    missing: List[Dict[str, Any]] = []
    warnings: List[str] = []

    for g in constraints.get("names_any") or []:
        keys_raw = g.get("keys") if isinstance(g, dict) else None
        if not isinstance(keys_raw, list):
            warnings.append(str(getattr(g, "text", "") or "names_any_unparsed"))
            continue
        keys = [str(k).strip().lower() for k in keys_raw if str(k).strip()]
        if not keys:
            warnings.append(str(g.get("text") or "names_any_unparsed"))
            continue
        if any(names_total.get(k, 0) > 0 for k in keys):
            continue
        missing.append(
            {
                "type": "name_any",
                "key": "|".join(keys),
                "options": keys,
                "op": ">",
                "required": 1.0,
                "actual": 0.0,
                "delta": 1.0,
                "direction": "under",
                "text": g.get("text") or "",
            }
        )

    for g in constraints.get("names_sum") or []:
        keys_raw = g.get("keys") if isinstance(g, dict) else None
        if not isinstance(keys_raw, list):
            warnings.append(str(getattr(g, "text", "") or "names_sum_unparsed"))
            continue
        keys = [str(k).strip().lower() for k in keys_raw if str(k).strip()]
        if not keys:
            warnings.append(str(g.get("text") or "names_sum_unparsed"))
            continue
        min_val = _coerce_constraint_value(g.get("min"))
        if min_val is None:
            min_val = _coerce_constraint_value(g.get("required"))
        if min_val is None:
            warnings.append(str(g.get("text") or "names_sum_unparsed"))
            continue
        total = float(sum(names_total.get(k, 0) for k in keys))
        if total + 1e-9 < float(min_val):
            missing.append(
                {
                    "type": "name_sum",
                    "key": "|".join(keys),
                    "options": keys,
                    "min": float(min_val),
                    "required": float(min_val),
                    "actual": total,
                    "delta": float(min_val) - total,
                    "direction": "under",
                    "text": g.get("text") or "",
                }
            )

    for c in constraints.get("tags") or []:
        key = str(c.get("key") or "").strip().lower()
        op = str(c.get("op") or "").strip()
        rhs = _coerce_constraint_value(c.get("value"))
        if not key or rhs is None:
            warnings.append(str(c.get("text") or "tag_constraint_unparsed"))
            continue
        lhs = float(tags_total.get(key, 0.0))
        ok = _compare(lhs, op, float(rhs))
        if not ok:
            delta, direction = _constraint_delta(lhs, op, float(rhs))
            missing.append(
                {
                    "type": "tag",
                    "key": key,
                    "op": op,
                    "required": float(rhs),
                    "actual": lhs,
                    "delta": delta,
                    "direction": direction,
                    "text": c.get("text") or "",
                }
            )

    for c in constraints.get("names") or []:
        key = str(c.get("key") or "").strip().lower()
        op = str(c.get("op") or "").strip()
        rhs = _coerce_constraint_value(c.get("value"))
        if not key or rhs is None:
            warnings.append(str(c.get("text") or "name_constraint_unparsed"))
            continue
        lhs = float(names_total.get(key, 0))
        ok = _compare(lhs, op, float(rhs))
        if not ok:
            delta, direction = _constraint_delta(lhs, op, float(rhs))
            missing.append(
                {
                    "type": "name",
                    "key": key,
                    "op": op,
                    "required": float(rhs),
                    "actual": lhs,
                    "delta": delta,
                    "direction": direction,
                    "text": c.get("text") or "",
                }
            )

    return len(missing) == 0, missing, warnings


def _build_conditions(
    recipe: CookingRecipe,
    *,
    tags_total: Dict[str, float],
    names_total: Dict[str, int],
) -> List[Dict[str, Any]]:
    constraints = _get_rule_constraints(recipe)
    conditions: List[Dict[str, Any]] = []

    if constraints:
        for g in constraints.get("names_any") or []:
            keys_raw = g.get("keys") if isinstance(g, dict) else None
            if not isinstance(keys_raw, list):
                continue
            keys = [str(k).strip().lower() for k in keys_raw if str(k).strip()]
            if not keys:
                continue
            ok = any(names_total.get(k, 0) > 0 for k in keys)
            conditions.append(
                {
                    "type": "name_any",
                    "options": keys,
                    "op": "any",
                    "required": 1.0,
                    "actual": 1.0 if ok else 0.0,
                    "ok": ok,
                }
            )

        for g in constraints.get("names_sum") or []:
            keys_raw = g.get("keys") if isinstance(g, dict) else None
            if not isinstance(keys_raw, list):
                continue
            keys = [str(k).strip().lower() for k in keys_raw if str(k).strip()]
            if not keys:
                continue
            min_val = _coerce_constraint_value(g.get("min"))
            if min_val is None:
                min_val = _coerce_constraint_value(g.get("required"))
            if min_val is None:
                continue
            total = float(sum(names_total.get(k, 0) for k in keys))
            ok = total + 1e-9 >= float(min_val)
            conditions.append(
                {
                    "type": "name_sum",
                    "options": keys,
                    "op": ">=",
                    "required": float(min_val),
                    "actual": total,
                    "ok": ok,
                }
            )

        for c in constraints.get("names") or []:
            key = str(c.get("key") or "").strip().lower()
            op = str(c.get("op") or "").strip()
            rhs = _coerce_constraint_value(c.get("value"))
            if not key or rhs is None:
                continue
            actual = float(names_total.get(key, 0))
            ok = _compare(actual, op, float(rhs))
            conditions.append(
                {
                    "type": "name",
                    "key": key,
                    "op": op,
                    "required": float(rhs),
                    "actual": actual,
                    "ok": ok,
                }
            )

        for c in constraints.get("tags") or []:
            key = str(c.get("key") or "").strip().lower()
            op = str(c.get("op") or "").strip()
            rhs = _coerce_constraint_value(c.get("value"))
            if not key or rhs is None:
                continue
            actual = float(tags_total.get(key, 0.0))
            ok = _compare(actual, op, float(rhs))
            conditions.append(
                {
                    "type": "tag",
                    "key": key,
                    "op": op,
                    "required": float(rhs),
                    "actual": actual,
                    "ok": ok,
                }
            )

        return conditions

    for item, need in recipe.card_ingredients or []:
        key = str(item or "").strip().lower()
        if not key:
            continue
        try:
            required = float(need)
        except Exception:
            continue
        actual = float(names_total.get(key, 0))
        ok = actual + 1e-9 >= required
        conditions.append(
            {
                "type": "name",
                "key": key,
                "op": ">=",
                "required": required,
                "actual": actual,
                "ok": ok,
            }
        )

    return conditions


def _score_recipe(priority: float, weight: float, missing: List[Dict[str, Any]]) -> Tuple[float, float]:
    penalty = 0.0
    for m in missing or []:
        delta = float(m.get("delta") or 0.0)
        if m.get("type") == "tag":
            penalty += delta * TAG_PENALTY
        elif m.get("type") in ("name", "name_any"):
            penalty += delta * NAME_PENALTY
    score = float(priority) * 1000.0 + float(weight) * 100.0 - penalty
    return score, penalty


def _evaluate_recipe(
    recipe: CookingRecipe,
    *,
    slots: Dict[str, int],
    tags_by_item: Dict[str, Dict[str, float]],
) -> Dict[str, Any]:
    constraints = _get_rule_constraints(recipe)
    if constraints:
        tags_total = _sum_tags(slots, tags_by_item)
        names_total = _sum_names(slots)
        ok, missing, warnings = _evaluate_constraints(
            constraints,
            tags_total=tags_total,
            names_total=names_total,
        )
        return {
            "ok": ok,
            "missing": missing,
            "warnings": warnings,
            "tags_total": tags_total,
            "names_total": names_total,
            "rule_mode": "rule",
        }

    if recipe.card_ingredients:
        ok = _requirements_satisfied(recipe.card_ingredients, {k: float(v) for k, v in slots.items()})
        missing = []
        if not ok:
            for item, need in recipe.card_ingredients:
                have = float(slots.get(item, 0))
                if have + 1e-9 < float(need):
                    missing.append(
                        {
                            "type": "name",
                            "key": str(item),
                            "op": ">=",
                            "required": float(need),
                            "actual": have,
                            "delta": float(need) - have,
                            "direction": "under",
                            "text": "",
                        }
                    )
        return {
            "ok": ok,
            "missing": missing,
            "warnings": [],
            "tags_total": {},
            "names_total": _sum_names(slots),
            "rule_mode": "card",
        }

    return {
        "ok": False,
        "missing": [],
        "warnings": ["no_rule_or_card_ingredients"],
        "tags_total": {},
        "names_total": _sum_names(slots),
        "rule_mode": "none",
    }


def _possible_with_remaining(
    constraints: Dict[str, List[Dict[str, Any]]],
    *,
    tags_total: Dict[str, float],
    names_total: Dict[str, int],
    remaining: int,
    max_by_tag: Dict[str, float],
    available_names: Optional[Set[str]] = None,
) -> bool:
    for g in constraints.get("names_any") or []:
        keys_raw = g.get("keys") if isinstance(g, dict) else None
        if not isinstance(keys_raw, list):
            continue
        keys = [str(k).strip().lower() for k in keys_raw if str(k).strip()]
        if not keys:
            continue
        if any(names_total.get(k, 0) > 0 for k in keys):
            continue
        if available_names is not None:
            if not any(k in available_names for k in keys):
                return False
        elif remaining <= 0:
            return False

    for g in constraints.get("names_sum") or []:
        keys_raw = g.get("keys") if isinstance(g, dict) else None
        if not isinstance(keys_raw, list):
            continue
        keys = [str(k).strip().lower() for k in keys_raw if str(k).strip()]
        if not keys:
            continue
        min_val = _coerce_constraint_value(g.get("min"))
        if min_val is None:
            min_val = _coerce_constraint_value(g.get("required"))
        if min_val is None:
            continue
        total = float(sum(names_total.get(k, 0) for k in keys))
        if total >= float(min_val) - 1e-9:
            continue
        if available_names is not None and not any(k in available_names for k in keys):
            return False
        max_possible = total + float(max(0, remaining))
        if max_possible + 1e-9 < float(min_val):
            return False

    for c in constraints.get("tags") or []:
        key = str(c.get("key") or "").strip().lower()
        op = str(c.get("op") or "").strip()
        rhs = _coerce_constraint_value(c.get("value"))
        if not key or rhs is None:
            continue
        lhs = float(tags_total.get(key, 0.0))
        max_add = float(max_by_tag.get(key, 0.0)) * float(max(0, remaining))
        max_possible = lhs + max_add
        if op in (">", ">=") and max_possible + 1e-9 < float(rhs):
            return False
        if op in ("<", "<=") and lhs > float(rhs) + 1e-9:
            return False
        if op == "==" and (float(rhs) < lhs - 1e-9 or float(rhs) > max_possible + 1e-9):
            return False
        if op == "~=" and abs(lhs - float(rhs)) <= 1e-9 and max_add <= 1e-9:
            return False

    for c in constraints.get("names") or []:
        key = str(c.get("key") or "").strip().lower()
        op = str(c.get("op") or "").strip()
        rhs = _coerce_constraint_value(c.get("value"))
        if not key or rhs is None:
            continue
        if available_names is not None and op in (">", ">=", "==") and float(rhs) > 0:
            if key not in available_names:
                return False
        lhs = float(names_total.get(key, 0))
        max_possible = lhs + float(max(0, remaining))
        if op in (">", ">=") and max_possible + 1e-9 < float(rhs):
            return False
        if op in ("<", "<=") and lhs > float(rhs) + 1e-9:
            return False
        if op == "==" and (float(rhs) < lhs - 1e-9 or float(rhs) > max_possible + 1e-9):
            return False
        if op == "~=" and abs(lhs - float(rhs)) <= 1e-9 and remaining <= 0:
            return False

    return True


def find_cookable(
    recipes: List[CookingRecipe],
    inventory: Dict[str, Any],
    *,
    limit: int = 200,
) -> List[CookingRecipe]:
    """Find cookable recipes based on catalog `card_ingredients`.

    Current limitation
    - Only recipes with `card_ingredients` can be evaluated.
    """

    inv = normalize_counts(inventory)
    limit = max(1, min(int(limit or 200), 2000))

    out: List[CookingRecipe] = []
    for r in recipes:
        if not r.card_ingredients:
            continue
        if _requirements_satisfied(r.card_ingredients, inv):
            out.append(r)

    # Stable order: higher priority first, then name.
    out.sort(key=lambda x: (-float(x.priority), x.name))
    return out[:limit]


@dataclass(frozen=True)
class SimCandidate:
    name: str
    priority: float
    weight: float


def simulate_cookpot(
    recipes: List[CookingRecipe],
    slots: Dict[str, Any],
    *,
    return_top: int = 25,
    ingredients: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """Simulate cookpot output using catalog `card_ingredients`.

    Input
    - slots: mapping of item -> count placed into the pot.

    Behavior
    - Requires total slot count == 4 (cookpot rule).
    - Only evaluates recipes that have `card_ingredients`.
    - If multiple match, choose the highest priority; tie-break by weight then name.
    - If none match, fall back to 'wetgoop' if present in recipe list.

    Returns a dict suitable for JSON response.
    """

    slots_i = _normalize_slots(slots)
    total = sum(int(v) for v in slots_i.values())
    if total != 4:
        return {
            "ok": False,
            "error": "cookpot_requires_4_items",
            "total": total,
            "slots": slots_i,
        }

    tags_by_item, _ = build_ingredient_index(ingredients or {}, extra_items=slots_i.keys())

    candidates: List[CookingRecipe] = []
    candidate_rows: List[Dict[str, Any]] = []
    near_miss: List[Dict[str, Any]] = []
    for r in recipes:
        req = _extract_recipe_requirements(r)
        attrs = _recipe_attrs(r)
        ev = _evaluate_recipe(r, slots=slots_i, tags_by_item=tags_by_item)
        score, penalty = _score_recipe(float(r.priority), float(r.weight), ev.get("missing") or [])
        conditions = _build_conditions(
            r,
            tags_total=ev.get("tags_total") or {},
            names_total=ev.get("names_total") or {},
        )
        row = {
            "name": r.name,
            "priority": float(r.priority),
            "weight": float(r.weight),
            "score": score,
            "penalty": penalty,
            "missing": ev.get("missing") or [],
            "rule_mode": ev.get("rule_mode"),
            "warnings": ev.get("warnings") or [],
            "req_names": req.get("req_names") or [],
            "req_name_groups": req.get("req_name_groups") or [],
            "req_tags": req.get("req_tags") or [],
            "attrs": attrs,
            "conditions": conditions,
            "conditions_ok": bool(ev.get("ok")),
        }
        if ev.get("ok"):
            candidates.append(r)
            row["ok"] = True
            candidate_rows.append(row)
        else:
            row["ok"] = False
            near_miss.append(row)

    if candidates:
        candidates.sort(key=lambda x: (-float(x.priority), -float(x.weight), x.name))
        best = candidates[0]
        top = [SimCandidate(name=r.name, priority=float(r.priority), weight=float(r.weight)) for r in candidates[:return_top]]
        pool_names, pool_tags = _collect_pool(slots_i.keys(), tags_by_item)
        near_sorted, near_tiers = _rank_near_miss(
            near_miss,
            pool_names=pool_names,
            pool_tags=pool_tags,
            tags_by_item=tags_by_item,
            limit=return_top,
        )
        return {
            "ok": True,
            "result": best.name,
            "reason": "matched_constraints",
            "candidates": [c.__dict__ for c in top],
            "cookable": sorted(candidate_rows, key=lambda x: (-x.get("score", 0.0), x.get("name", "")))[: return_top],
            "slots": slots_i,
            "near_miss": near_sorted,
            "near_miss_tiers": near_tiers,
            "formula": "score = priority*1000 + weight*100 - missing_penalty",
        }

    # fallback
    wet = next((r for r in recipes if r.name == "wetgoop"), None)
    if wet is not None:
        return {
            "ok": True,
            "result": "wetgoop",
            "reason": "fallback_wetgoop",
            "candidates": [],
            "cookable": [],
            "slots": slots_i,
            "near_miss": [],
            "formula": "score = priority*1000 + weight*100 - missing_penalty",
        }

    return {
        "ok": False,
        "error": "no_match_and_no_wetgoop",
        "candidates": [],
        "slots": slots_i,
    }


def explore_cookpot(
    recipes: List[CookingRecipe],
    slots: Dict[str, Any],
    *,
    ingredients: Dict[str, Any],
    limit: int = 200,
    available: Optional[Iterable[str]] = None,
) -> Dict[str, Any]:
    slots_i = _normalize_slots(slots)
    total = sum(int(v) for v in slots_i.values())
    if total > 4:
        return {
            "ok": False,
            "error": "cookpot_requires_max_4_items",
            "total": total,
            "slots": slots_i,
        }

    remaining = 4 - total
    avail_list = _normalize_available(available)
    avail_set = set(avail_list)
    extra_items = list(slots_i.keys()) + avail_list
    tags_by_item, max_by_tag = build_ingredient_index(ingredients or {}, extra_items=extra_items)
    if avail_list:
        max_by_tag = {}
        for iid in avail_list:
            for tag, val in (tags_by_item.get(iid) or {}).items():
                if tag not in max_by_tag or val > max_by_tag[tag]:
                    max_by_tag[tag] = val

    if avail_list:
        combos = _build_slot_combos(avail_list, remaining, max_count=MAX_AVAILABLE_COMBOS)
        if combos is not None:
            cookable: List[Dict[str, Any]] = []
            near_miss: List[Dict[str, Any]] = []
            for r in recipes:
                req = _extract_recipe_requirements(r)
                attrs = _recipe_attrs(r)
                best_ok: Optional[Dict[str, Any]] = None
                best_any: Optional[Dict[str, Any]] = None
                for combo in combos:
                    slots_full = _merge_slots(slots_i, combo)
                    ev = _evaluate_recipe(r, slots=slots_full, tags_by_item=tags_by_item)
                    score, penalty = _score_recipe(float(r.priority), float(r.weight), ev.get("missing") or [])
                    conditions = _build_conditions(
                        r,
                        tags_total=ev.get("tags_total") or {},
                        names_total=ev.get("names_total") or {},
                    )
                    row = {
                        "name": r.name,
                        "priority": float(r.priority),
                        "weight": float(r.weight),
                        "score": score,
                        "penalty": penalty,
                        "missing": ev.get("missing") or [],
                        "rule_mode": ev.get("rule_mode"),
                        "warnings": ev.get("warnings") or [],
                        "req_names": req.get("req_names") or [],
                        "req_name_groups": req.get("req_name_groups") or [],
                        "req_tags": req.get("req_tags") or [],
                        "attrs": attrs,
                        "conditions": conditions,
                        "conditions_ok": bool(ev.get("ok")),
                    }
                    if best_any is None or score > float(best_any.get("score", 0.0)):
                        best_any = row
                    if ev.get("ok") and (best_ok is None or score > float(best_ok.get("score", 0.0))):
                        best_ok = row
                if best_ok is not None:
                    cookable.append(best_ok)
                elif best_any is not None:
                    near_miss.append(best_any)

            cookable.sort(key=lambda x: (-x.get("score", 0.0), x.get("name", "")))
            near_miss.sort(key=lambda x: (-x.get("score", 0.0), x.get("name", "")))
            pool_names, pool_tags = _collect_pool(list(slots_i.keys()) + avail_list, tags_by_item)
            near_sorted, near_tiers = _rank_near_miss(
                near_miss,
                pool_names=pool_names,
                pool_tags=pool_tags,
                tags_by_item=tags_by_item,
                limit=limit,
            )
            return {
                "ok": True,
                "slots": slots_i,
                "total": total,
                "remaining": remaining,
                "available": avail_list,
                "cookable": cookable[:limit],
                "near_miss": near_sorted,
                "near_miss_tiers": near_tiers,
                "formula": "score = priority*1000 + weight*100 - missing_penalty",
            }

    cookable: List[Dict[str, Any]] = []
    near_miss: List[Dict[str, Any]] = []
    for r in recipes:
        req = _extract_recipe_requirements(r)
        attrs = _recipe_attrs(r)
        ev = _evaluate_recipe(r, slots=slots_i, tags_by_item=tags_by_item)
        score, penalty = _score_recipe(float(r.priority), float(r.weight), ev.get("missing") or [])
        conditions = _build_conditions(
            r,
            tags_total=ev.get("tags_total") or {},
            names_total=ev.get("names_total") or {},
        )
        row = {
            "name": r.name,
            "priority": float(r.priority),
            "weight": float(r.weight),
            "score": score,
            "penalty": penalty,
            "missing": ev.get("missing") or [],
            "rule_mode": ev.get("rule_mode"),
            "warnings": ev.get("warnings") or [],
            "req_names": req.get("req_names") or [],
            "req_name_groups": req.get("req_name_groups") or [],
            "req_tags": req.get("req_tags") or [],
            "attrs": attrs,
            "conditions": conditions,
            "conditions_ok": bool(ev.get("ok")),
        }
        if ev.get("rule_mode") == "rule":
            possible = _possible_with_remaining(
                _get_rule_constraints(r),
                tags_total=ev.get("tags_total") or {},
                names_total=ev.get("names_total") or {},
                remaining=remaining,
                max_by_tag=max_by_tag,
                available_names=avail_set if avail_set else None,
            )
            if possible:
                cookable.append(row)
            else:
                near_miss.append(row)
        elif ev.get("rule_mode") == "card" and total == 4:
            if ev.get("ok"):
                cookable.append(row)
            else:
                near_miss.append(row)
        else:
            near_miss.append(row)

    cookable.sort(key=lambda x: (-x.get("score", 0.0), x.get("name", "")))
    pool_names, pool_tags = _collect_pool(list(slots_i.keys()) + avail_list, tags_by_item)
    near_sorted, near_tiers = _rank_near_miss(
        near_miss,
        pool_names=pool_names,
        pool_tags=pool_tags,
        tags_by_item=tags_by_item,
        limit=limit,
    )
    return {
        "ok": True,
        "slots": slots_i,
        "total": total,
        "remaining": remaining,
        "cookable": cookable[:limit],
        "near_miss": near_sorted,
        "near_miss_tiers": near_tiers,
        "formula": "score = priority*1000 + weight*100 - missing_penalty",
    }
```

### File: apps/webcraft/i18n_index.py
- mode: full
- size_bytes: 4390
- sha256_12: 0fa925eef1c1

```py
# -*- coding: utf-8 -*-
from __future__ import annotations

import json
import threading
from pathlib import Path
from typing import Any, Dict, List, Optional


class I18nIndexStore:
    """Load + query i18n index JSON (thread-safe)."""

    def __init__(self, path: Path):
        self._path = Path(path)
        self._lock = threading.RLock()
        self._mtime: float = -1.0
        self._doc: Dict[str, Any] = {}
        self.load(force=True)

    @property
    def path(self) -> Path:
        return self._path

    def mtime(self) -> float:
        with self._lock:
            return float(self._mtime or 0)

    def load(self, force: bool = False) -> bool:
        """Load index file if changed. Returns True if reload occurred."""
        with self._lock:
            if not self._path.exists():
                self._doc = {}
                self._mtime = -1.0
                return False
            try:
                mtime = self._path.stat().st_mtime
            except Exception:
                return False
            if (not force) and self._doc and self._mtime == mtime:
                return False
            try:
                doc = json.loads(self._path.read_text(encoding="utf-8"))
            except Exception:
                doc = {}
            self._doc = doc if isinstance(doc, dict) else {}
            self._mtime = mtime
            return True

    def meta(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._doc.get("meta") or {})

    def langs(self) -> List[str]:
        with self._lock:
            langs = self._doc.get("langs")
            if isinstance(langs, list) and langs:
                return sorted({str(x) for x in langs if x})
            names = self._doc.get("names") or {}
            if isinstance(names, dict):
                return sorted({str(k) for k, v in names.items() if k and isinstance(v, dict) and v})
            return []

    def ui_langs(self) -> List[str]:
        with self._lock:
            ui = self._doc.get("ui") or {}
            if isinstance(ui, dict):
                return sorted({str(k) for k, v in ui.items() if k and isinstance(v, dict) and v})
            return []

    def names(self, lang: str) -> Dict[str, str]:
        l = str(lang or "").strip().lower()
        if not l:
            return {}
        with self._lock:
            names = (self._doc.get("names") or {}).get(l) if isinstance(self._doc.get("names"), dict) else None
            if not isinstance(names, dict):
                return {}
            return {str(k): str(v) for k, v in names.items() if k and v}

    def ui_strings(self, lang: str) -> Dict[str, str]:
        l = str(lang or "").strip().lower()
        if not l:
            return {}
        with self._lock:
            ui = (self._doc.get("ui") or {}).get(l) if isinstance(self._doc.get("ui"), dict) else None
            if not isinstance(ui, dict):
                return {}
            return {str(k): str(v) for k, v in ui.items() if k and v}

    def tags(self, lang: str) -> Dict[str, str]:
        l = str(lang or "").strip().lower()
        if not l:
            return {}
        with self._lock:
            tags = (self._doc.get("tags") or {}).get(l) if isinstance(self._doc.get("tags"), dict) else None
            if not isinstance(tags, dict):
                return {}
            return {str(k): str(v) for k, v in tags.items() if k and v}

    def tags_meta(self, lang: str) -> Dict[str, str]:
        l = str(lang or "").strip().lower()
        if not l:
            return {}
        with self._lock:
            meta = (self._doc.get("tags_meta") or {}).get(l) if isinstance(self._doc.get("tags_meta"), dict) else None
            if not isinstance(meta, dict):
                return {}
            return {str(k): str(v) for k, v in meta.items() if k and v}

    def public_meta(self) -> Dict[str, Any]:
        name_langs = self.langs()
        return {
            "enabled": bool(name_langs),
            "langs": name_langs,
            "ui_langs": self.ui_langs(),
            "modes": ["en", "zh", "id"],
            "default_mode": "en",
        }

    def count_names(self, lang: str) -> int:
        return len(self.names(lang))

    def count_ui(self, lang: str) -> int:
        return len(self.ui_strings(lang))

    def count_tags(self, lang: str) -> int:
        return len(self.tags(lang))
```

### File: apps/webcraft/i18n_service.py
- mode: full
- size_bytes: 26143
- sha256_12: 28951d7fce69

```py
# -*- coding: utf-8 -*-
from __future__ import annotations

import hashlib
import json
import os
import threading
import zipfile
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, Optional, Tuple, List


# ----------------------------
# Minimal PO parser (DST)
# ----------------------------


def _po_unquote(s: str) -> str:
    """Unquote a PO string literal line segment.

    PO strings use C-like escapes. We implement a small, predictable subset.
    """

    s = (s or "").strip()
    if not (s.startswith('"') and s.endswith('"')):
        return ""
    inner = s[1:-1]
    out: List[str] = []
    i = 0
    while i < len(inner):
        ch = inner[i]
        if ch == "\\" and i + 1 < len(inner):
            nxt = inner[i + 1]
            if nxt == "n":
                out.append("\n")
            elif nxt == "t":
                out.append("\t")
            elif nxt == "r":
                out.append("\r")
            elif nxt == '"':
                out.append('"')
            elif nxt == "\\":
                out.append("\\")
            else:
                # Best-effort: keep the escaped char
                out.append(nxt)
            i += 2
            continue
        out.append(ch)
        i += 1
    return "".join(out)


def parse_po(text: str) -> Dict[str, str]:
    """Parse a PO file and return mapping: msgctxt -> msgstr.

    Notes
    - We only keep entries with non-empty msgctxt and msgstr.
    - For plural forms, we only take msgstr[0].
    """

    lines = (text or "").splitlines()
    cur: Dict[str, Any] = {}
    last_key: Optional[str] = None
    out: Dict[str, str] = {}

    def commit() -> None:
        nonlocal cur, last_key
        ctx = cur.get("msgctxt")
        msgstr = cur.get("msgstr")
        if isinstance(ctx, str) and ctx and isinstance(msgstr, str) and msgstr:
            out[ctx] = msgstr
        cur = {}
        last_key = None

    for raw in lines:
        line = raw.rstrip("\n")
        s = line.strip()
        if not s:
            commit()
            continue
        if s.startswith("#"):
            continue

        if s.startswith("msgctxt "):
            cur["msgctxt"] = _po_unquote(s[len("msgctxt ") :].strip())
            last_key = "msgctxt"
            continue

        if s.startswith("msgid "):
            # keep for state tracking (and multiline), though we don't use it in output
            cur["msgid"] = _po_unquote(s[len("msgid ") :].strip())
            last_key = "msgid"
            continue

        if s.startswith("msgid_plural "):
            cur["msgid_plural"] = _po_unquote(s[len("msgid_plural ") :].strip())
            last_key = "msgid_plural"
            continue

        if s.startswith("msgstr["):
            # msgstr[0] "..."
            rb = s.find("]")
            idx_s = s[len("msgstr[") : rb].strip() if rb != -1 else ""
            try:
                idx = int(idx_s)
            except Exception:
                idx = -1
            if idx == 0:
                # only take msgstr[0]
                rest = s[rb + 1 :].strip() if rb != -1 else ""
                cur["msgstr"] = _po_unquote(rest)
                last_key = "msgstr"
            else:
                last_key = None
            continue

        if s.startswith("msgstr "):
            cur["msgstr"] = _po_unquote(s[len("msgstr ") :].strip())
            last_key = "msgstr"
            continue

        if s.startswith('"') and last_key:
            # multiline continuation
            cur[last_key] = str(cur.get(last_key) or "") + _po_unquote(s)
            continue

    commit()
    return out


# ----------------------------
# I18n service
# ----------------------------

_NAMES_PREFIX = "STRINGS.NAMES."

# Optional override via environment variable:
# - If set, it takes precedence over engine-mounted scripts source.
_ENV_LANG_PO = {
    "zh": ("WAGSTAFF_PO_ZH", "WAGSTAFF_ZH_PO"),
}

# Default DST language pack locations inside scripts source (zip/folder).
# NOTE: When mounted via WagstaffEngine, both "scripts/..." and "..." may be accepted,
# but we keep explicit candidates to avoid relying on internal normalization.
_ENGINE_PO_CANDIDATES: Dict[str, List[str]] = {
    "zh": [
        "scripts/languages/chinese_s.po",
        "languages/chinese_s.po",
    ],
}


def _norm_lang(lang: str) -> str:
    return str(lang or "").strip().lower()


def _items_sig(item_ids: Iterable[str]) -> str:
    """Stable signature for item ids."""

    keys = sorted([str(k) for k in (item_ids or []) if k])
    h = hashlib.sha1()
    for k in keys:
        h.update(k.encode("utf-8", errors="ignore"))
        h.update(b"\n")
    return f"items:{len(keys)}:{h.hexdigest()}"


def _atomic_write_text(path: Path, text: str) -> bool:
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass

    tmp = path.with_suffix(path.suffix + ".tmp")
    try:
        tmp.write_text(text, encoding="utf-8")
        tmp.replace(path)
        return True
    except Exception:
        try:
            if tmp.exists():
                tmp.unlink()
        except Exception:
            pass
        return False


@dataclass(frozen=True)
class I18nConfig:
    """Runtime i18n configuration.

    lang_to_po:
      - {lang: path_to_po} for external overrides (optional).

    The PO file is expected to contain msgctxt entries like:
      STRINGS.NAMES.AXE
    """

    lang_to_po: Dict[str, Path] = field(default_factory=dict)

    @staticmethod
    def from_env(*, extra: Optional[Dict[str, Path]] = None) -> "I18nConfig":
        mp: Dict[str, Path] = {}

        for lang, keys in _ENV_LANG_PO.items():
            for k in keys:
                v = os.environ.get(k)
                if not v:
                    continue
                p = Path(v).expanduser()
                if p.exists() and p.is_file():
                    mp[_norm_lang(lang)] = p
                    break

        for k, v in (extra or {}).items():
            kk = _norm_lang(str(k))
            if not kk or not v:
                continue
            p2 = Path(v).expanduser()
            if p2.exists() and p2.is_file():
                mp[kk] = p2

        return I18nConfig(lang_to_po=mp)


@dataclass(frozen=True)
class _PoSource:
    kind: str  # 'file' | 'engine' | 'zip'
    file_path: Optional[Path] = None
    engine: Any = None
    zip_path: Optional[Path] = None
    inner_path: Optional[str] = None


class I18nService:
    """Load PO translations and expose item-name translation maps.

    Key behavior
    - Prefer explicit external PO path (env/config)
    - Else try analyzer engine (mounted scripts source)
    - Else try catalog's scripts.zip hint (store.meta['scripts_zip'])
    - Optionally cache compiled id->name mapping to static_dir (served under /static)

    Design goals
    - Keep i18n concerns isolated from CatalogStore.
    - Work even when analyzer engine mounts a "no language" scripts bundle.
    - Keep runtime fast by compiling once and reusing a static JSON.
    """

    def __init__(
        self,
        cfg: I18nConfig,
        *,
        engine: Any = None,
        static_dir: Optional[Path] = None,
        scripts_zip_hint: Optional[str] = None,
        encoding: str = "utf-8",
    ):
        self.cfg = cfg
        self.engine = engine
        self.scripts_zip_hint = str(scripts_zip_hint) if scripts_zip_hint else None
        self.encoding = str(encoding or "utf-8")

        self.static_dir: Optional[Path] = None
        if static_dir is not None:
            try:
                sd = Path(static_dir).expanduser().resolve()
                sd.mkdir(parents=True, exist_ok=True)
                self.static_dir = sd
            except Exception:
                self.static_dir = None

        self._lock = threading.RLock()

        # Raw name cache: lang -> (po_sig, {normalized_key -> zh_name})
        self._raw_cache: Dict[str, Tuple[str, Dict[str, str]]] = {}

        # Item mapping cache: lang -> (assets_sig, po_sig_or_none, {item_id -> name})
        self._item_cache: Dict[str, Tuple[str, Optional[str], Dict[str, str]]] = {}

    # ----------------------------
    # Engine helpers
    # ----------------------------

    def _engine_has(self, eng: Any, inner_path: str) -> bool:
        """Check whether an inner path exists in the mounted scripts source."""

        if eng is None or not inner_path:
            return False

        mode = str(getattr(eng, "mode", "") or "").lower()
        src = getattr(eng, "source", None)

        if mode == "zip" and src is not None and hasattr(src, "getinfo"):
            try:
                src.getinfo(inner_path)
                return True
            except Exception:
                return False

        if mode == "folder":
            base = getattr(eng, "source", None)
            if isinstance(base, str) and base:
                rel = inner_path
                if rel.startswith("scripts/"):
                    rel = rel.replace("scripts/", "", 1)
                p = Path(base) / rel
                return p.exists() and p.is_file()

        # Fallback: best-effort by trying to read.
        try:
            rf = getattr(eng, "read_file", None)
            if callable(rf):
                return rf(inner_path) is not None
        except Exception:
            return False

        return False

    def _resolve_engine_po_path(self, eng: Any, lang: str) -> Optional[str]:
        cand = _ENGINE_PO_CANDIDATES.get(lang) or []
        for p in cand:
            if self._engine_has(eng, p):
                return p
        return None

    def _engine_source_sig(self, eng: Any, inner_path: str) -> str:
        """Compute a cache signature for a PO file loaded from engine."""

        mode = str(getattr(eng, "mode", "") or "").lower()
        src = getattr(eng, "source", None)

        if mode == "zip" and src is not None:
            zip_path = str(getattr(src, "filename", "") or "")
            zip_mtime = 0.0
            zip_size = 0
            try:
                st = Path(zip_path).stat()
                zip_mtime = float(st.st_mtime)
                zip_size = int(st.st_size)
            except Exception:
                pass

            crc = -1
            fsz = -1
            dt = None
            try:
                info = src.getinfo(inner_path) if hasattr(src, "getinfo") else None
                if info is not None:
                    crc = int(getattr(info, "CRC", -1))
                    fsz = int(getattr(info, "file_size", -1))
                    dt = getattr(info, "date_time", None)
            except Exception:
                pass

            return f"enginezip:{zip_path}:{zip_mtime}:{zip_size}:{inner_path}:{crc}:{fsz}:{dt}"

        if mode == "folder":
            base = getattr(eng, "source", None)
            rel = inner_path
            if rel.startswith("scripts/"):
                rel = rel.replace("scripts/", "", 1)
            try:
                p = Path(str(base)) / rel
                st = p.stat()
                return f"enginefolder:{p}:{float(st.st_mtime)}:{int(st.st_size)}"
            except Exception:
                return f"enginefolder:{base}:{inner_path}"

        return f"engine:{id(eng)}:{inner_path}"

    # ----------------------------
    # scripts.zip (hint) helpers
    # ----------------------------

    @staticmethod
    def _zip_has(z: zipfile.ZipFile, inner: str) -> bool:
        try:
            z.getinfo(inner)
            return True
        except Exception:
            return False

    def _resolve_zip_po_path(self, zip_path: Path, lang: str) -> Optional[str]:
        cand = _ENGINE_PO_CANDIDATES.get(lang) or []
        if not cand:
            return None
        try:
            with zipfile.ZipFile(str(zip_path), "r") as z:
                for p in cand:
                    if self._zip_has(z, p):
                        return p
        except Exception:
            return None
        return None

    def _zip_source_sig(self, zip_path: Path, inner_path: str) -> str:
        zip_mtime = 0.0
        zip_size = 0
        try:
            st = Path(zip_path).stat()
            zip_mtime = float(st.st_mtime)
            zip_size = int(st.st_size)
        except Exception:
            pass

        crc = -1
        fsz = -1
        dt = None
        try:
            with zipfile.ZipFile(str(zip_path), "r") as z:
                info = z.getinfo(inner_path)
                crc = int(getattr(info, "CRC", -1))
                fsz = int(getattr(info, "file_size", -1))
                dt = getattr(info, "date_time", None)
        except Exception:
            pass

        return f"zipfile:{zip_path}:{zip_mtime}:{zip_size}:{inner_path}:{crc}:{fsz}:{dt}"

    # ----------------------------
    # Compiled static mapping helpers
    # ----------------------------

    def _compiled_path(self, lang: str) -> Optional[Path]:
        if self.static_dir is None:
            return None
        l = _norm_lang(lang)
        if not l:
            return None
        return self.static_dir / f"names_{l}.json"

    def _read_compiled(self, lang: str) -> Tuple[Optional[str], Optional[str], Dict[str, str]]:
        """Return (po_sig, assets_sig, names) from compiled file."""

        p = self._compiled_path(lang)
        if p is None or not p.exists() or not p.is_file():
            return (None, None, {})

        try:
            doc = json.loads(p.read_text(encoding="utf-8", errors="replace"))
        except Exception:
            return (None, None, {})

        # Backward-compatible: allow file to be just a dict of names.
        if isinstance(doc, dict) and "names" in doc:
            names = doc.get("names")
            if not isinstance(names, dict):
                names = {}
            return (
                str(doc.get("po_sig") or "") or None,
                str(doc.get("assets_sig") or "") or None,
                {str(k): str(v) for k, v in (names or {}).items() if k and v},
            )

        if isinstance(doc, dict):
            return (None, None, {str(k): str(v) for k, v in doc.items() if k and v})

        return (None, None, {})

    def _write_compiled(self, *, lang: str, po_sig: Optional[str], assets_sig: str, names: Dict[str, str]) -> None:
        p = self._compiled_path(lang)
        if p is None:
            return

        doc = {
            "lang": _norm_lang(lang),
            "po_sig": str(po_sig or "") or None,
            "assets_sig": str(assets_sig or ""),
            "count": int(len(names or {})),
            "names": dict(names or {}),
        }
        txt = json.dumps(doc, ensure_ascii=False, separators=(",", ":"))
        _atomic_write_text(p, txt)

    # ----------------------------
    # PO source resolving
    # ----------------------------

    def _resolve_po_source(
        self,
        *,
        lang: str,
        engine: Any = None,
        scripts_zip_hint: Optional[str] = None,
    ) -> Optional[_PoSource]:
        l = _norm_lang(lang)
        if not l:
            return None

        # 1) External override: explicit path.
        p = (self.cfg.lang_to_po or {}).get(l)
        if p:
            try:
                if p.exists() and p.is_file():
                    return _PoSource(kind="file", file_path=p)
            except Exception:
                pass

        # 2) Engine-mounted scripts source.
        eng = engine if engine is not None else self.engine
        if eng is not None:
            inner = self._resolve_engine_po_path(eng, l)
            if inner:
                return _PoSource(kind="engine", engine=eng, inner_path=inner)

        # 3) Catalog-provided scripts.zip hint (usually DST's scripts.zip with languages).
        hint = str(scripts_zip_hint or self.scripts_zip_hint or "").strip()
        if hint:
            zp = Path(hint).expanduser()
            try:
                if zp.exists() and zp.is_file():
                    inner2 = self._resolve_zip_po_path(zp, l)
                    if inner2:
                        return _PoSource(kind="zip", zip_path=zp, inner_path=inner2)
            except Exception:
                pass

        return None

    def _po_sig(self, *, lang: str, engine: Any = None, scripts_zip_hint: Optional[str] = None) -> Optional[str]:
        src = self._resolve_po_source(lang=lang, engine=engine, scripts_zip_hint=scripts_zip_hint)
        if src is None:
            return None

        if src.kind == "file" and src.file_path is not None:
            try:
                st = src.file_path.stat()
                return f"file:{src.file_path}:{float(st.st_mtime)}:{int(st.st_size)}"
            except Exception:
                return None

        if src.kind == "engine" and src.engine is not None and src.inner_path:
            try:
                return self._engine_source_sig(src.engine, src.inner_path)
            except Exception:
                return None

        if src.kind == "zip" and src.zip_path is not None and src.inner_path:
            try:
                return self._zip_source_sig(src.zip_path, src.inner_path)
            except Exception:
                return None

        return None

    def _read_po_text(
        self,
        *,
        lang: str,
        engine: Any = None,
        scripts_zip_hint: Optional[str] = None,
    ) -> Tuple[Optional[str], Optional[str]]:
        """Return (po_sig, po_text)."""

        src = self._resolve_po_source(lang=lang, engine=engine, scripts_zip_hint=scripts_zip_hint)
        if src is None:
            return (None, None)

        sig = self._po_sig(lang=lang, engine=engine, scripts_zip_hint=scripts_zip_hint)

        if src.kind == "file" and src.file_path is not None:
            try:
                txt = src.file_path.read_text(encoding=self.encoding, errors="replace")
                return (sig, txt)
            except Exception:
                return (None, None)

        if src.kind == "engine" and src.engine is not None and src.inner_path:
            try:
                rf = getattr(src.engine, "read_file", None)
                txt = rf(src.inner_path) if callable(rf) else None
                if isinstance(txt, bytes):
                    try:
                        txt = txt.decode(self.encoding, errors="replace")
                    except Exception:
                        txt = txt.decode("utf-8", errors="replace")
                if not txt:
                    return (None, None)
                return (sig, str(txt))
            except Exception:
                return (None, None)

        if src.kind == "zip" and src.zip_path is not None and src.inner_path:
            try:
                with zipfile.ZipFile(str(src.zip_path), "r") as z:
                    data = z.read(src.inner_path)
                try:
                    txt2 = data.decode(self.encoding, errors="replace")
                except Exception:
                    txt2 = data.decode("utf-8", errors="replace")
                return (sig, txt2)
            except Exception:
                return (None, None)

        return (None, None)

    # ----------------------------
    # Public API
    # ----------------------------

    def available_langs(self, *, engine: Any = None, scripts_zip_hint: Optional[str] = None) -> List[str]:
        eng = engine if engine is not None else self.engine
        langs: List[str] = []

        # 1) Config-provided overrides
        for k in (self.cfg.lang_to_po or {}).keys():
            kk = _norm_lang(str(k))
            if kk and kk not in langs:
                langs.append(kk)

        # 2) Compiled static files
        for k in _ENGINE_PO_CANDIDATES.keys():
            kk = _norm_lang(str(k))
            if not kk or kk in langs:
                continue
            p = self._compiled_path(kk)
            if p is not None and p.exists():
                # only enable if it has some names
                _, _, mp = self._read_compiled(kk)
                if mp:
                    langs.append(kk)

        # 3) Engine-provided defaults
        for k in _ENGINE_PO_CANDIDATES.keys():
            kk = _norm_lang(str(k))
            if not kk or kk in langs:
                continue
            if eng is not None and self._resolve_engine_po_path(eng, kk):
                langs.append(kk)

        # 4) scripts.zip hint
        hint = str(scripts_zip_hint or self.scripts_zip_hint or "").strip()
        if hint:
            zp = Path(hint).expanduser()
            for k in _ENGINE_PO_CANDIDATES.keys():
                kk = _norm_lang(str(k))
                if not kk or kk in langs:
                    continue
                try:
                    if zp.exists() and zp.is_file() and self._resolve_zip_po_path(zp, kk):
                        langs.append(kk)
                except Exception:
                    continue

        langs.sort()
        return langs

    def public_meta(self, *, engine: Any = None, scripts_zip_hint: Optional[str] = None) -> Dict[str, Any]:
        langs = self.available_langs(engine=engine, scripts_zip_hint=scripts_zip_hint)
        return {
            "enabled": bool(langs),
            "langs": langs,
            "modes": ["en", "zh", "id"],
            "default_mode": "en",
        }

    def _load_names_raw(self, lang: str, *, engine: Any = None, scripts_zip_hint: Optional[str] = None) -> Tuple[Optional[str], Dict[str, str]]:
        """Return (po_sig, {normalized_key -> localized_name})."""

        l = _norm_lang(lang)
        if not l:
            return (None, {})

        sig, txt = self._read_po_text(lang=l, engine=engine, scripts_zip_hint=scripts_zip_hint)
        if not sig or not txt:
            return (None, {})

        with self._lock:
            cached = self._raw_cache.get(l)
            if cached and str(cached[0]) == str(sig):
                return (str(sig), dict(cached[1]))

        ctx_map = parse_po(txt)
        names: Dict[str, str] = {}
        for ctx, val in ctx_map.items():
            if not isinstance(ctx, str) or not ctx.startswith(_NAMES_PREFIX):
                continue
            key = ctx[len(_NAMES_PREFIX) :].strip()
            if not key:
                continue
            kid = key.strip().lower()
            if not kid:
                continue
            v = str(val).strip()
            if not v:
                continue
            names[kid] = v
            # Common alt-id: strip underscores
            if "_" in kid:
                names.setdefault(kid.replace("_", ""), v)

        with self._lock:
            self._raw_cache[l] = (str(sig), dict(names))

        return (str(sig), dict(names))

    def item_name_map(
        self,
        *,
        lang: str,
        assets: Dict[str, Any],
        item_ids: Optional[Iterable[str]] = None,
        engine: Any = None,
        scripts_zip_hint: Optional[str] = None,
    ) -> Dict[str, str]:
        """Build {item_id: localized_name} for item ids.

        If static_dir is configured, it will also compile/cache the mapping as:
          <static_dir>/names_<lang>.json
        """

        l = _norm_lang(lang)
        if not l or l in ("en", "id"):
            return {}

        ids = list(item_ids) if item_ids is not None else list((assets or {}).keys())
        aset_sig = _items_sig(ids)

        with self._lock:
            ic = self._item_cache.get(l)
            if ic and str(ic[0]) == str(aset_sig):
                return dict(ic[2] or {})

        # 1) If we have a compiled file, use it (and verify freshness when possible).
        po_sig_file, aset_sig_file, compiled = self._read_compiled(l)
        if compiled:
            if not aset_sig_file or str(aset_sig_file) == str(aset_sig):
                # If we can compute current po sig, ensure it matches; otherwise trust compiled.
                cur_po_sig = self._po_sig(lang=l, engine=engine, scripts_zip_hint=scripts_zip_hint)
                if (not cur_po_sig) or (po_sig_file and str(po_sig_file) == str(cur_po_sig)):
                    with self._lock:
                        self._item_cache[l] = (str(aset_sig), po_sig_file or cur_po_sig, dict(compiled))
                    return dict(compiled)

        # 2) Rebuild from PO (engine or scripts.zip hint or explicit file).
        po_sig, raw = self._load_names_raw(l, engine=engine, scripts_zip_hint=scripts_zip_hint)
        if not raw:
            # No source now; fall back to compiled content if any (filter to current assets if possible).
            if compiled:
                out2 = {iid: compiled[iid] for iid in ids if iid in compiled}
                with self._lock:
                    self._item_cache[l] = (str(aset_sig), po_sig_file, dict(out2))
                return out2
            return {}

        out: Dict[str, str] = {}
        for iid in ids:
            if not iid:
                continue
            k1 = str(iid).strip().lower()
            if not k1:
                continue
            k2 = k1.replace("_", "")
            v = raw.get(k1) or raw.get(k2)
            if v:
                out[str(iid)] = v

        # 3) Persist compiled mapping (best-effort).
        try:
            self._write_compiled(lang=l, po_sig=po_sig, assets_sig=aset_sig, names=out)
        except Exception:
            pass

        with self._lock:
            self._item_cache[l] = (str(aset_sig), po_sig, dict(out))

        return out

    def warmup(
        self,
        *,
        assets: Dict[str, Any],
        item_ids: Optional[Iterable[str]] = None,
        engine: Any = None,
        scripts_zip_hint: Optional[str] = None,
        langs: Optional[List[str]] = None,
    ) -> None:
        """Best-effort precompile.

        This is safe to call at server startup.
        """

        ls = langs or ["zh"]
        for l in ls:
            try:
                self.item_name_map(
                    lang=str(l),
                    assets=assets,
                    item_ids=item_ids,
                    engine=engine,
                    scripts_zip_hint=scripts_zip_hint,
                )
            except Exception:
                continue
```

### File: apps/webcraft/icon_service.py
- mode: full
- size_bytes: 12077
- sha256_12: 96a6a2513eea

```py
# -*- coding: utf-8 -*-
from __future__ import annotations

import re
import threading
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

from core.assets.klei_atlas_tex import (
    Atlas,
    decode_ktex_to_image,
    parse_atlas_xml,
    pick_first_existing,
    resolve_tex_path_from_atlas,
    extract_atlas_element,
)


_SAFE_ID_RE = re.compile(r"^[A-Za-z0-9_]+$")
_ICON_ALIAS_ELEMENTS = {
    "onion": ["quagmire_onion"],
    "onion_cooked": ["quagmire_onion_cooked"],
    "tomato": ["quagmire_tomato"],
    "tomato_cooked": ["quagmire_tomato_cooked"],
}


@dataclass(frozen=True)
class IconConfig:
    """Runtime icon configuration.

    mode:
      - off      : UI shows no icons (always fallback to text)
      - static   : serve only prebuilt PNG icons from static_dir
      - dynamic  : generate icons from (atlas xml + tex) on demand (cached as png)
      - auto     : static first, if missing then dynamic

    static_dir:
      - directory that contains <id>.png files (served via /static/data/icons/...)

    game_data_dir:
      - directory root where atlas/xml/tex can be resolved, e.g. DST data folder that contains "images/..."
        Example: /path/to/Don't Starve Together/data

    unpremultiply:
      - whether to unpremultiply alpha for cropped icons (usually True for DST inventory icons)
    """

    mode: str = "auto"
    static_dir: Path = Path("data/static/icons")
    game_data_dir: Optional[Path] = None
    unpremultiply: bool = True

    def normalized_mode(self) -> str:
        m = (self.mode or "").strip().lower()
        if m in ("off", "0", "false", "none"):
            return "off"
        if m in ("static", "png"):
            return "static"
        if m in ("dynamic", "tex"):
            return "dynamic"
        return "auto"

    def to_public_dict(self) -> Dict[str, Any]:
        # URL bases are fixed by server routing.
        return {
            "mode": self.normalized_mode(),
            "static_base": "/static/data/icons",
            "api_base": "/api/v1/icon",
        }


class IconService:
    """Icon build/serve helper.

    The service is safe for concurrent requests.
    """

    def __init__(self, cfg: IconConfig):
        self.cfg = cfg
        self._lock = threading.RLock()
        self._atlas_cache: Dict[Path, Tuple[float, Atlas]] = {}
        self._tex_cache: Dict[Path, Tuple[float, Any]] = {}  # PIL.Image.Image, but keep Any to avoid import
        self._tex_cache_order: list[Path] = []
        self._tex_cache_max = 4
        self._inventory_atlases: Optional[list[Path]] = None

        # Ensure static directory exists (even in off mode).
        try:
            self.cfg.static_dir.mkdir(parents=True, exist_ok=True)
        except Exception:
            pass

    @staticmethod
    def is_safe_id(item_id: str) -> bool:
        return bool(item_id) and bool(_SAFE_ID_RE.match(item_id))

    def icon_path(self, item_id: str) -> Path:
        return self.cfg.static_dir / f"{item_id}.png"

    def resolve_existing(self, item_id: str) -> Optional[Path]:
        if not self.is_safe_id(item_id):
            return None
        p = self.icon_path(item_id)
        return p if p.exists() else None

    def ensure_icon(self, item_id: str, asset: Optional[Dict[str, Any]] = None) -> Optional[Path]:
        """Ensure icon PNG exists and return its path.

        Behavior depends on cfg.mode.
        """

        if not self.is_safe_id(item_id):
            return None

        mode = self.cfg.normalized_mode()
        if mode == "off":
            return None

        # 1) static
        p = self.icon_path(item_id)
        if p.exists():
            return p

        alias = self._resolve_existing_alias(item_id, asset=asset)
        if alias is not None:
            return alias

        if mode == "static":
            return None

        # 2) dynamic
        if mode in ("dynamic", "auto"):
            gd = self.cfg.game_data_dir
            if gd is None:
                return None
            ok = False
            if asset:
                ok = self._build_from_asset(item_id, asset, out_path=p)
            if not ok:
                ok = self._build_from_inventory_alias(item_id, out_path=p)
            return p if ok and p.exists() else None

        return None

    # ----------------- internals -----------------
    def _resolve_existing_alias(self, item_id: str, asset: Optional[Dict[str, Any]] = None) -> Optional[Path]:
        for cand in self._alias_candidates(item_id, asset=asset):
            if not self.is_safe_id(cand):
                continue
            p = self.icon_path(cand)
            if p.exists():
                return p
        return None

    def _alias_candidates(self, item_id: str, asset: Optional[Dict[str, Any]] = None) -> list[str]:
        if not item_id:
            return []

        out: list[str] = []

        def _push(v: Optional[str]) -> None:
            if not v:
                return
            if v == item_id:
                return
            if v not in out:
                out.append(v)

        # explicit known aliases
        explicit = {
            "waterpump": ["waterpump_item"],
            "hermit_bundle_shells": ["hermit_bundle"],
            "dragonboat_kit": ["dragonboat_pack"],
            "onion": ["quagmire_onion"],
            "onion_cooked": ["quagmire_onion_cooked"],
            "tomato": ["quagmire_tomato"],
            "tomato_cooked": ["quagmire_tomato_cooked"],
        }
        for v in explicit.get(item_id, []):
            _push(v)

        if item_id.startswith("wintercooking_"):
            _push(item_id[len("wintercooking_") :])

        if item_id.endswith("_builder"):
            base = item_id[: -len("_builder")]
            _push(f"{base}_sketch")
            _push(base)

        if item_id.endswith("_kit"):
            base = item_id[: -len("_kit")]
            _push(f"{base}_item")
            _push(f"{base}_pack")
            _push(base)

        if item_id.endswith("_item"):
            _push(item_id[: -len("_item")])

        if item_id.endswith("_blueprint"):
            _push(item_id[: -len("_blueprint")])

        if item_id.endswith("_recipe"):
            _push(item_id[: -len("_recipe")])

        if asset:
            image_rel = str(asset.get("image") or "").strip()
            if image_rel:
                base = Path(image_rel).name
                if base.lower().endswith(".tex"):
                    base = base[: -len(".tex")]
                _push(base)

        return out

    def _read_atlas(self, xml_path: Path) -> Optional[Atlas]:
        try:
            mtime = xml_path.stat().st_mtime
        except Exception:
            return None

        with self._lock:
            cached = self._atlas_cache.get(xml_path)
            if cached and cached[0] == mtime:
                return cached[1]

        try:
            xml_text = xml_path.read_text(encoding="utf-8", errors="ignore")
            atlas = parse_atlas_xml(xml_text)
        except Exception:
            return None

        with self._lock:
            self._atlas_cache[xml_path] = (mtime, atlas)
        return atlas

    def _read_tex_image(self, tex_path: Path):
        """Decode and cache tex image (mipmap0)."""
        try:
            mtime = tex_path.stat().st_mtime
        except Exception:
            return None

        with self._lock:
            cached = self._tex_cache.get(tex_path)
            if cached and cached[0] == mtime:
                return cached[1]

        try:
            tex_bytes = tex_path.read_bytes()
            img = decode_ktex_to_image(tex_bytes)
        except Exception:
            return None

        with self._lock:
            self._tex_cache[tex_path] = (mtime, img)
            # LRU bookkeeping
            if tex_path in self._tex_cache_order:
                self._tex_cache_order.remove(tex_path)
            self._tex_cache_order.append(tex_path)
            while len(self._tex_cache_order) > self._tex_cache_max:
                old = self._tex_cache_order.pop(0)
                self._tex_cache.pop(old, None)

        return img

    def _build_from_asset(self, item_id: str, asset: Dict[str, Any], *, out_path: Path) -> bool:
        """Build <item_id>.png from one catalog assets entry."""

        gd = self.cfg.game_data_dir
        if gd is None:
            return False

        atlas_rel = str(asset.get("atlas") or "").strip()
        if not atlas_rel:
            return False

        # normalize leading slashes
        while atlas_rel.startswith("/"):
            atlas_rel = atlas_rel[1:]

        xml_path = (gd / atlas_rel).resolve()
        if not xml_path.exists():
            return False

        atlas = self._read_atlas(xml_path)
        if not atlas:
            return False

        tex_path = resolve_tex_path_from_atlas(xml_path, atlas)
        if not tex_path or not tex_path.exists():
            # fallback: swap suffix
            alt = xml_path.with_suffix(".tex")
            if alt.exists():
                tex_path = alt
            else:
                return False

        tex_img = self._read_tex_image(tex_path)
        if tex_img is None:
            return False

        # Pick element name candidates
        image_rel = str(asset.get("image") or "").strip()
        candidates = []
        if image_rel:
            base = Path(image_rel).name
            if base and not base.lower().endswith(".tex"):
                base = base + ".tex"
            if base:
                candidates.append(base)
        candidates.append(f"{item_id}.tex")
        if "_" in item_id:
            candidates.append(f"{item_id.replace('_','')}.tex")

        element_name = pick_first_existing(candidates, atlas.elements)
        if not element_name:
            return False

        invert_v = self._invert_v_for_atlas(atlas_rel)
        cropped = extract_atlas_element(
            atlas,
            tex_img,
            element_name,
            unpremultiply=bool(self.cfg.unpremultiply),
            invert_v=invert_v,
        )
        if cropped is None:
            return False

        try:
            out_path.parent.mkdir(parents=True, exist_ok=True)
            cropped.save(out_path, format="PNG")
            return True
        except Exception:
            return False

    def _build_from_inventory_alias(self, item_id: str, *, out_path: Path) -> bool:
        alias_names = _ICON_ALIAS_ELEMENTS.get(item_id)
        if not alias_names:
            return False
        atlases = self._inventory_atlas_paths()
        if not atlases:
            return False
        for xml_path in atlases:
            atlas_rel = self._atlas_rel(xml_path)
            if not atlas_rel:
                continue
            for name in alias_names:
                asset = {"atlas": atlas_rel, "image": f"{name}.tex"}
                if self._build_from_asset(item_id, asset, out_path=out_path):
                    return True
        return False

    def _inventory_atlas_paths(self) -> list[Path]:
        if self._inventory_atlases is not None:
            return list(self._inventory_atlases)
        gd = self.cfg.game_data_dir
        if gd is None:
            self._inventory_atlases = []
            return []
        img_dir = gd / "images"
        if not img_dir.is_dir():
            self._inventory_atlases = []
            return []
        paths = sorted(img_dir.glob("inventoryimages*.xml"), key=lambda p: p.name.lower())
        self._inventory_atlases = paths
        return list(paths)

    def _atlas_rel(self, xml_path: Path) -> str:
        gd = self.cfg.game_data_dir
        if gd is None:
            return ""
        try:
            rel = xml_path.resolve().relative_to(gd.resolve())
        except Exception:
            return ""
        return str(rel).replace("\\", "/")

    @staticmethod
    def _invert_v_for_atlas(atlas_rel: str) -> bool:
        p = str(atlas_rel or "").lower()
        if "inventoryimages" in p:
            return False
        return True
```

### File: apps/webcraft/mechanism_store.py
- mode: full
- size_bytes: 15278
- sha256_12: f247804f6708

```py
# -*- coding: utf-8 -*-
from __future__ import annotations

import json
import sqlite3
import threading
from pathlib import Path
from typing import Any, Dict, List, Optional

_SQLITE_SUFFIXES = (".sqlite", ".sqlite3", ".db")


def _is_sqlite_path(path: Path) -> bool:
    return path.suffix.lower() in _SQLITE_SUFFIXES


def _find_sqlite_peer(path: Path) -> Optional[Path]:
    if path.suffix.lower() != ".json":
        return None
    for ext in _SQLITE_SUFFIXES:
        candidate = path.with_suffix(ext)
        if candidate.exists():
            return candidate
    return None


def _load_json(value: Any) -> Any:
    if value is None:
        return None
    if isinstance(value, (dict, list)):
        return value
    try:
        return json.loads(value)
    except Exception:
        return value


class MechanismError(RuntimeError):
    pass


class MechanismStore:
    """Load + query mechanism index (thread-safe)."""

    def __init__(self, path: Path):
        resolved = self.resolve_path(Path(path))
        self._path = resolved
        self._use_sqlite = _is_sqlite_path(self._path)
        self._lock = threading.RLock()
        self._mtime: float = -1.0

        self._doc: Dict[str, Any] = {}
        self._meta: Dict[str, Any] = {}
        self._counts: Dict[str, Any] = {}

        self._components: Dict[str, Dict[str, Any]] = {}
        self._component_ids: List[str] = []
        self._prefabs: Dict[str, Dict[str, Any]] = {}
        self._prefab_ids: List[str] = []
        self._component_usage: Dict[str, List[str]] = {}
        self._links: List[Dict[str, Any]] = []

        self.load(force=True)

    @staticmethod
    def resolve_path(path: Path) -> Path:
        if _is_sqlite_path(path):
            return path
        peer = _find_sqlite_peer(path)
        return peer if peer else path

    @property
    def path(self) -> Path:
        return self._path

    def mtime(self) -> float:
        with self._lock:
            return float(self._mtime or 0)

    def meta(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._meta)

    def counts(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._counts)

    def schema_version(self) -> int:
        with self._lock:
            return int(self._doc.get("schema_version") or (self._meta or {}).get("schema") or 0)

    def component_ids(self) -> List[str]:
        with self._lock:
            return list(self._component_ids)

    def prefab_ids(self) -> List[str]:
        with self._lock:
            return list(self._prefab_ids)

    def get_component(self, component_id: str) -> Optional[Dict[str, Any]]:
        cid = str(component_id or "").strip()
        if not cid:
            return None
        with self._lock:
            row = self._components.get(cid)
            if row is None and cid.lower() != cid:
                row = self._components.get(cid.lower())
            return dict(row) if isinstance(row, dict) else None

    def get_prefab(self, prefab_id: str) -> Optional[Dict[str, Any]]:
        pid = str(prefab_id or "").strip()
        if not pid:
            return None
        with self._lock:
            row = self._prefabs.get(pid)
            if row is None and pid.lower() != pid:
                row = self._prefabs.get(pid.lower())
            return dict(row) if isinstance(row, dict) else None

    def component_usage(self, component_id: str) -> List[str]:
        cid = str(component_id or "").strip()
        if not cid:
            return []
        with self._lock:
            if cid in self._component_usage:
                return list(self._component_usage.get(cid) or [])
            if cid.lower() in self._component_usage:
                return list(self._component_usage.get(cid.lower()) or [])
            return []

    def prefabs_for_component(self, component_id: str) -> List[str]:
        return self.component_usage(component_id)

    def search_components(self, query: str) -> List[str]:
        q = str(query or "").strip().lower()
        if not q:
            return self.component_ids()
        out: List[str] = []
        with self._lock:
            for cid in self._component_ids:
                row = self._components.get(cid) or {}
                hay = [cid, row.get("class_name")] + list(row.get("aliases") or [])
                if any(q in str(v).lower() for v in hay if v):
                    out.append(cid)
        return out

    def search_prefabs(self, query: str) -> List[str]:
        q = str(query or "").strip().lower()
        if not q:
            return self.prefab_ids()
        out: List[str] = []
        with self._lock:
            for pid in self._prefab_ids:
                if q in pid.lower():
                    out.append(pid)
        return out

    def links(self) -> List[Dict[str, Any]]:
        with self._lock:
            return [dict(row) for row in self._links]

    def load(self, force: bool = False) -> bool:
        """Load index if changed. Returns True if reload occurred."""
        with self._lock:
            try:
                mtime = self._path.stat().st_mtime
            except FileNotFoundError as exc:
                raise MechanismError(f"Mechanism index not found: {self._path}") from exc

            if (not force) and self._doc and self._mtime == mtime:
                return False

            doc = self._load_doc()
            self._validate(doc)

            self._doc = doc
            self._meta = doc.get("meta") or {}
            self._mtime = mtime

            self._build_indexes(doc)
            return True

    def _load_doc(self) -> Dict[str, Any]:
        if self._use_sqlite:
            return self._load_doc_from_sqlite(self._path)
        return json.loads(self._path.read_text(encoding="utf-8"))

    def _load_doc_from_sqlite(self, path: Path) -> Dict[str, Any]:
        try:
            conn = sqlite3.connect(str(path))
            conn.row_factory = sqlite3.Row
        except Exception as exc:
            raise MechanismError(f"Failed to open SQLite mechanism index: {path}") from exc
        try:
            cur = conn.cursor()
            tables = {row["name"] for row in cur.execute("SELECT name FROM sqlite_master WHERE type='table'")}
            required = {"meta", "components", "prefabs", "prefab_components", "links"}
            missing = sorted(required - tables)
            if missing:
                raise MechanismError(f"SQLite mechanism index missing tables: {', '.join(missing)}")

            meta_rows = {row["key"]: row["value"] for row in cur.execute("SELECT key, value FROM meta")}
            comp_rows = cur.execute(
                """
                SELECT id, class_name, path, aliases_json, methods_json, fields_json, events_json, requires_json, raw_json
                FROM components
                """
            ).fetchall()
            prefab_cols = {row["name"] for row in cur.execute("PRAGMA table_info(prefabs)")}
            prefab_select = [
                "id",
                "components_json",
                "tags_json",
                "brains_json",
                "stategraphs_json",
                "helpers_json",
                "files_json",
                "raw_json",
            ]
            for col in ("events_json", "assets_json", "component_calls_json"):
                if col in prefab_cols:
                    prefab_select.append(col)
            prefab_rows = cur.execute(f"SELECT {', '.join(prefab_select)} FROM prefabs").fetchall()
            prefab_component_rows = cur.execute(
                "SELECT prefab_id, component_id FROM prefab_components"
            ).fetchall()
            link_cols = {row["name"] for row in cur.execute("PRAGMA table_info(links)")}
            link_select = "source, source_id, target, target_id"
            if "relation" in link_cols:
                link_select = f"{link_select}, relation"
            link_rows = cur.execute(f"SELECT {link_select} FROM links").fetchall()
        except Exception as exc:
            raise MechanismError(f"SQLite mechanism index query failed: {path}") from exc
        finally:
            conn.close()

        meta_obj = _load_json(meta_rows.get("meta")) or {}
        schema_version = _load_json(meta_rows.get("schema_version"))
        if schema_version is None:
            schema_version = (meta_obj or {}).get("schema") or 0

        components: Dict[str, Dict[str, Any]] = {}
        for row in comp_rows:
            cid = str(row["id"] or "").strip()
            if not cid:
                continue
            raw = _load_json(row["raw_json"])
            if isinstance(raw, dict):
                data = dict(raw)
            else:
                data = {
                    "type": "component",
                    "id": cid,
                    "class_name": row["class_name"],
                    "path": row["path"],
                    "aliases": _load_json(row["aliases_json"]) or [],
                    "methods": _load_json(row["methods_json"]) or [],
                    "fields": _load_json(row["fields_json"]) or [],
                    "events": _load_json(row["events_json"]) or [],
                    "requires": _load_json(row["requires_json"]) or [],
                }
            components[cid] = data

        prefabs: Dict[str, Dict[str, Any]] = {}
        for row in prefab_rows:
            pid = str(row["id"] or "").strip()
            if not pid:
                continue
            raw = _load_json(row["raw_json"])
            if isinstance(raw, dict):
                data = dict(raw)
            else:
                data = {
                    "components": _load_json(row["components_json"]) or [],
                    "tags": _load_json(row["tags_json"]) or [],
                    "brains": _load_json(row["brains_json"]) or [],
                    "stategraphs": _load_json(row["stategraphs_json"]) or [],
                    "helpers": _load_json(row["helpers_json"]) or [],
                    "files": _load_json(row["files_json"]) or [],
                }
                if "events_json" in row.keys():
                    data["events"] = _load_json(row["events_json"]) or []
                if "assets_json" in row.keys():
                    data["assets"] = _load_json(row["assets_json"]) or []
                if "component_calls_json" in row.keys():
                    data["component_calls"] = _load_json(row["component_calls_json"]) or []
            prefabs[pid] = data

        component_usage: Dict[str, List[str]] = {}
        for row in prefab_component_rows:
            pid = str(row["prefab_id"] or "").strip()
            cid = str(row["component_id"] or "").strip()
            if not pid or not cid:
                continue
            component_usage.setdefault(cid, []).append(pid)
        component_usage = {k: sorted(set(v)) for k, v in component_usage.items()}

        links: List[Dict[str, Any]] = []
        for row in link_rows:
            src = row["source"]
            tgt = row["target"]
            if not src or not tgt:
                continue
            link = {
                "source": src,
                "source_id": row["source_id"],
                "target": tgt,
                "target_id": row["target_id"],
            }
            if "relation" in row.keys():
                link["relation"] = row["relation"]
            links.append(link)

        counts = {
            "components_total": len(components),
            "prefabs_total": len(prefabs),
            "components_used": len(component_usage),
            "prefab_component_edges": len({(row["prefab_id"], row["component_id"]) for row in prefab_component_rows}),
        }

        return {
            "schema_version": schema_version,
            "meta": meta_obj,
            "counts": counts,
            "components": {"total_files": len(components), "items": components},
            "prefabs": {"items": prefabs},
            "component_usage": component_usage,
            "links": {"prefab_component": links},
        }

    def _validate(self, doc: Dict[str, Any]) -> None:
        if not isinstance(doc, dict):
            raise MechanismError("Mechanism index root must be a JSON object")
        if "meta" not in doc:
            raise MechanismError("Mechanism index missing key: meta")

    def _build_indexes(self, doc: Dict[str, Any]) -> None:
        comp_items = (doc.get("components") or {}).get("items") or {}
        prefab_items = (doc.get("prefabs") or {}).get("items") or {}
        usage = doc.get("component_usage") or {}
        links = (doc.get("links") or {}).get("prefab_component") or []

        components: Dict[str, Dict[str, Any]] = {}
        for cid, row in comp_items.items():
            if not cid or not isinstance(row, dict):
                continue
            data = dict(row)
            data.setdefault("id", str(cid))
            data.setdefault("type", "component")
            components[str(cid)] = data

        prefabs: Dict[str, Dict[str, Any]] = {}
        for pid, row in prefab_items.items():
            if not pid or not isinstance(row, dict):
                continue
            data = dict(row)
            data.setdefault("id", str(pid))
            prefabs[str(pid)] = data

        usage_map: Dict[str, List[str]] = {}
        if isinstance(usage, dict):
            for cid, pids in usage.items():
                if not cid or not isinstance(pids, list):
                    continue
                usage_map[str(cid)] = sorted({str(pid) for pid in pids if pid})

        links_list: List[Dict[str, Any]] = []
        if isinstance(links, list):
            for row in links:
                if not isinstance(row, dict):
                    continue
                links_list.append(dict(row))

        if not usage_map and links_list:
            usage_map = {}
            for row in links_list:
                cid = row.get("target_id")
                pid = row.get("source_id")
                if row.get("source") != "prefab" or row.get("target") != "component":
                    continue
                if not cid or not pid:
                    continue
                usage_map.setdefault(str(cid), set()).add(str(pid))
            usage_map = {k: sorted(v) for k, v in usage_map.items()}

        if not links_list and usage_map:
            links_list = [
                {"source": "prefab", "source_id": pid, "target": "component", "target_id": cid}
                for cid, pids in usage_map.items()
                for pid in pids
            ]

        counts = dict(doc.get("counts") or {})
        counts.setdefault("components_total", len(components))
        counts.setdefault("prefabs_total", len(prefabs))
        counts.setdefault("components_used", len(usage_map))
        counts.setdefault(
            "prefab_component_edges",
            len({(row.get("source_id"), row.get("target_id")) for row in links_list if row.get("source") == "prefab"}),
        )

        self._components = components
        self._component_ids = sorted(components.keys())
        self._prefabs = prefabs
        self._prefab_ids = sorted(prefabs.keys())
        self._component_usage = usage_map
        self._links = links_list
        self._counts = counts
```

### File: apps/webcraft/planner.py
- mode: full
- size_bytes: 3808
- sha256_12: a9ac339eaf54

```py
# -*- coding: utf-8 -*-
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

from .catalog_store import CraftRecipe


def normalize_inventory(inv: Dict[str, Any]) -> Dict[str, float]:
    out: Dict[str, float] = {}
    for k, v in (inv or {}).items():
        key = str(k).strip()
        if not key:
            continue
        try:
            num = float(v)
        except Exception:
            continue
        if num <= 0:
            continue
        out[key] = num
    return out


def _recipe_requires_builder_tag(recipe: CraftRecipe) -> Optional[str]:
    # builder_tags preferred
    if recipe.builder_tags:
        # a recipe can accept multiple tags; treat as "any-of"
        return ",".join(recipe.builder_tags)
    return None


def is_builder_allowed(recipe: CraftRecipe, builder_tag: Optional[str], strict: bool = False) -> Tuple[bool, str]:
    """Check if builder can craft this recipe.

    strict:
      - if True: treat builder_skill as locked (unknown to us), require builder_tag for builder_tags
      - if False: only enforce builder_tags; ignore builder_skill
    """
    bt_required = recipe.builder_tags or []
    if bt_required:
        if not builder_tag:
            return False, "missing_builder_tag"
        if builder_tag not in bt_required:
            return False, "builder_tag_mismatch"

    if strict and recipe.builder_skill:
        # we don't model skill tree yet; mark as blocked
        return False, "builder_skill_locked"

    return True, ""


@dataclass
class MissingItem:
    item: str
    need: float
    have: float
    reason: str = ""


def missing_for(recipe: CraftRecipe, inventory: Dict[str, float]) -> List[MissingItem]:
    """Compute missing materials for a recipe based on numeric amounts.

    If amount_num is missing/unresolvable => treated as missing with reason 'unresolved_amount'.
    """
    inv = inventory or {}
    missing: List[MissingItem] = []

    # unresolved list from catalog (e.g. CHARACTER_INGREDIENT.HEALTH)
    for unresolved in (recipe.ingredients_unresolved or []):
        missing.append(MissingItem(item=str(unresolved), need=1.0, have=0.0, reason="unresolved_ingredient"))

    for ing in (recipe.ingredients or []):
        item = str(ing.get("item") or "").strip()
        if not item:
            continue
        amt_num = ing.get("amount_num", None)
        if amt_num is None:
            # fallback: try parse amount string
            try:
                amt_num = float(ing.get("amount"))
            except Exception:
                amt_num = None
        if amt_num is None:
            missing.append(MissingItem(item=item, need=1.0, have=float(inv.get(item, 0.0)), reason="unresolved_amount"))
            continue

        have = float(inv.get(item, 0.0))
        need = float(amt_num)
        if have + 1e-9 < need:
            missing.append(MissingItem(item=item, need=need, have=have, reason="insufficient"))

    return missing


def craftable_recipes(
    recipes: List[CraftRecipe],
    inventory: Dict[str, float],
    builder_tag: Optional[str] = None,
    strict: bool = False,
) -> Tuple[List[CraftRecipe], List[Dict[str, Any]]]:
    """Return craftable recipes + blocked details."""
    inv = inventory or {}
    ok: List[CraftRecipe] = []
    blocked: List[Dict[str, Any]] = []

    for r in recipes:
        allowed, reason = is_builder_allowed(r, builder_tag, strict=strict)
        if not allowed:
            blocked.append({"name": r.name, "reason": reason})
            continue

        miss = missing_for(r, inv)
        if miss:
            blocked.append({"name": r.name, "reason": "missing_material", "missing": [m.__dict__ for m in miss]})
            continue

        ok.append(r)

    return ok, blocked
```

### File: apps/webcraft/settings.py
- mode: full
- size_bytes: 919
- sha256_12: 57f5a4c92b56

```py
# -*- coding: utf-8 -*-
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional


@dataclass(frozen=True)
class WebCraftSettings:
    """Runtime settings for WebCraft server.

    Notes
    - catalog_path can point to wagstaff_catalog_v2.json or wagstaff_catalog_v2.sqlite
      (SQLite is preferred when both exist).
    - i18n requires wagstaff_i18n_v1.json (no runtime PO fallback).
    - root_path is for reverse-proxy mount (e.g. '/webcraft')
    """

    catalog_path: Path
    root_path: str = ""
    cors_allow_origins: Optional[List[str]] = None
    gzip_minimum_size: int = 800

    @staticmethod
    def normalize_root_path(root_path: str) -> str:
        rp = (root_path or "").strip()
        if not rp:
            return ""
        if not rp.startswith("/"):
            rp = "/" + rp
        rp = rp.rstrip("/")
        return rp
```

### File: apps/webcraft/tuning_trace.py
- mode: full
- size_bytes: 3858
- sha256_12: 0a796c1a7679

```py
# -*- coding: utf-8 -*-
from __future__ import annotations

import json
import sqlite3
import threading
from pathlib import Path
from typing import Any, Dict, List, Optional

import bisect


_SQLITE_SUFFIXES = (".sqlite", ".sqlite3", ".db")


def _is_sqlite_path(path: Path) -> bool:
    return path.suffix.lower() in _SQLITE_SUFFIXES


class TuningTraceStore:
    """Load + index tuning trace JSON for on-demand queries (thread-safe)."""

    def __init__(self, path: Path):
        self._path = Path(path)
        self._use_sqlite = _is_sqlite_path(self._path)
        self._lock = threading.RLock()
        self._mtime: float = -1.0
        self._doc: Dict[str, Any] = {}
        self._keys: List[str] = []
        self.load(force=True)

    @property
    def path(self) -> Path:
        return self._path

    def mtime(self) -> float:
        with self._lock:
            return float(self._mtime or 0)

    def load(self, force: bool = False) -> bool:
        """Load trace file if changed. Returns True if reload occurred."""
        with self._lock:
            if not self._path.exists():
                self._doc = {}
                self._keys = []
                self._mtime = -1.0
                return False
            try:
                mtime = self._path.stat().st_mtime
            except Exception:
                return False
            if (not force) and self._doc and self._mtime == mtime:
                return False
            if self._use_sqlite:
                doc = self._load_sqlite(self._path)
            else:
                try:
                    doc = json.loads(self._path.read_text(encoding="utf-8"))
                except Exception:
                    doc = {}
            self._doc = doc if isinstance(doc, dict) else {}
            self._keys = sorted(k for k in self._doc.keys() if isinstance(k, str))
            self._mtime = mtime
            return True

    @staticmethod
    def _load_sqlite(path: Path) -> Dict[str, Any]:
        try:
            conn = sqlite3.connect(str(path))
            conn.row_factory = sqlite3.Row
        except Exception:
            return {}
        try:
            cur = conn.cursor()
            tables = {row["name"] for row in cur.execute("SELECT name FROM sqlite_master WHERE type='table'")}
            if "tuning_trace" not in tables:
                return {}
            rows = cur.execute("SELECT trace_key, raw_json FROM tuning_trace").fetchall()
        except Exception:
            return {}
        finally:
            conn.close()

        out: Dict[str, Any] = {}
        for row in rows:
            key = str(row["trace_key"] or "").strip()
            if not key:
                continue
            raw = row["raw_json"]
            if raw is None:
                out[key] = None
                continue
            try:
                out[key] = json.loads(raw)
            except Exception:
                out[key] = raw
        return out

    def count(self) -> int:
        with self._lock:
            return len(self._doc)

    def get(self, key: str) -> Optional[Any]:
        if not key:
            return None
        with self._lock:
            v = self._doc.get(str(key))
            if isinstance(v, dict):
                return dict(v)
            return v

    def get_prefix(self, prefix: str, *, limit: int = 2000) -> Dict[str, Any]:
        p = str(prefix or "")
        if not p:
            return {}
        out: Dict[str, Any] = {}
        with self._lock:
            keys = self._keys
            start = bisect.bisect_left(keys, p)
            end = bisect.bisect_right(keys, p + "\uffff")
            for k in keys[start:end]:
                v = self._doc.get(k)
                out[k] = dict(v) if isinstance(v, dict) else v
                if len(out) >= limit:
                    break
        return out
```

### File: apps/webcraft/ui.py
- mode: full
- size_bytes: 1260
- sha256_12: 8082523ecd0a

```py
# -*- coding: utf-8 -*-
"""WebCraft UI template renderer."""

from __future__ import annotations

from functools import lru_cache
from html import escape
from pathlib import Path


_TEMPLATE_DIR = Path(__file__).resolve().parent / "templates"


@lru_cache(maxsize=None)
def _load_template(name: str) -> str:
    path = _TEMPLATE_DIR / name
    return path.read_text(encoding="utf-8")


def _render_template(name: str, app_root: str) -> str:
    root = str(app_root or "").rstrip("/")
    return _load_template(name).replace("__WAGSTAFF_APP_ROOT__", escape(root))


def render_catalog_html(app_root: str = "") -> str:
    """Render the Catalog UI page."""
    return _render_template("catalog.html", app_root)


def render_index_html(app_root: str = "") -> str:
    """Render the Craft UI page.

    app_root:
      - ""       normal direct serving
      - "/xxx"   reverse proxy mount path
    """
    return _render_template("index.html", app_root)


def render_cooking_html(app_root: str = "") -> str:
    """Render the Cooking UI page."""
    return _render_template("cooking.html", app_root)


def render_cooking_tools_html(app_root: str = "") -> str:
    """Render the Cooking tools UI page."""
    return _render_template("cooking_tools.html", app_root)
```

### File: core/__init__.py
- mode: full
- size_bytes: 61
- sha256_12: ac551446262f

```py
# -*- coding: utf-8 -*-
"""Core package for Wagstaff-Lab."""
```

### File: core/assets/__init__.py
- mode: full
- size_bytes: 61
- sha256_12: 7ddf1961ae47

```py
# -*- coding: utf-8 -*-
"""Asset helpers (atlas/tex/png)."""
```

### File: core/assets/klei_atlas_tex.py
- mode: full
- size_bytes: 17536
- sha256_12: 14b3a7105299

```py
# -*- coding: utf-8 -*-
"""klei_atlas_tex.py

A small, self-contained parser for:
- Klei atlas XML files (e.g. images/inventoryimages.xml)
- Klei TEX (KTEX) texture files (e.g. images/inventoryimages.tex)

Primary goal
- Extract a named atlas element (e.g. "armor_wood.tex") into a normal PNG.

Important correctness notes
- Klei atlas v coordinates count *up from the bottom* (v=0 is bottom).
  This is a common gotcha in DST modding discussions.
- TEX files are commonly stored with premultiplied alpha. For inventory/craft
  icons, unpremultiplying the cropped icon usually produces the expected
  appearance.

This module intentionally has no Wagstaff-specific imports.
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import struct
import xml.etree.ElementTree as ET

from PIL import Image


# -----------------------------
# Atlas XML
# -----------------------------


@dataclass(frozen=True)
class AtlasElement:
    name: str
    u1: float
    u2: float
    v1: float
    v2: float


@dataclass(frozen=True)
class Atlas:
    """Parsed atlas XML."""

    texture_filename: str
    elements: Dict[str, AtlasElement]

    def get(self, name: str) -> Optional[AtlasElement]:
        if not name:
            return None
        return self.elements.get(name)


def _xml_find_first(root: ET.Element, tag_local_name: str) -> Optional[ET.Element]:
    # Namespace-tolerant lookup
    return root.find(f".//{{*}}{tag_local_name}")


def _xml_find_all(root: ET.Element, tag_local_name: str) -> List[ET.Element]:
    return list(root.findall(f".//{{*}}{tag_local_name}"))


def parse_atlas_xml(xml_text: str) -> Atlas:
    """Parse a Klei atlas XML into an Atlas object."""

    root = ET.fromstring(xml_text)

    tex_node = _xml_find_first(root, "Texture")
    tex_filename = (tex_node.get("filename") if tex_node is not None else None) or ""

    elements: Dict[str, AtlasElement] = {}
    for el in _xml_find_all(root, "Element"):
        name = (el.get("name") or "").strip()
        if not name:
            continue
        try:
            u1 = float(el.get("u1") or "0")
            u2 = float(el.get("u2") or "0")
            v1 = float(el.get("v1") or "0")
            v2 = float(el.get("v2") or "0")
        except Exception:
            continue

        elements[name] = AtlasElement(name=name, u1=u1, u2=u2, v1=v1, v2=v2)

    return Atlas(texture_filename=tex_filename, elements=elements)


def atlas_uv_to_box(
    elem: AtlasElement,
    tex_w: int,
    tex_h: int,
    *,
    invert_v: bool = True,
) -> Tuple[int, int, int, int]:
    """Convert atlas UVs into a PIL crop box (left, top, right, bottom).

    Atlas coordinates:
      - u: 0..1 left->right
      - v: 0..1 top->bottom (for DST inventory atlases); some atlases use bottom-origin.

    PIL image coordinates:
      - (0,0) is top-left
    """

    x1 = int(round(elem.u1 * tex_w))
    x2 = int(round(elem.u2 * tex_w))

    if invert_v:
        y1_from_bottom = int(round(elem.v1 * tex_h))
        y2_from_bottom = int(round(elem.v2 * tex_h))
        top = tex_h - y2_from_bottom
        bottom = tex_h - y1_from_bottom
    else:
        y1 = int(round(elem.v1 * tex_h))
        y2 = int(round(elem.v2 * tex_h))
        top = min(y1, y2)
        bottom = max(y1, y2)

    # Normalize / clamp
    left = max(0, min(tex_w, min(x1, x2)))
    right = max(0, min(tex_w, max(x1, x2)))
    top2 = max(0, min(tex_h, min(top, bottom)))
    bottom2 = max(0, min(tex_h, max(top, bottom)))

    return left, top2, right, bottom2


# -----------------------------
# KTEX (Klei TEX)
# -----------------------------


class KTexError(RuntimeError):
    pass


@dataclass(frozen=True)
class KTexMipmap:
    width: int
    height: int
    pitch: int
    data_size: int
    data_offset: int


def _parse_ktex_variant(data: bytes, *, variant: str) -> Optional[Tuple[List[KTexMipmap], int]]:
    """Try parsing KTEX mipmap table.

    Returns:
      (mipmaps, end_offset) or None
    """

    if len(data) < 8:
        return None

    if data[:4] != b"KTEX":
        return None

    specs = struct.unpack_from("<I", data, 4)[0]

    if variant == "pre":
        mipmap_count = (specs >> 9) & 0xF
    elif variant == "post":
        mipmap_count = (specs >> 13) & 0x1F
    else:
        raise ValueError(f"unknown variant: {variant}")

    if mipmap_count <= 0:
        return None

    off = 8
    mips_meta: List[Tuple[int, int, int, int]] = []
    for _ in range(mipmap_count):
        if off + 10 > len(data):
            return None
        w, h, pitch, size = struct.unpack_from("<HHHI", data, off)
        off += 10
        if w <= 0 or h <= 0 or size <= 0:
            return None
        mips_meta.append((int(w), int(h), int(pitch), int(size)))

    data_off = off
    mips: List[KTexMipmap] = []
    for (w, h, pitch, size) in mips_meta:
        if data_off + size > len(data):
            return None
        mips.append(KTexMipmap(width=w, height=h, pitch=pitch, data_size=size, data_offset=data_off))
        data_off += size

    return mips, data_off


def parse_ktex(data: bytes) -> List[KTexMipmap]:
    """Parse a KTEX file and return mipmap descriptors.

    The KTEX header has two known variants (pre-caves-update and post-caves-update)
    which differ only in how the bitfield encodes mipmap_count.

    We attempt both and pick the one that yields the most plausible layout.

    Source for header bitfield layouts:
      - Stexatlaser project README (KTEX format section)
    """

    if len(data) < 8 or data[:4] != b"KTEX":
        raise KTexError("Not a KTEX file")

    candidates: List[Tuple[int, str, List[KTexMipmap], int]] = []
    for variant in ("post", "pre"):
        res = _parse_ktex_variant(data, variant=variant)
        if not res:
            continue
        mips, end_off = res
        # score: prefer layouts that consume most bytes (small remainder)
        remainder = abs(len(data) - end_off)
        candidates.append((remainder, variant, mips, end_off))

    if not candidates:
        raise KTexError("Failed to parse KTEX mipmap table")

    candidates.sort(key=lambda x: (x[0], 0 if x[1] == "post" else 1))
    _, _, mips, _ = candidates[0]
    return mips


def _infer_tex_payload_format(width: int, height: int, data_size: int, pitch: int) -> str:
    """Infer payload format from (w,h,data_size,pitch).

    Returns one of: "RGBA", "RGB", "DXT1", "DXT5".

    Many DST textures are DXT5.
    """

    w = int(width)
    h = int(height)
    if w <= 0 or h <= 0 or data_size <= 0:
        return ""

    rgba_size = w * h * 4
    rgb_size = w * h * 3

    blocks_w = (w + 3) // 4
    blocks_h = (h + 3) // 4
    dxt1_size = blocks_w * blocks_h * 8
    dxt5_size = blocks_w * blocks_h * 16

    # Exact matches first
    if data_size == rgba_size:
        return "RGBA"
    if data_size == rgb_size:
        return "RGB"
    if data_size == dxt1_size:
        return "DXT1"
    if data_size == dxt5_size:
        return "DXT5"

    # Heuristic via pitch
    if pitch in (w * 4, (w * 4) + 2):
        return "RGBA"
    if pitch in (w * 3, (w * 3) + 2):
        return "RGB"
    if pitch == blocks_w * 8 and data_size <= dxt1_size:
        return "DXT1"
    if pitch == blocks_w * 16 and data_size <= dxt5_size:
        return "DXT5"

    return ""


def _rgb565_to_rgb888(c: int) -> Tuple[int, int, int]:
    r = ((c >> 11) & 0x1F) * 255 // 31
    g = ((c >> 5) & 0x3F) * 255 // 63
    b = (c & 0x1F) * 255 // 31
    return int(r), int(g), int(b)


def _decompress_dxt1(payload: bytes, width: int, height: int) -> bytes:
    """Decompress DXT1 blocks into RGBA8888 bytes."""

    w = int(width)
    h = int(height)
    blocks_w = (w + 3) // 4
    blocks_h = (h + 3) // 4

    out = bytearray(w * h * 4)
    off = 0

    for by in range(blocks_h):
        for bx in range(blocks_w):
            if off + 8 > len(payload):
                raise KTexError("DXT1 payload truncated")

            c0, c1 = struct.unpack_from("<HH", payload, off)
            bits = struct.unpack_from("<I", payload, off + 4)[0]
            off += 8

            r0, g0, b0 = _rgb565_to_rgb888(c0)
            r1, g1, b1 = _rgb565_to_rgb888(c1)

            colors = [
                (r0, g0, b0, 255),
                (r1, g1, b1, 255),
            ]

            if c0 > c1:
                colors.append(((2 * r0 + r1) // 3, (2 * g0 + g1) // 3, (2 * b0 + b1) // 3, 255))
                colors.append(((r0 + 2 * r1) // 3, (g0 + 2 * g1) // 3, (b0 + 2 * b1) // 3, 255))
            else:
                colors.append(((r0 + r1) // 2, (g0 + g1) // 2, (b0 + b1) // 2, 255))
                colors.append((0, 0, 0, 0))

            # Indices: 2 bits each, little-endian, row-major
            idx_bits = bits
            for py in range(4):
                for px in range(4):
                    idx = idx_bits & 0x3
                    idx_bits >>= 2

                    x = bx * 4 + px
                    y = by * 4 + py
                    if x >= w or y >= h:
                        continue

                    o = (y * w + x) * 4
                    r, g, b, a = colors[idx]
                    out[o + 0] = r
                    out[o + 1] = g
                    out[o + 2] = b
                    out[o + 3] = a

    return bytes(out)


def _decompress_dxt5(payload: bytes, width: int, height: int) -> bytes:
    """Decompress DXT5 blocks into RGBA8888 bytes."""

    w = int(width)
    h = int(height)
    blocks_w = (w + 3) // 4
    blocks_h = (h + 3) // 4

    out = bytearray(w * h * 4)
    off = 0

    for by in range(blocks_h):
        for bx in range(blocks_w):
            if off + 16 > len(payload):
                raise KTexError("DXT5 payload truncated")

            a0 = payload[off]
            a1 = payload[off + 1]
            alpha_bits = int.from_bytes(payload[off + 2 : off + 8], "little")

            # Alpha palette
            alphas = [0] * 8
            alphas[0] = a0
            alphas[1] = a1
            if a0 > a1:
                # 6 interpolated values
                alphas[2] = (6 * a0 + 1 * a1) // 7
                alphas[3] = (5 * a0 + 2 * a1) // 7
                alphas[4] = (4 * a0 + 3 * a1) // 7
                alphas[5] = (3 * a0 + 4 * a1) // 7
                alphas[6] = (2 * a0 + 5 * a1) // 7
                alphas[7] = (1 * a0 + 6 * a1) // 7
            else:
                # 4 interpolated, then 0 and 255
                alphas[2] = (4 * a0 + 1 * a1) // 5
                alphas[3] = (3 * a0 + 2 * a1) // 5
                alphas[4] = (2 * a0 + 3 * a1) // 5
                alphas[5] = (1 * a0 + 4 * a1) // 5
                alphas[6] = 0
                alphas[7] = 255

            # Color block (DXT1-style)
            c0, c1 = struct.unpack_from("<HH", payload, off + 8)
            color_bits = struct.unpack_from("<I", payload, off + 12)[0]

            r0, g0, b0 = _rgb565_to_rgb888(c0)
            r1, g1, b1 = _rgb565_to_rgb888(c1)

            # In DXT5, colors are always 4-color interpolations (alpha is separate)
            colors = [
                (r0, g0, b0),
                (r1, g1, b1),
                ((2 * r0 + r1) // 3, (2 * g0 + g1) // 3, (2 * b0 + b1) // 3),
                ((r0 + 2 * r1) // 3, (g0 + 2 * g1) // 3, (b0 + 2 * b1) // 3),
            ]

            off += 16

            a_bits = alpha_bits
            c_bits = color_bits
            for py in range(4):
                for px in range(4):
                    a_idx = a_bits & 0x7
                    a_bits >>= 3

                    c_idx = c_bits & 0x3
                    c_bits >>= 2

                    x = bx * 4 + px
                    y = by * 4 + py
                    if x >= w or y >= h:
                        continue

                    r, g, b = colors[c_idx]
                    a = alphas[a_idx]

                    o = (y * w + x) * 4
                    out[o + 0] = r
                    out[o + 1] = g
                    out[o + 2] = b
                    out[o + 3] = a

    return bytes(out)


def decode_ktex_to_image(tex_bytes: bytes) -> Image.Image:
    """Decode KTEX to a PIL RGBA image using mipmap level 0."""

    mips = parse_ktex(tex_bytes)
    mip0 = mips[0]

    payload = tex_bytes[mip0.data_offset : mip0.data_offset + mip0.data_size]
    fmt = _infer_tex_payload_format(mip0.width, mip0.height, mip0.data_size, mip0.pitch)
    if not fmt:
        raise KTexError(
            f"Unsupported/unknown TEX payload format (w={mip0.width}, h={mip0.height}, size={mip0.data_size}, pitch={mip0.pitch})"
        )

    if fmt == "RGBA":
        rgba = payload
        if len(rgba) < mip0.width * mip0.height * 4:
            raise KTexError("RGBA payload truncated")
        rgba = rgba[: mip0.width * mip0.height * 4]
    elif fmt == "RGB":
        if len(payload) < mip0.width * mip0.height * 3:
            raise KTexError("RGB payload truncated")
        rgb = payload[: mip0.width * mip0.height * 3]
        # expand to RGBA
        out = bytearray(mip0.width * mip0.height * 4)
        j = 0
        for i in range(0, len(rgb), 3):
            out[j + 0] = rgb[i + 0]
            out[j + 1] = rgb[i + 1]
            out[j + 2] = rgb[i + 2]
            out[j + 3] = 255
            j += 4
        rgba = bytes(out)
    elif fmt == "DXT1":
        rgba = _decompress_dxt1(payload, mip0.width, mip0.height)
    else:
        rgba = _decompress_dxt5(payload, mip0.width, mip0.height)

    return Image.frombytes("RGBA", (mip0.width, mip0.height), rgba)


# -----------------------------
# Extraction helpers
# -----------------------------


def unpremultiply_alpha_rgba(img: Image.Image) -> Image.Image:
    """Return a new image with straight (unpremultiplied) alpha.

    This is applied *after* cropping for performance (icons are small).
    """

    if img.mode != "RGBA":
        img = img.convert("RGBA")

    raw = bytearray(img.tobytes())
    for i in range(0, len(raw), 4):
        a = raw[i + 3]
        if a == 0 or a == 255:
            continue
        # Avoid rounding to >255
        raw[i + 0] = min(255, (raw[i + 0] * 255) // a)
        raw[i + 1] = min(255, (raw[i + 1] * 255) // a)
        raw[i + 2] = min(255, (raw[i + 2] * 255) // a)

    return Image.frombytes("RGBA", img.size, bytes(raw))


def fix_ktex_orientation(img: Image.Image) -> Image.Image:
    """Fix KTEX orientation for UI output."""

    try:
        return img.transpose(Image.FLIP_TOP_BOTTOM)
    except Exception:
        return img


def extract_atlas_element(
    atlas: Atlas,
    tex_image: Image.Image,
    element_name: str,
    *,
    unpremultiply: bool = True,
    invert_v: bool = True,
) -> Optional[Image.Image]:
    """Crop a named element from an atlas texture image."""

    el = atlas.get(element_name)
    if not el:
        return None

    w, h = tex_image.size
    left, top, right, bottom = atlas_uv_to_box(el, w, h, invert_v=invert_v)
    if right <= left or bottom <= top:
        return None

    cropped = tex_image.crop((left, top, right, bottom))
    cropped = fix_ktex_orientation(cropped)
    if unpremultiply:
        try:
            cropped = unpremultiply_alpha_rgba(cropped)
        except Exception:
            # If anything goes wrong, still return the cropped image.
            pass
    return cropped


def resolve_tex_path_from_atlas(xml_path: Path, atlas: Atlas) -> Optional[Path]:
    """Resolve the atlas <Texture filename="..."> into a filesystem path."""

    fn = (atlas.texture_filename or "").strip()
    if not fn:
        # Common fallback: same basename as xml
        return xml_path.with_suffix(".tex")

    # If XML stores full-ish path, try as-is relative to xml parent.
    p = Path(fn)
    if p.is_absolute():
        return p

    return (xml_path.parent / p).resolve()


def write_element_png(
    *,
    atlas_xml_path: Path,
    tex_path: Path,
    element_name: str,
    out_png_path: Path,
    unpremultiply: bool = True,
    overwrite: bool = False,
) -> bool:
    """Extract one element and write a PNG.

    Returns True on success.
    """

    if out_png_path.exists() and not overwrite:
        return True

    xml_text = atlas_xml_path.read_text(encoding="utf-8", errors="ignore")
    atlas = parse_atlas_xml(xml_text)

    tex_bytes = tex_path.read_bytes()
    tex_image = decode_ktex_to_image(tex_bytes)

    cropped = extract_atlas_element(atlas, tex_image, element_name, unpremultiply=unpremultiply)
    if cropped is None:
        return False

    out_png_path.parent.mkdir(parents=True, exist_ok=True)
    cropped.save(out_png_path, format="PNG")
    return True


def pick_first_existing(names: Iterable[str], available: Dict[str, AtlasElement]) -> Optional[str]:
    for n in names:
        if not n:
            continue
        if n in available:
            return n
    return None


# =========================================================
# PNG writer helper (added by hotfix)
# =========================================================

def write_png(img, out_path):
    '''Write a PIL Image to PNG, ensuring parent directories exist.

    This is intentionally tiny and dependency-light (relies on Pillow already
    used by the atlas/tex pipeline).
    '''
    from pathlib import Path

    p = Path(out_path)
    p.parent.mkdir(parents=True, exist_ok=True)

    # Ensure RGBA to avoid mode issues on some Pillow builds
    try:
        img.save(str(p), format="PNG")
    except Exception:
        img.convert("RGBA").save(str(p), format="PNG")
```

### File: core/config/__init__.py
- mode: full
- size_bytes: 155
- sha256_12: f66f248d1bf3

```py
# -*- coding: utf-8 -*-
"""Config helpers."""

from core.config.loader import ConfigLoader, wagstaff_config

__all__ = ["ConfigLoader", "wagstaff_config"]
```

### File: core/config/loader.py
- mode: full
- size_bytes: 1022
- sha256_12: b40df7bcb20c

```py
import configparser
import os
from pathlib import Path

class ConfigLoader:
    def __init__(self):
        # 自动定位项目根目录 (core/config/*)
        self.project_root = Path(__file__).resolve().parents[2]
        self.config_path = self.project_root / "conf" / "settings.ini"
        
        self.config = configparser.ConfigParser()
        if not self.config_path.exists():
            raise FileNotFoundError(f"❌ 配置文件丢失: {self.config_path}")
        
        self.config.read(self.config_path)

    def get(self, section, key):
        """获取配置值并自动展开用户路径 (~)"""
        val = self.config.get(section, key, fallback=None)
        if val and "~" in val:
            return os.path.expanduser(val)
        return val

# 单例模式：直接导出的实例
wagstaff_config = ConfigLoader()

# === 测试代码 ===
if __name__ == "__main__":
    print(f"Project Root: {wagstaff_config.project_root}")
    print(f"DST Path: {wagstaff_config.get('PATHS', 'DST_ROOT')}")
```

### File: core/craft_recipes.py
- mode: full
- size_bytes: 28217
- sha256_12: 651e3208046b

```py
# -*- coding: utf-8 -*-
"""core/craft_recipes.py

Crafting recipes (Recipe/Recipe2/AddRecipe2) + recipe filter organization.

Why this module exists
- `scripts/recipes*.lua` and `scripts/recipes_filter.lua` are data sources.
- UI layers (wiki/GUI/web) need a stable, query-friendly Python representation.

Design goals (M0)
- Reuse `core/lua` parsing primitives (single source of truth).
- Keep outputs JSON-serializable for catalog/index (M2).

Public API (expected by existing CLI)
- CraftRecipeDB.get(name)
- CraftRecipeDB.list_by_tab(tab)
- CraftRecipeDB.list_by_filter(filter)
- CraftRecipeDB.list_by_builder_tag(tag)
- CraftRecipeDB.list_by_tech(tech)

New (M2)
- CraftRecipeDB.list_by_ingredient(item)
- CraftRecipeDB.craftable(inventory)
- CraftRecipeDB.missing_for(recipe, inventory)
- CraftRecipeDB.to_dict() / CraftRecipeDB.from_dict()
"""

from __future__ import annotations

from collections import defaultdict
from dataclasses import dataclass
import json
import re
from typing import Any, Dict, Iterable, List, Mapping, Optional, Sequence, Set, Tuple

from core.lua import (
    LuaCallExtractor,
    LuaRaw,
    LuaTableValue,
    find_matching,
    lua_to_python,
    parse_lua_expr,
    parse_lua_string,
    strip_lua_comments,
)

__all__ = [
    "CraftRecipeDB",
    "parse_filter_defs",
]


# =========================================================
# Regex helpers
# =========================================================

_TECH_TOKEN_RE = re.compile(r"\bTECH\.[A-Z0-9_]+\b")
_TAB_TOKEN_RE = re.compile(r"\bRECIPETABS\.[A-Z0-9_]+\b")
_FILTER_TOKEN_RE = re.compile(r"\bCRAFTING_FILTERS\.([A-Z0-9_]+)\b")
_FILTER_ASSIGN_RE = re.compile(r"\bCRAFTING_FILTERS\.([A-Z0-9_]+)\.recipes\s*=\s*\{", re.MULTILINE)

# string literals inside a table list; recipes_filter.lua uses plain quoted names
_QUOTED_STR_RE = re.compile(r"([\"'])([^\"']+)\1")

# If a "{ ... }" argument contains '=', it is most likely a config table not a filter list.
# This heuristic is intentionally conservative.


# =========================================================
# Data model helpers
# =========================================================

def _dedup_preserve(seq: Iterable[str]) -> List[str]:
    out: List[str] = []
    seen: Set[str] = set()
    for x in seq:
        if not x:
            continue
        if x in seen:
            continue
        seen.add(x)
        out.append(x)
    return out


def _looks_like_kv_table(expr_text: str) -> bool:
    s = (expr_text or "").strip()
    return s.startswith("{") and ("=" in s)


def _extract_first_token(rx: re.Pattern[str], text: str) -> Optional[str]:
    m = rx.search(text or "")
    return m.group(0) if m else None


_NUM_RE = re.compile(r"^[+-]?(?:\d+\.?\d*|\d*\.?\d+)(?:[eE][+-]?\d+)?$")


def _to_amount(expr_text: str) -> Tuple[Optional[float], str]:
    """Return (amount_num, amount_expr)."""
    raw = (expr_text or "").strip() or "1"

    # Fast path numeric
    if _NUM_RE.match(raw):
        try:
            return float(raw), raw
        except Exception:
            return None, raw

    v = parse_lua_expr(raw)
    if isinstance(v, (int, float)):
        return float(v), raw

    # LuaRaw with a numeric string
    if isinstance(v, LuaRaw) and _NUM_RE.match(v.text.strip()):
        try:
            return float(v.text.strip()), raw
        except Exception:
            pass

    return None, raw


def _parse_string_list(v: Any) -> List[str]:
    """Try to turn a Lua table value into a list[str]."""
    if isinstance(v, LuaTableValue):
        out: List[str] = []
        for x in v.array:
            if isinstance(x, str):
                out.append(x)
            elif isinstance(x, LuaRaw):
                # keep raw symbol if it looks like a string constant
                out.append(x.text)
            else:
                py = lua_to_python(x)
                if isinstance(py, str):
                    out.append(py)
        return _dedup_preserve(out)

    py = lua_to_python(v)
    if isinstance(py, list):
        return _dedup_preserve([str(x) for x in py if x is not None])

    return []


def _parse_config_fields(tbl: LuaTableValue) -> Dict[str, Any]:
    """Extract commonly useful fields from a recipe config table."""
    out: Dict[str, Any] = {}

    for k, v in (tbl.map or {}).items():
        if not isinstance(k, str):
            continue

        if k in {
            "builder_tag",
            "builder_skill",
            "station_tag",
            "product",
            "placer",
            "image",
            "atlas",
            "nounlock",
            "numtogive",
            "min_spacing",
            "testfn",  # sometimes exists
        }:
            out[k] = lua_to_python(v)
        elif k in {"builder_tags"}:
            out[k] = _parse_string_list(v)
        # Keep unknown keys out for now (index bloat).

    # normalize: builder_tags may be an object/dict if table is keyed; accept only list
    if "builder_tags" in out and not isinstance(out["builder_tags"], list):
        out.pop("builder_tags", None)

    return out


def _parse_filters_from_text(expr_text: str) -> List[str]:
    if not expr_text:
        return []

    filters: List[str] = []

    # CRAFTING_FILTERS.NAME tokens
    filters += [m.group(1) for m in _FILTER_TOKEN_RE.finditer(expr_text)]

    # quoted uppercase strings (rare but exists in some mods)
    for q in re.findall(r"[\"']([A-Z0-9_]+)[\"']", expr_text):
        if q and q.upper() == q:
            filters.append(q)

    return _dedup_preserve([f.upper() for f in filters if f])


# =========================================================
# Parsing: recipes.lua / recipes2.lua
# =========================================================


def _parse_ingredients_from_expr(expr_text: str) -> Tuple[List[Dict[str, Any]], List[str]]:
    """Extract Ingredient("item", amount) calls.

    Returns (ingredients, unresolved_items).

    Ingredient amount is stored as:
    - amount: original expression string
    - amount_num: parsed float if possible (else None)
    """
    if not expr_text:
        return [], []

    out: List[Dict[str, Any]] = []
    unresolved: List[str] = []

    ex = LuaCallExtractor(expr_text)
    for call in ex.iter_calls(["Ingredient"], include_member_calls=False):
        if not call.arg_list:
            continue

        item_expr = call.arg_list[0].strip()
        item = parse_lua_string(item_expr) or item_expr

        if len(call.arg_list) >= 2:
            amount_expr = call.arg_list[1]
        else:
            amount_expr = "1"

        amount_num, amount_raw = _to_amount(amount_expr)

        rec = {
            "item": item,
            "amount": amount_raw,
            "amount_num": amount_num,
        }
        out.append(rec)

        if amount_num is None:
            unresolved.append(str(item))

    return out, _dedup_preserve(unresolved)


def _init_recipe_record(name: str) -> Dict[str, Any]:
    return {
        "name": name,
        "product": name,  # default: in most DST recipes, name == prefab product
        "ingredients": [],
        "ingredients_unresolved": [],
        "tech": "UNKNOWN",
        "tab": "UNKNOWN",  # assigned later from filter membership
        "filters": [],
        "builder_tag": None,
        "builder_tags": [],
        "builder_skill": None,
        "station_tag": None,
        "sources": [],
        # optional metadata (for GUI)
        "image": None,
        "atlas": None,
        "placer": None,
        "nounlock": None,
        "numtogive": None,
    }


def _parse_recipe_call(call_name: str, args: Sequence[str]) -> Optional[Dict[str, Any]]:
    """Parse a Recipe/Recipe2/AddRecipe2 call into a recipe record."""
    if not args:
        return None

    name = parse_lua_string(args[0])
    if not name:
        return None

    rec = _init_recipe_record(name)
    rec["sources"].append(call_name)

    # ingredients: usually arg[1]
    if len(args) >= 2:
        ing, unresolved = _parse_ingredients_from_expr(args[1])
        if ing:
            rec["ingredients"] = ing
        if unresolved:
            rec["ingredients_unresolved"] = unresolved

    # tech: usually arg[2], but also scan whole call for TECH.X
    tech = None
    if len(args) >= 3:
        tech = _extract_first_token(_TECH_TOKEN_RE, args[2])
    if tech is None:
        tech = _extract_first_token(_TECH_TOKEN_RE, " ".join(args))
    if tech:
        rec["tech"] = tech

    # explicit RECIPETABS (legacy) if present
    tab = _extract_first_token(_TAB_TOKEN_RE, " ".join(args))
    if tab:
        rec["tab"] = tab

    # parse remaining args: config tables and/or explicit filter lists
    for a in args[2:]:
        a = (a or "").strip()
        if not a.startswith("{"):
            # Sometimes filters are passed as plain symbols; token-scan anyway.
            fs = _parse_filters_from_text(a)
            if fs:
                rec["filters"] = _dedup_preserve(list(rec.get("filters", [])) + fs)
            continue

        v = parse_lua_expr(a)
        if isinstance(v, LuaTableValue) and _looks_like_kv_table(a):
            fields = _parse_config_fields(v)
            for k, val in fields.items():
                if k == "builder_tags":
                    rec["builder_tags"] = _dedup_preserve(list(rec.get("builder_tags") or []) + list(val or []))
                    continue
                if val is None:
                    continue
                if rec.get(k) in (None, "UNKNOWN", [], {}):
                    rec[k] = val
                else:
                    # avoid overwriting unless empty
                    pass

            # product override
            if fields.get("product"):
                rec["product"] = fields.get("product")

        else:
            fs = _parse_filters_from_text(a)
            if fs:
                rec["filters"] = _dedup_preserve(list(rec.get("filters", [])) + fs)

    # Normalize builder_tags
    if rec.get("builder_tag"):
        rec["builder_tags"] = _dedup_preserve(list(rec.get("builder_tags") or []) + [str(rec["builder_tag"])])

    return rec


def parse_craft_recipes(recipes_lua: str, recipes2_lua: str) -> Dict[str, Dict[str, Any]]:
    """Parse craft recipes from recipes.lua + recipes2.lua.

    - recipes.lua: Recipe/Recipe2
    - recipes2.lua: AddRecipe2

    Returns dict: name -> recipe_record
    """
    out: Dict[str, Dict[str, Any]] = {}

    for src, fnames in ((recipes_lua or "", ["Recipe", "Recipe2"]), (recipes2_lua or "", ["AddRecipe2"])):
        if not src:
            continue
        ex = LuaCallExtractor(src)
        for call in ex.iter_calls(fnames, include_member_calls=False):
            rec = _parse_recipe_call(call.name, call.arg_list)
            if not rec:
                continue
            name = rec["name"]
            if name not in out:
                out[name] = rec
            else:
                # Merge: later source wins for missing fields; union for lists.
                cur = out[name]
                cur["sources"] = _dedup_preserve(list(cur.get("sources", [])) + list(rec.get("sources", [])))

                # ingredients: keep first non-empty
                if (not cur.get("ingredients")) and rec.get("ingredients"):
                    cur["ingredients"] = rec["ingredients"]

                # union unresolved
                cur["ingredients_unresolved"] = _dedup_preserve(
                    list(cur.get("ingredients_unresolved") or []) + list(rec.get("ingredients_unresolved") or [])
                )

                # prefer concrete tech
                if cur.get("tech") in (None, "", "UNKNOWN") and rec.get("tech") not in (None, "", "UNKNOWN"):
                    cur["tech"] = rec["tech"]

                # union filters
                cur["filters"] = _dedup_preserve(list(cur.get("filters") or []) + list(rec.get("filters") or []))

                # fill other scalar fields if empty
                for k in (
                    "builder_tag",
                    "builder_skill",
                    "station_tag",
                    "product",
                    "tab",
                    "image",
                    "atlas",
                    "placer",
                    "nounlock",
                    "numtogive",
                ):
                    if cur.get(k) in (None, "", "UNKNOWN") and rec.get(k) not in (None, "", "UNKNOWN"):
                        cur[k] = rec[k]

                # builder_tags union
                cur["builder_tags"] = _dedup_preserve(list(cur.get("builder_tags") or []) + list(rec.get("builder_tags") or []))

    return out


# =========================================================
# Parsing: recipes_filter.lua
# =========================================================


def parse_filter_defs(src: str) -> List[Dict[str, Any]]:
    """Parse the CRAFTING_FILTER_DEFS table as an ordered list of dicts."""
    if not src:
        return []

    clean = strip_lua_comments(src)
    idx = clean.find("CRAFTING_FILTER_DEFS")
    if idx == -1:
        return []

    brace = clean.find("{", idx)
    if brace == -1:
        return []

    end = find_matching(clean, brace, "{", "}")
    if end is None:
        return []

    block = clean[brace : end + 1]
    tbl = parse_lua_expr(block)
    if not isinstance(tbl, LuaTableValue):
        return []

    defs: List[Dict[str, Any]] = []
    for entry in tbl.array:
        if not isinstance(entry, LuaTableValue):
            continue
        d: Dict[str, Any] = {}
        for k, v in entry.map.items():
            if isinstance(k, str):
                d[k] = lua_to_python(v)
        if d:
            defs.append(d)

    return defs


def _parse_filter_order(src: str) -> List[str]:
    defs = parse_filter_defs(src)
    order: List[str] = []
    for d in defs:
        nm = d.get("name")
        if isinstance(nm, str) and nm:
            order.append(nm.upper())
    return order


def _parse_filter_recipe_lists(src: str) -> Dict[str, List[str]]:
    """Parse `CRAFTING_FILTERS.X.recipes = { ... }` lists."""
    out: Dict[str, List[str]] = {}
    if not src:
        return out

    clean = strip_lua_comments(src)

    for m in _FILTER_ASSIGN_RE.finditer(clean):
        flt = m.group(1).upper()
        brace = m.end() - 1
        end = find_matching(clean, brace, "{", "}")
        if end is None:
            continue
        inner = clean[brace + 1 : end]
        names = [sm.group(2) for sm in _QUOTED_STR_RE.finditer(inner)]
        # recipes list is typically plain strings (prefab names)
        out[flt] = _dedup_preserve(names)

    return out


def _parse_filter_bindings_by_calls(src: str) -> Dict[str, List[str]]:
    """Parse AddRecipeToFilter(s) calls.

    Returns: recipe_name -> [filters...]
    """
    out: Dict[str, List[str]] = defaultdict(list)
    if not src:
        return out

    ex = LuaCallExtractor(src)
    for call in ex.iter_calls(["AddRecipeToFilter", "AddRecipeToFilters"], include_member_calls=False):
        args = call.arg_list
        if not args:
            continue

        # recipe name is usually the first non-UPPERCASE string literal
        recipe_name: Optional[str] = None
        for a in args:
            s = parse_lua_string(a)
            if s and not s.isupper():
                recipe_name = s
                break
        if recipe_name is None:
            for a in args:
                s = parse_lua_string(a)
                if s:
                    recipe_name = s
                    break
        if not recipe_name:
            continue

        filters: List[str] = []
        for a in args:
            filters += _parse_filters_from_text(a)

        if filters:
            out[recipe_name] = _dedup_preserve(list(out.get(recipe_name, [])) + filters)

    return out


# =========================================================
# CraftRecipeDB (public)
# =========================================================


class CraftRecipeDB:
    """Queryable craft recipe database.

    Build sources:
    - recipes.lua / recipes2.lua (Recipe/Recipe2/AddRecipe2)
    - recipes_filter.lua (filters order + filter membership)

    Notes
    - `tab` is derived from filter order: first non-special filter in membership.
    - `filters` holds all known filter memberships.
    """

    _SPECIAL_FILTERS = {"FAVORITES", "CRAFTING_STATION", "SPECIAL_EVENT", "MODS", "CHARACTER", "EVERYTHING"}

    def __init__(self, *args, **kwargs):
        """Supported initializers:

        - CraftRecipeDB(recipes_lua=..., recipes2_lua=..., recipes_filter_lua=...)
        - CraftRecipeDB(recipes_lua, recipes2_lua, recipes_filter_lua)
        - CraftRecipeDB(recipes_lua, recipes_filter_lua)  # legacy fallback
        """
        recipes_lua = kwargs.get("recipes_lua", "")
        recipes2_lua = kwargs.get("recipes2_lua", "")
        recipes_filter_lua = kwargs.get("recipes_filter_lua", "")

        if args:
            if len(args) == 1:
                recipes_lua = args[0]
            elif len(args) == 2:
                recipes_lua = args[0]
                recipes_filter_lua = args[1]
            else:
                recipes_lua = args[0]
                recipes2_lua = args[1]
                recipes_filter_lua = args[2]

        self.recipes: Dict[str, Dict[str, Any]] = {}
        self.aliases: Dict[str, str] = {}

        self.filter_defs: List[Dict[str, Any]] = []
        self.filter_order: List[str] = []
        self.tab_order: List[str] = []

        self.by_tab: Dict[str, List[str]] = defaultdict(list)
        self.by_filter: Dict[str, List[str]] = defaultdict(list)
        self.by_tech: Dict[str, List[str]] = defaultdict(list)
        self.by_builder_tag: Dict[str, List[str]] = defaultdict(list)
        self.by_ingredient: Dict[str, List[str]] = defaultdict(list)

        self._build(recipes_lua or "", recipes2_lua or "", recipes_filter_lua or "")

    # -----------------
    # build
    # -----------------

    def _build(self, recipes_lua: str, recipes2_lua: str, recipes_filter_lua: str) -> None:
        # 1) base recipes
        self.recipes = parse_craft_recipes(recipes_lua, recipes2_lua)

        # 2) aliases
        for name in self.recipes.keys():
            self.aliases[name.lower()] = name
            self.aliases[name.replace("_", "").lower()] = name

        # 3) filter defs + membership
        self.filter_defs = parse_filter_defs(recipes_filter_lua)
        self.filter_order = _parse_filter_order(recipes_filter_lua)
        filter_lists = _parse_filter_recipe_lists(recipes_filter_lua)
        call_bindings = _parse_filter_bindings_by_calls(recipes_filter_lua)

        # membership: recipe -> filters
        membership: Dict[str, Set[str]] = {name: set(map(str.upper, (self.recipes[name].get("filters") or []))) for name in self.recipes.keys()}

        # from `CRAFTING_FILTERS.X.recipes = {...}`
        for flt, rlist in filter_lists.items():
            for r in rlist:
                if r in membership:
                    membership[r].add(flt)

        # from AddRecipeToFilter(s)
        for r, fs in call_bindings.items():
            if r in membership:
                for f in fs:
                    membership[r].add(f.upper())

        # 4) finalize per recipe: filters, tab
        for name, rec in self.recipes.items():
            fset = membership.get(name, set())
            # include whatever was already present
            rec["filters"] = sorted(_dedup_preserve([f.upper() for f in fset if f]))

            # choose tab: first non-special in defs order
            chosen: Optional[str] = None
            for f in self.filter_order:
                if f in fset and f not in self._SPECIAL_FILTERS:
                    chosen = f
                    break
            if chosen is None:
                # fallback: any in order
                for f in self.filter_order:
                    if f in fset:
                        chosen = f
                        break
            rec["tab"] = chosen or rec.get("tab") or "UNKNOWN"

            # normalize builder_tags again (after merges)
            if rec.get("builder_tag"):
                rec["builder_tags"] = _dedup_preserve(list(rec.get("builder_tags") or []) + [str(rec["builder_tag"])])
            else:
                rec["builder_tags"] = _dedup_preserve(list(rec.get("builder_tags") or []))

        # 5) build indices
        # by_filter / by_tab
        for name, rec in self.recipes.items():
            for f in rec.get("filters", []) or []:
                self.by_filter[str(f).upper()].append(name)

            t = str(rec.get("tab") or "UNKNOWN").upper()
            self.by_tab[t].append(name)

            # by_tech
            tech = str(rec.get("tech") or "UNKNOWN")
            if tech.upper().startswith("TECH."):
                tech = tech.split(".", 1)[1]
            self.by_tech[tech.upper()].append(name)

            # by_builder_tag(s)
            for bt in rec.get("builder_tags") or []:
                if bt:
                    self.by_builder_tag[str(bt).lower()].append(name)

            # by_ingredient
            for ing in rec.get("ingredients") or []:
                it = ing.get("item")
                if it:
                    self.by_ingredient[str(it).lower()].append(name)

        # sort & unique
        for mp in (self.by_filter, self.by_tab, self.by_tech, self.by_builder_tag, self.by_ingredient):
            for k in list(mp.keys()):
                mp[k] = sorted(set(mp[k]))

        # tab order: filter_order excluding specials
        self.tab_order = [f for f in self.filter_order if f and f not in self._SPECIAL_FILTERS]

    # -----------------
    # Public query API
    # -----------------

    def __len__(self) -> int:
        return len(self.recipes)

    def get(self, query_name: str) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:
        if not query_name:
            return None, None
        q = query_name.strip().lower()
        canonical = self.aliases.get(q) or self.aliases.get(q.replace("_", ""))
        if not canonical:
            return None, None
        return canonical, self.recipes.get(canonical)

    def list_tabs(self) -> List[str]:
        return list(self.tab_order)

    def list_filters(self) -> List[str]:
        return list(self.filter_order)

    def list_by_tab(self, tab: str) -> List[str]:
        key = (tab or "").strip().upper()
        # allow TECH/RECIPETABS raw
        if key.startswith("RECIPETABS."):
            key = key.split(".", 1)[1]
        return list(self.by_tab.get(key, []))

    def list_by_filter(self, flt: str) -> List[str]:
        key = (flt or "").strip().upper()
        if key.startswith("CRAFTING_FILTERS."):
            key = key.split(".", 1)[1]
        return list(self.by_filter.get(key, []))

    def list_by_builder_tag(self, tag: str) -> List[str]:
        key = (tag or "").strip().lower()
        return list(self.by_builder_tag.get(key, []))

    def list_by_tech(self, tech: str) -> List[str]:
        t = (tech or "").strip()
        if t.upper().startswith("TECH."):
            t = t.split(".", 1)[1]
        return list(self.by_tech.get(t.upper(), []))

    def list_by_ingredient(self, item: str) -> List[str]:
        key = (item or "").strip().lower()
        return list(self.by_ingredient.get(key, []))

    # -----------------
    # Craft planner (M2)
    # -----------------

    def missing_for(self, recipe_name: str, inventory: Mapping[str, float]) -> List[Dict[str, Any]]:
        """Return missing ingredients for a recipe under a given inventory.

        Inventory is a mapping item->count (int/float).

        If an ingredient has non-numeric amount (amount_num is None), it is returned
        as missing with reason="unresolved_amount".
        """
        _, rec = self.get(recipe_name)
        if not rec:
            return []

        inv = {str(k).lower(): float(v) for k, v in (inventory or {}).items()}

        missing: List[Dict[str, Any]] = []
        for ing in rec.get("ingredients") or []:
            item = str(ing.get("item") or "").lower()
            if not item:
                continue
            need_num = ing.get("amount_num")
            need_expr = ing.get("amount")
            have = inv.get(item, 0.0)

            if need_num is None:
                missing.append({"item": item, "need": need_expr, "have": have, "reason": "unresolved_amount"})
                continue

            if have + 1e-9 < float(need_num):
                missing.append({"item": item, "need": need_num, "have": have, "reason": "insufficient"})

        return missing

    def craftable(
        self,
        inventory: Mapping[str, float],
        *,
        builder_tag: Optional[str] = None,
        strict: bool = True,
    ) -> List[str]:
        """List craftable recipes.

        - builder_tag: if set, only recipes that are not character-locked or match builder_tag.
        - strict: if True, recipes with unresolved ingredient amounts are excluded.
        """
        inv = {str(k).lower(): float(v) for k, v in (inventory or {}).items()}
        bt = builder_tag.strip().lower() if builder_tag else None

        out: List[str] = []
        for name, rec in self.recipes.items():
            # builder constraints
            if bt:
                tags = [str(x).lower() for x in (rec.get("builder_tags") or [])]
                if tags and bt not in tags:
                    continue

            miss = self.missing_for(name, inv)
            if not miss:
                out.append(name)
            else:
                if strict:
                    continue
                # if not strict, allow unresolved-only
                if all(m.get("reason") == "unresolved_amount" for m in miss):
                    out.append(name)

        return sorted(out)

    # -----------------
    # Serialization (M2)
    # -----------------

    def to_dict(self) -> Dict[str, Any]:
        """JSON-serializable snapshot (do not include derived indices)."""
        return {
            "schema": 1,
            "recipes": self.recipes,
            "aliases": self.aliases,
            "filter_defs": self.filter_defs,
            "filter_order": self.filter_order,
        }

    @classmethod
    def from_dict(cls, data: Mapping[str, Any]) -> "CraftRecipeDB":
        """Load from `to_dict()` output."""
        obj = cls(recipes_lua="", recipes2_lua="", recipes_filter_lua="")

        obj.recipes = {str(k): v for k, v in (data.get("recipes") or {}).items()}
        obj.aliases = {str(k): str(v) for k, v in (data.get("aliases") or {}).items()}
        obj.filter_defs = list(data.get("filter_defs") or [])
        obj.filter_order = [str(x).upper() for x in (data.get("filter_order") or [])]
        obj.tab_order = [f for f in obj.filter_order if f and f not in obj._SPECIAL_FILTERS]

        # rebuild indices
        obj.by_tab = defaultdict(list)
        obj.by_filter = defaultdict(list)
        obj.by_tech = defaultdict(list)
        obj.by_builder_tag = defaultdict(list)
        obj.by_ingredient = defaultdict(list)

        for name, rec in obj.recipes.items():
            for f in rec.get("filters", []) or []:
                obj.by_filter[str(f).upper()].append(name)
            t = str(rec.get("tab") or "UNKNOWN").upper()
            obj.by_tab[t].append(name)

            tech = str(rec.get("tech") or "UNKNOWN")
            if tech.upper().startswith("TECH."):
                tech = tech.split(".", 1)[1]
            obj.by_tech[tech.upper()].append(name)

            for bt in rec.get("builder_tags") or []:
                if bt:
                    obj.by_builder_tag[str(bt).lower()].append(name)

            for ing in rec.get("ingredients") or []:
                it = ing.get("item")
                if it:
                    obj.by_ingredient[str(it).lower()].append(name)

        for mp in (obj.by_filter, obj.by_tab, obj.by_tech, obj.by_builder_tag, obj.by_ingredient):
            for k in list(mp.keys()):
                mp[k] = sorted(set(mp[k]))

        return obj

    def dumps(self, *, indent: int = 2, ensure_ascii: bool = False) -> str:
        return json.dumps(self.to_dict(), ensure_ascii=ensure_ascii, indent=indent)
```

### File: core/engine.py
- mode: full
- size_bytes: 16480
- sha256_12: fec51fe8b91f

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""WagstaffEngine (core)

This module is intentionally UI-agnostic.

Responsibilities
- Mount DST scripts source (zip or folder) with a consistent "scripts/..." namespace.
- Provide fast `read_file()` + `find_file()` primitives.
- Load small, stable databases:
  - TuningResolver (scripts/tuning.lua)
  - CraftRecipeDB (scripts/recipes.lua + scripts/recipes2.lua + scripts/recipes_filter.lua)
  - CookingRecipeAnalyzer (scripts/preparedfoods.lua + scripts/prefabs/preparedfoods.lua)
  - CookingIngredientAnalyzer (scripts/ingredients.lua + scripts/cooking.lua)

Design notes
- Engine must be usable by CLI, GUI, and Web layers.
- Avoid hard dependency on Rich (it is optional). Use `silent=True` to suppress logs.
"""

from __future__ import annotations

import logging
import os
import zipfile
from functools import lru_cache
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# Optional project config (exists in repo under core/config/loader.py)
try:
    from core.config import wagstaff_config  # type: ignore
except Exception:  # pragma: no cover
    wagstaff_config = None  # type: ignore

from core.parsers import (
    CookingIngredientAnalyzer,
    CookingRecipeAnalyzer,
    LuaAnalyzer,
    TuningResolver,
    parse_oceanfish_ingredients,
)
from core.craft_recipes import CraftRecipeDB

logger = logging.getLogger(__name__)


def _expanduser(p: Optional[str]) -> Optional[str]:
    return os.path.expanduser(p) if p else None


def _merge_cooking_ingredients(base: Dict[str, Dict], extra: Dict[str, Dict]) -> Dict[str, Dict]:
    out: Dict[str, Dict] = {}
    for iid, row in (base or {}).items():
        if not isinstance(row, dict):
            continue
        merged = dict(row)
        merged.setdefault("id", iid)
        out[str(iid)] = merged

    for iid, row in (extra or {}).items():
        if not isinstance(row, dict):
            continue
        key = str(iid)
        if key not in out:
            merged = dict(row)
            merged.setdefault("id", key)
            out[key] = merged
            continue

        cur = dict(out[key])
        cur.setdefault("id", key)

        sources: List[str] = []
        for src in cur.get("sources") or []:
            if src not in sources:
                sources.append(src)
        for src in row.get("sources") or []:
            if src not in sources:
                sources.append(src)
        if sources:
            cur["sources"] = sources

        tags = dict(cur.get("tags") or {})
        for tag, val in (row.get("tags") or {}).items():
            if tag not in tags or tags.get(tag) in (None, 0, 0.0):
                tags[tag] = val
        if tags:
            cur["tags"] = tags

        tags_expr = dict(cur.get("tags_expr") or {})
        for tag, val in (row.get("tags_expr") or {}).items():
            if tag not in tags_expr:
                tags_expr[tag] = val
        if tags_expr:
            cur["tags_expr"] = tags_expr

        for field in ("name", "atlas", "image", "prefab", "foodtype"):
            if field not in cur and field in row:
                cur[field] = row[field]

        out[key] = cur

    return out


class WagstaffEngine:
    """Main entry used by CLI / devtools / GUI / Web.

    Parameters
    - load_db: load tuning + recipe DBs (and cooking recipes).
    - silent: suppress all logs.
    - dst_root: optional DST root path (overrides config).
    - scripts_zip: optional scripts zip path (highest priority).
    - scripts_dir: optional scripts folder path (highest priority for folder mode).
    - prefer_local_bundles: search project-root bundle drops first.
    """

    def __init__(
        self,
        load_db: bool = True,
        silent: bool = False,
        *,
        dst_root: Optional[str] = None,
        scripts_zip: Optional[str] = None,
        scripts_dir: Optional[str] = None,
        prefer_local_bundles: bool = True,
        encoding: str = "utf-8",
    ):
        self.encoding = encoding
        self.silent = bool(silent)

        self.mode: str = ""  # 'zip' | 'folder'
        self.source: object = None  # ZipFile or folder path (str)
        self.file_list: List[str] = []

        # basename index for fast fuzzy find
        self._basename_index: Dict[str, List[str]] = {}

        self.tuning: Optional[TuningResolver] = None
        self.recipes: Optional[CraftRecipeDB] = None
        self.cooking_recipes: Dict[str, Dict] = {}
        self.cooking_ingredients: Dict[str, Dict] = {}

        self._init_source(
            dst_root=dst_root,
            scripts_zip=scripts_zip,
            scripts_dir=scripts_dir,
            prefer_local_bundles=prefer_local_bundles,
        )
        self._build_basename_index()

        if load_db:
            self._init_databases()

    # --------------------------------------------------------
    # Context manager
    # --------------------------------------------------------

    def __enter__(self) -> "WagstaffEngine":
        return self

    def __exit__(self, exc_type, exc, tb) -> None:
        self.close()

    # --------------------------------------------------------
    # Source mounting
    # --------------------------------------------------------

    def _project_root(self) -> Path:
        """Best-effort repo root.

        - Prefer wagstaff_config.project_root when available.
        - Fallback to core/.. (engine.py is expected under core/).
        """
        if wagstaff_config is not None and hasattr(wagstaff_config, "project_root"):
            try:
                return Path(str(wagstaff_config.project_root)).resolve()
            except Exception:
                pass
        # engine.py is usually core/engine.py
        return Path(__file__).resolve().parent.parent

    def _detect_candidates(self, dst_root: Optional[str], prefer_local_bundles: bool) -> Tuple[List[str], List[str]]:
        """Return (zip_candidates, dir_candidates)."""
        pr = self._project_root()

        dst_root = _expanduser(dst_root)
        if dst_root is None and wagstaff_config is not None:
            try:
                dst_root = _expanduser(wagstaff_config.get("PATHS", "DST_ROOT"))
            except Exception:
                dst_root = None

        zip_candidates: List[str] = []
        dir_candidates: List[str] = []

        # Prefer local bundle drops for faster iteration
        if prefer_local_bundles:
            zip_candidates += [
                str(pr / "scripts-no-language-pac.zip"),
                str(pr / "scripts_no_language.zip"),
                str(pr / "scripts.zip"),
                str(pr / "data" / "databundles" / "scripts.zip"),
            ]
            dir_candidates += [
                str(pr / "scripts"),
            ]

        if dst_root:
            zip_candidates += [
                os.path.join(dst_root, "data", "databundles", "scripts.zip"),
                os.path.join(dst_root, "data", "databundles", "scripts_no_language.zip"),
            ]
            dir_candidates += [
                os.path.join(dst_root, "data", "scripts"),
            ]

        return zip_candidates, dir_candidates

    def _log(self, msg: str) -> None:
        if not self.silent:
            logger.info(msg)

    def _init_source(
        self,
        *,
        dst_root: Optional[str],
        scripts_zip: Optional[str],
        scripts_dir: Optional[str],
        prefer_local_bundles: bool,
    ) -> None:
        # explicit overrides
        if scripts_zip:
            zp = _expanduser(scripts_zip)
            if zp and os.path.exists(zp):
                self.mode = "zip"
                self.source = zipfile.ZipFile(zp, "r")
                self.file_list = list(getattr(self.source, "namelist")())
                self._log(f"Mounted scripts zip: {zp}")
                return

        if scripts_dir:
            dp = _expanduser(scripts_dir)
            if dp and os.path.isdir(dp):
                self.mode = "folder"
                self.source = dp
                self.file_list = self._walk_folder(dp)
                self._log(f"Mounted scripts folder: {dp}")
                return

        zip_candidates, dir_candidates = self._detect_candidates(dst_root, prefer_local_bundles)

        for zp in zip_candidates:
            if zp and os.path.exists(zp):
                self.mode = "zip"
                self.source = zipfile.ZipFile(zp, "r")
                self.file_list = list(getattr(self.source, "namelist")())
                self._log(f"Mounted scripts zip: {zp}")
                return

        for dp in dir_candidates:
            if dp and os.path.isdir(dp):
                self.mode = "folder"
                self.source = dp
                self.file_list = self._walk_folder(dp)
                self._log(f"Mounted scripts folder: {dp}")
                return

        raise FileNotFoundError("Cannot find scripts source (zip or folder).")

    def _walk_folder(self, folder: str) -> List[str]:
        folder = os.path.abspath(folder)
        out: List[str] = []
        for root, _, files in os.walk(folder):
            for name in files:
                full = os.path.join(root, name)
                rel = os.path.relpath(full, folder).replace("\\", "/")
                out.append("scripts/" + rel)  # normalize namespace
        return out

    def _build_basename_index(self) -> None:
        mp: Dict[str, List[str]] = {}
        for p in self.file_list:
            if not p.endswith(".lua"):
                continue
            base = os.path.basename(p)
            key = base.replace(".lua", "").replace("_", "").lower()
            mp.setdefault(key, []).append(p)
        self._basename_index = mp

    # --------------------------------------------------------
    # IO
    # --------------------------------------------------------

    def _normalize_path_candidates(self, path: str) -> List[str]:
        p = (path or "").replace("\\", "/").lstrip("/")
        if not p:
            return []
        if p.startswith("scripts/"):
            return [p, p.replace("scripts/", "", 1)]
        return [p, "scripts/" + p]

    @lru_cache(maxsize=4096)
    def read_file(self, path: str) -> Optional[str]:
        """Read a UTF-8 text file from the mounted source.

        Accepts paths with or without the "scripts/" prefix.
        Returns None if not found.
        """
        candidates = self._normalize_path_candidates(path)
        if not candidates:
            return None

        try:
            if self.mode == "zip":
                zf: zipfile.ZipFile = self.source  # type: ignore[assignment]
                for p in candidates:
                    if p in self.file_list:
                        return zf.read(p).decode(self.encoding, errors="replace")
                return None

            # folder
            base: str = self.source  # type: ignore[assignment]
            for p in candidates:
                real = os.path.join(base, p.replace("scripts/", "", 1))
                if os.path.exists(real):
                    with open(real, "r", encoding=self.encoding, errors="replace") as f:
                        return f.read()
        except Exception:
            return None

        return None

    def find_file(self, name: str, fuzzy: bool = True) -> Optional[str]:
        """Find a file by short name.

        Examples
        - armorwood -> scripts/prefabs/armorwood.lua (or armor_wood.lua)
        - prefabs/armorwood.lua -> scripts/prefabs/armorwood.lua

        Returns a path in the normalized namespace (usually "scripts/...").
        """
        if not name:
            return None
        q = name.replace("\\", "/").strip()
        if not q:
            return None

        # direct hit if user passed a path
        for cand in self._normalize_path_candidates(q):
            if cand in self.file_list:
                return cand

        base = q.replace(".lua", "")
        candidates = [
            f"scripts/prefabs/{base}.lua",
            f"scripts/{base}.lua",
            f"scripts/{base}",
        ]
        for c in candidates:
            if c in self.file_list:
                return c

        if not fuzzy:
            return None

        key = os.path.basename(base).replace("_", "").lower()
        hits = self._basename_index.get(key)
        if hits:
            # prefer prefabs if ambiguous
            if len(hits) == 1:
                return hits[0]
            pref = [h for h in hits if h.startswith("scripts/prefabs/")]
            if len(pref) == 1:
                return pref[0]
            return hits[0]

        # final fallback: scan
        target = key
        for fname in self.file_list:
            if not fname.endswith(".lua"):
                continue
            b = os.path.basename(fname).replace(".lua", "").replace("_", "").lower()
            if b == target:
                return fname

        return None

    def close(self) -> None:
        if self.mode == "zip" and self.source is not None:
            try:
                self.source.close()  # type: ignore[attr-defined]
            except Exception:
                pass

    # --------------------------------------------------------
    # DB initialization
    # --------------------------------------------------------

    def _init_databases(self) -> None:
        self._log("Loading tuning / crafting / cooking databases...")

        t_content = self.read_file("scripts/tuning.lua") or self.read_file("tuning.lua") or ""
        self.tuning = TuningResolver(t_content)

        r1 = self.read_file("scripts/recipes.lua") or self.read_file("recipes.lua") or ""
        r2 = self.read_file("scripts/recipes2.lua") or self.read_file("recipes2.lua") or ""
        rf = self.read_file("scripts/recipes_filter.lua") or self.read_file("recipes_filter.lua") or ""
        self.recipes = CraftRecipeDB(recipes_lua=r1, recipes2_lua=r2, recipes_filter_lua=rf)

        # cooking recipes (optional)
        food_src = self.read_file("scripts/preparedfoods.lua") or ""
        if food_src:
            self.cooking_recipes.update(CookingRecipeAnalyzer(food_src).recipes)

        food_prefab_src = self.read_file("scripts/prefabs/preparedfoods.lua") or ""
        if food_prefab_src:
            # prefab file often contains the same table; merge (prefab wins)
            self.cooking_recipes.update(CookingRecipeAnalyzer(food_prefab_src).recipes)

        ing_path = "scripts/ingredients.lua"
        ing_src = self.read_file(ing_path)
        if not ing_src:
            ing_path = "ingredients.lua"
            ing_src = self.read_file(ing_path)
        if ing_src:
            self.cooking_ingredients = CookingIngredientAnalyzer(ing_src, source=ing_path).ingredients

        cook_path = "scripts/cooking.lua"
        cook_src = self.read_file(cook_path)
        if not cook_src:
            cook_path = "cooking.lua"
            cook_src = self.read_file(cook_path)
        if cook_src:
            extra = CookingIngredientAnalyzer(cook_src, source=cook_path).ingredients
            if extra:
                self.cooking_ingredients = _merge_cooking_ingredients(self.cooking_ingredients, extra)

        fish_path = "scripts/prefabs/oceanfishdef.lua"
        fish_src = self.read_file(fish_path)
        if not fish_src:
            fish_path = "prefabs/oceanfishdef.lua"
            fish_src = self.read_file(fish_path)
        if fish_src:
            fish_extra = parse_oceanfish_ingredients(fish_src, source=fish_path)
            if fish_extra:
                self.cooking_ingredients = _merge_cooking_ingredients(self.cooking_ingredients, fish_extra)

    # --------------------------------------------------------
    # High-level helpers
    # --------------------------------------------------------

    def analyze_prefab(self, item_name: str) -> Optional[Dict]:
        """High-level prefab analysis (LuaAnalyzer + tuning enrichment)."""
        path = self.find_file(item_name, fuzzy=True)
        if not path:
            return None

        content = self.read_file(path)
        if not content:
            return None

        data = LuaAnalyzer(content, path=path).get_report()

        if self.tuning:
            for comp in data.get("components", []) or []:
                comp["properties"] = [self.tuning.enrich(p) for p in comp.get("properties", [])]
                comp["methods"] = [self.tuning.enrich(m) for m in comp.get("methods", [])]

        return data
```

### File: core/indexers/__init__.py
- mode: full
- size_bytes: 48
- sha256_12: e7fb45ee44bb

```py
# -*- coding: utf-8 -*-
"""Indexers package."""
```

### File: core/indexers/catalog_index.py
- mode: full
- size_bytes: 7063
- sha256_12: ea5d556592d6

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Catalog index builder (core).

Build a compact, search-friendly index from wagstaff_catalog_v2.json.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

from core.schemas.meta import build_meta

def _dedup_preserve_order(items: Iterable[str]) -> List[str]:
    out: List[str] = []
    seen = set()
    for x in items:
        if not x:
            continue
        if x in seen:
            continue
        out.append(x)
        seen.add(x)
    return out


def load_icon_index(path: Optional[Path]) -> Dict[str, str]:
    if path is None:
        return {}
    p = Path(path)
    if not p.exists() or not p.is_file():
        return {}
    try:
        doc = json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return {}
    icons = doc.get("icons") if isinstance(doc, dict) else None
    if not isinstance(icons, dict):
        return {}
    out: Dict[str, str] = {}
    for k, v in icons.items():
        if not k or not isinstance(k, str):
            continue
        if isinstance(v, dict) and v.get("png"):
            out[k] = str(v.get("png"))
    return out


def _build_item_list(
    catalog: Dict[str, Any],
    *,
    icon_index: Optional[Dict[str, str]] = None,
) -> List[Dict[str, Any]]:
    items_obj = catalog.get("items") or {}
    assets_obj = catalog.get("assets") or {}

    if not isinstance(items_obj, dict):
        items_obj = {}
    if not isinstance(assets_obj, dict):
        assets_obj = {}

    icon_index = icon_index or {}

    ids: List[str] = []
    ids.extend([str(k) for k in items_obj.keys() if k])
    ids.extend([str(k) for k in assets_obj.keys() if k])
    ids.extend([str(k) for k in icon_index.keys() if k])
    ids = _dedup_preserve_order(ids)

    out: List[Dict[str, Any]] = []

    for iid in ids:
        if not iid:
            continue
        item = items_obj.get(iid) if isinstance(items_obj.get(iid), dict) else {}
        asset = assets_obj.get(iid) if isinstance(assets_obj.get(iid), dict) else {}
        asset = asset or (item.get("assets") if isinstance(item, dict) else {}) or {}
        name = asset.get("name") or item.get("name") or iid
        icon = asset.get("icon") or asset.get("image") or icon_index.get(iid)
        entry = {
            "id": iid,
            "name": name,
            "image": asset.get("image") or icon,
            "icon": icon,
            "has_icon": bool(icon),
            "icon_only": bool(iid not in items_obj),
            "kind": item.get("kind"),
            "categories": item.get("categories") or [],
            "behaviors": item.get("behaviors") or [],
            "sources": item.get("sources") or [],
            "tags": item.get("tags") or [],
            "components": item.get("components") or [],
            "slots": item.get("slots") or [],
        }
        out.append(entry)

    out.sort(key=lambda x: x.get("id") or "")
    return out


def _build_indexes(items: List[Dict[str, Any]]) -> Dict[str, Dict[str, List[str]]]:
    by_kind: Dict[str, List[str]] = {}
    by_category: Dict[str, List[str]] = {}
    by_behavior: Dict[str, List[str]] = {}
    by_source: Dict[str, List[str]] = {}
    by_component: Dict[str, List[str]] = {}
    by_tag: Dict[str, List[str]] = {}
    by_slot: Dict[str, List[str]] = {}

    def _as_list(val: Any) -> List[str]:
        if isinstance(val, str):
            return [val]
        if isinstance(val, (list, tuple, set)):
            return [str(x) for x in val if x]
        return []

    def _push(bucket: Dict[str, List[str]], key: Optional[str], iid: str) -> None:
        if not key:
            return
        bucket.setdefault(str(key), []).append(iid)

    for item in items:
        iid = str(item.get("id") or "").strip()
        if not iid:
            continue
        kind = item.get("kind")
        if kind:
            _push(by_kind, str(kind), iid)
        for cat in _as_list(item.get("categories")):
            _push(by_category, cat, iid)
        for beh in _as_list(item.get("behaviors")):
            _push(by_behavior, beh, iid)
        for src in _as_list(item.get("sources")):
            _push(by_source, src, iid)
        for comp in _as_list(item.get("components")):
            _push(by_component, comp, iid)
        for tag in _as_list(item.get("tags")):
            _push(by_tag, tag, iid)
        for slot in _as_list(item.get("slots")):
            _push(by_slot, slot, iid)

    for bucket in (by_kind, by_category, by_behavior, by_source, by_component, by_tag, by_slot):
        for k in list(bucket.keys()):
            bucket[k] = sorted(_dedup_preserve_order(bucket[k]))

    return {
        "by_kind": by_kind,
        "by_category": by_category,
        "by_behavior": by_behavior,
        "by_source": by_source,
        "by_component": by_component,
        "by_tag": by_tag,
        "by_slot": by_slot,
    }


def build_catalog_index(
    catalog: Dict[str, Any],
    *,
    icon_index: Optional[Dict[str, str]] = None,
) -> Dict[str, Any]:
    items = _build_item_list(catalog, icon_index=icon_index)
    indexes = _build_indexes(items)

    items_total = len(items)
    icon_only = len([i for i in items if i.get("icon_only")])
    icons_total = len([i for i in items if i.get("has_icon")])

    meta_src = catalog.get("meta") if isinstance(catalog, dict) else {}
    meta_src = meta_src if isinstance(meta_src, dict) else {}

    meta = build_meta(
        schema=1,
        tool="build_catalog_index",
        sources={
            "catalog": "wagstaff_catalog_v2.json",
            "scripts_zip": meta_src.get("scripts_zip"),
            "scripts_dir": meta_src.get("scripts_dir"),
        },
        extra={
            "catalog_schema": int(catalog.get("schema_version") or meta_src.get("schema") or 0),
            "scripts_sha256_12": meta_src.get("scripts_sha256_12"),
            "scripts_zip": meta_src.get("scripts_zip"),
            "scripts_dir": meta_src.get("scripts_dir"),
        },
    )

    return {
        "schema_version": 1,
        "meta": meta,
        "counts": {
            "items_total": items_total,
            "items_with_icon": icons_total,
            "icon_only": icon_only,
        },
        "items": items,
        "indexes": indexes,
    }


def render_index_summary(index_doc: Dict[str, Any]) -> str:
    meta = index_doc.get("meta") or {}
    counts = index_doc.get("counts") or {}
    lines = []
    lines.append("# Wagstaff Catalog Index Summary")
    lines.append("")
    lines.append("## Meta")
    lines.append("```yaml")
    lines.append(f"schema_version: {index_doc.get('schema_version')}")
    lines.append(f"catalog_schema: {meta.get('catalog_schema')}")
    lines.append(f"scripts_sha256_12: {meta.get('scripts_sha256_12')}")
    lines.append("```")
    lines.append("")
    lines.append("## Counts")
    lines.append("```yaml")
    for k, v in counts.items():
        lines.append(f"{k}: {v}")
    lines.append("```")
    return "\n".join(lines) + "\n"
```

### File: core/indexers/catalog_v2.py
- mode: full
- size_bytes: 25945
- sha256_12: e1330d4f874e

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Catalog v2 builder (core).

Generates an item-centric, taggable catalog from DST scripts and data.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple
import re

from core.lua import LuaCallExtractor, strip_lua_comments, _skip_string_or_long_string
from core.parsers import LootParser, PrefabParser
from core.indexers.shared import _sha256_12_file
from core.craft_recipes import CraftRecipeDB
from core.tagging import TagProfile, apply_overrides, infer_tags, load_tag_overrides
from core.schemas.catalog_v2 import WagstaffCatalogV2
from core.schemas.meta import build_meta


SCHEMA_VERSION = 2
_ID_RE = re.compile(r"^[a-z0-9_]+$")

TUNING_FIELDS = (
    "hunger",
    "health",
    "sanity",
    "perishtime",
    "cooktime",
    "temperature",
    "temperatureduration",
    "fuelvalue",
    "maxsize",
)

_STAT_METHODS = {
    "weapon": {
        "SetDamage": [("weapon_damage", 0)],
        "SetRange": [("weapon_range_min", 0), ("weapon_range_max", 1)],
        "SetAttackRange": [("weapon_range", 0)],
    },
    "combat": {
        "SetDefaultDamage": [("combat_damage", 0)],
        "SetAttackPeriod": [("attack_period", 0)],
        "SetRange": [("attack_range", 0), ("attack_range_max", 1)],
        "SetAreaDamage": [("area_damage", 0)],
    },
    "finiteuses": {
        "SetMaxUses": [("uses_max", 0)],
        "SetUses": [("uses", 0)],
    },
    "armor": {
        "InitCondition": [("armor_condition", 0), ("armor_absorption", 1)],
        "SetCondition": [("armor_condition", 0)],
        "SetAbsorption": [("armor_absorption", 0)],
    },
    "edible": {
        "SetHealth": [("edible_health", 0)],
        "SetHunger": [("edible_hunger", 0)],
        "SetSanity": [("edible_sanity", 0)],
    },
    "perishable": {
        "SetPerishTime": [("perish_time", 0)],
    },
    "fueled": {
        "SetFuelLevel": [("fuel_level", 0)],
        "InitializeFuelLevel": [("fuel_level", 0)],
        "SetMaxFuel": [("fuel_max", 0)],
    },
    "equippable": {
        "SetDapperness": [("dapperness", 0)],
        "SetEquipSlot": [("equip_slot", 0)],
        "SetWalkSpeedMult": [("equip_walk_speed_mult", 0)],
        "SetRunSpeedMult": [("equip_run_speed_mult", 0)],
        "SetRestrictedTag": [("equip_restricted_tag", 0)],
        "SetPreventUnequipping": [("equip_prevent_unequip", 0)],
        "SetEquipStack": [("equip_stack", 0)],
        "SetInsulated": [("equip_insulated", 0)],
        "SetEquippedMoisture": [("equip_moisture", 0)],
        "SetMaxEquippedMoisture": [("equip_moisture_max", 0)],
    },
    "insulator": {
        "SetInsulation": [("insulation", 0)],
        "SetWinterInsulation": [("insulation_winter", 0)],
        "SetSummerInsulation": [("insulation_summer", 0)],
    },
    "waterproofer": {
        "SetEffectiveness": [("waterproof", 0)],
    },
    "light": {
        "SetRadius": [("light_radius", 0)],
        "SetIntensity": [("light_intensity", 0)],
        "SetFalloff": [("light_falloff", 0)],
    },
    "stackable": {
        "SetMaxSize": [("stack_size", 0)],
    },
    "health": {
        "SetMaxHealth": [("health_max", 0)],
    },
    "sanity": {
        "SetMax": [("sanity_max", 0)],
        "SetRate": [("sanity_rate", 0)],
    },
    "sanityaura": {
        "SetAura": [("sanity_aura", 0)],
    },
    "hunger": {
        "SetMax": [("hunger_max", 0)],
        "SetRate": [("hunger_rate", 0)],
    },
    "locomotor": {
        "SetWalkSpeed": [("walk_speed", 0)],
        "SetRunSpeed": [("run_speed", 0)],
        "SetExternalSpeedMultiplier": [("speed_multiplier", 2)],
        "SetSpeedMultiplier": [("speed_multiplier", 0)],
    },
    "rechargeable": {
        "SetRechargeTime": [("recharge_time", 0)],
        "SetChargeTime": [("recharge_time", 0)],
        "SetMaxCharge": [("recharge_max", 0)],
        "SetPercent": [("recharge_percent", 0)],
        "SetCharge": [("recharge_charge", 0)],
    },
    "heater": {
        "SetHeat": [("heat", 0)],
        "SetRadius": [("heat_radius", 0)],
        "SetThermics": [("heater_exothermic", 0), ("heater_endothermic", 1)],
        "SetShouldFalloff": [("heat_falloff", 0)],
        "SetHeatRadiusCutoff": [("heat_radius_cutoff", 0)],
        "SetEquippedHeat": [("equipped_heat", 0)],
        "SetCarriedHeat": [("carried_heat", 0)],
        "SetCarriedHeatMultiplier": [("carried_heat_multiplier", 0)],
        "SetHeatRate": [("heat_rate", 0)],
    },
    "planardamage": {
        "SetBaseDamage": [("planar_damage_base", 0)],
        "SetBonusDamage": [("planar_damage_bonus", 0)],
        "SetDamage": [("planar_damage", 0)],
    },
    "planararmor": {
        "SetAbsorption": [("planar_absorption", 0)],
        "SetBaseAbsorption": [("planar_absorption_base", 0)],
    },
    "workable": {
        "SetWorkLeft": [("work_left", 0)],
    },
}

_STAT_PROPERTIES = {
    "weapon": {"damage": "weapon_damage"},
    "combat": {"defaultdamage": "combat_damage"},
    "finiteuses": {"maxuses": "uses_max", "uses": "uses"},
    "armor": {"absorption": "armor_absorption", "condition": "armor_condition"},
    "edible": {
        "healthvalue": "edible_health",
        "hungervalue": "edible_hunger",
        "sanityvalue": "edible_sanity",
    },
    "perishable": {"perishtime": "perish_time"},
    "fueled": {"maxfuel": "fuel_max"},
    "equippable": {
        "dapperness": "dapperness",
        "equipslot": "equip_slot",
        "walkspeedmult": "equip_walk_speed_mult",
        "runspeedmult": "equip_run_speed_mult",
        "restrictedtag": "equip_restricted_tag",
        "preventunequipping": "equip_prevent_unequip",
        "equipstack": "equip_stack",
        "insulated": "equip_insulated",
        "equippedmoisture": "equip_moisture",
        "maxequippedmoisture": "equip_moisture_max",
        "is_magic_dapperness": "equip_magic_dapperness",
    },
    "insulator": {"insulation": "insulation"},
    "waterproofer": {"effectiveness": "waterproof"},
    "light": {"radius": "light_radius", "intensity": "light_intensity", "falloff": "light_falloff"},
    "stackable": {"maxsize": "stack_size"},
    "health": {"maxhealth": "health_max"},
    "sanity": {"max": "sanity_max", "rate": "sanity_rate"},
    "sanityaura": {"aura": "sanity_aura"},
    "hunger": {"max": "hunger_max", "rate": "hunger_rate"},
    "locomotor": {"walkspeed": "walk_speed", "runspeed": "run_speed"},
    "rechargeable": {
        "recharge_time": "recharge_time",
        "chargetime": "recharge_time",
        "percent": "recharge_percent",
        "charge": "recharge_charge",
        "maxcharge": "recharge_max",
        "maxrecharge": "recharge_max",
        "total": "recharge_max",
        "current": "recharge_charge",
    },
    "heater": {
        "heat": "heat",
        "radius": "heat_radius",
        "equippedheat": "equipped_heat",
        "carriedheat": "carried_heat",
        "carriedheatfn": "carried_heat",
        "carriedheatmultiplier": "carried_heat_multiplier",
        "heatrate": "heat_rate",
        "radius_cutoff": "heat_radius_cutoff",
        "exothermic": "heater_exothermic",
        "endothermic": "heater_endothermic",
    },
    "planardamage": {"basedamage": "planar_damage_base", "bonusdamage": "planar_damage_bonus", "damage": "planar_damage"},
    "planararmor": {"absorption": "planar_absorption", "baseabsorption": "planar_absorption_base"},
    "workable": {"workleft": "work_left"},
}


def _clean_id(x: Any) -> Optional[str]:
    if not isinstance(x, str):
        return None
    s = x.strip().lower()
    if not s or not _ID_RE.match(s):
        return None
    return s


def _collect_craft_sets(craft: CraftRecipeDB) -> Dict[str, Set[str]]:
    recipe_ids: Set[str] = set()
    product_ids: Set[str] = set()
    ingredient_ids: Set[str] = set()

    for name, rec in (getattr(craft, "recipes", {}) or {}).items():
        nm = _clean_id(name)
        if nm:
            recipe_ids.add(nm)
        if not isinstance(rec, dict):
            continue
        prod = _clean_id(rec.get("product"))
        if prod:
            product_ids.add(prod)
        for ing in rec.get("ingredients", []) or []:
            item = _clean_id((ing or {}).get("item"))
            if item:
                ingredient_ids.add(item)

    return {
        "recipe_ids": recipe_ids,
        "product_ids": product_ids,
        "ingredient_ids": ingredient_ids,
    }


def _collect_cooking_sets(cooking: Dict[str, Any]) -> Dict[str, Set[str]]:
    recipe_ids: Set[str] = set()
    ingredient_ids: Set[str] = set()

    for name, rec in (cooking or {}).items():
        nm = _clean_id(name)
        if nm:
            recipe_ids.add(nm)
        if not isinstance(rec, dict):
            continue
        for row in (rec.get("card_ingredients") or []):
            if not isinstance(row, (list, tuple)) or not row:
                continue
            item = _clean_id(row[0])
            if item:
                ingredient_ids.add(item)

    return {
        "recipe_ids": recipe_ids,
        "ingredient_ids": ingredient_ids,
    }


def _scan_loot_items(engine: Any) -> Set[str]:
    items: Set[str] = set()
    patterns = ("SetSharedLootTable", "AddChanceLoot", "AddRandomLoot", "AddRandomLootTable")

    for path in getattr(engine, "file_list", []) or []:
        if not str(path).endswith(".lua"):
            continue
        p = str(path)
        if "loot" not in p and "prefabs" not in p:
            continue
        content = engine.read_file(p) or ""
        if not content:
            continue
        if not any(tok in content for tok in patterns):
            continue
        try:
            rep = LootParser(content, path=p).parse()
        except Exception:
            continue
        for entry in rep.get("entries") or []:
            item = _clean_id(entry.get("item"))
            if item:
                items.add(item)

    return items


def _select_asset(prefab_assets: List[Dict[str, Any]]) -> Dict[str, str]:
    atlas = None
    image = None
    for a in prefab_assets:
        t = str(a.get("type") or "").upper()
        p = str(a.get("path") or "")
        if not p:
            continue
        if t == "ATLAS" and atlas is None:
            atlas = p
        if t == "IMAGE" and image is None:
            image = p
    out: Dict[str, str] = {}
    if atlas:
        out["atlas"] = atlas
    if image:
        out["image"] = image
    return out


def _resolve_tuning_field(
    value: Any,
    *,
    tuning: Any,
    mode: str,
    trace_sink: Optional[Dict[str, Any]] = None,
    trace_key: Optional[str] = None,
) -> Any:
    if not tuning or not isinstance(value, str) or "TUNING." not in value:
        return value
    try:
        trace = tuning.trace_expr(value)
    except Exception:
        return value

    if trace_sink is not None and trace_key:
        trace_sink[trace_key] = trace

    if mode == "full":
        return {"value": trace.get("value"), "expr": trace.get("expr"), "trace": trace}

    # value_only
    return trace.get("value") if trace.get("value") is not None else value


def _parse_number(expr: str) -> Optional[float]:
    if not expr:
        return None
    s = str(expr).strip()
    if not s:
        return None
    try:
        if re.match(r"^[+-]?\d+(\.\d+)?$", s):
            val = float(s)
            return int(val) if val.is_integer() else val
    except Exception:
        return None
    return None


def _resolve_stat_expr(
    expr: str,
    *,
    tuning: Any,
    mode: str,
    trace_sink: Optional[Dict[str, Any]] = None,
    trace_key: Optional[str] = None,
) -> Dict[str, Any]:
    out: Dict[str, Any] = {"expr": expr}
    if not expr:
        return out

    if tuning and isinstance(expr, str) and "TUNING." in expr:
        try:
            trace = tuning.trace_expr(expr)
        except Exception:
            trace = {"expr": expr, "value": None, "expr_resolved": expr, "refs": {}}
        if trace_sink is not None and trace_key:
            trace_sink[trace_key] = trace
        out["value"] = trace.get("value")
        out["expr_resolved"] = trace.get("expr_resolved") or expr
        if mode == "full":
            out["trace"] = trace
        if trace_key:
            out["trace_key"] = trace_key
        return out

    expr_norm = str(expr).strip()
    if expr_norm in ("true", "false"):
        out["value"] = expr_norm == "true"
        out["expr_resolved"] = expr_norm
        return out

    num = _parse_number(expr)
    if num is not None:
        out["value"] = num
    out["expr_resolved"] = expr
    return out


def _score_stat_expr(expr: str) -> int:
    if not expr:
        return 0
    if "TUNING." in expr:
        return 3
    if str(expr).strip() in ("true", "false"):
        return 2
    if _parse_number(expr) is not None:
        return 2
    return 1


def _scan_assignment_expr(text: str, start: int) -> str:
    n = len(text)
    i = start
    depth = 0
    started = False
    while i < n:
        nxt = _skip_string_or_long_string(text, i)
        if nxt is not None:
            started = True
            i = nxt
            continue
        ch = text[i]
        if not started and ch.isspace():
            i += 1
            continue
        started = True
        if ch == "\n" and depth == 0:
            break
        if ch == ";" and depth == 0:
            break
        if ch in "([{":
            depth += 1
        elif ch in ")]}":
            depth = max(0, depth - 1)
        i += 1
    return text[start:i].strip().rstrip(",")


def _extract_component_aliases(clean: str) -> Dict[str, str]:
    aliases: Dict[str, str] = {}
    for m in re.finditer(
        r"\blocal\s+([A-Za-z0-9_]+)\s*=\s*(?:inst|self)[.:]AddComponent\(\s*['\"]([A-Za-z0-9_]+)['\"]",
        clean,
    ):
        aliases[m.group(1)] = m.group(2).lower()
    for m in re.finditer(
        r"\b([A-Za-z0-9_]+)\s*=\s*(?:inst|self)[.:]AddComponent\(\s*['\"]([A-Za-z0-9_]+)['\"]",
        clean,
    ):
        if m.group(1) not in aliases:
            aliases[m.group(1)] = m.group(2).lower()
    for m in re.finditer(r"\blocal\s+([A-Za-z0-9_]+)\s*=\s*(?:inst|self)\.components\.([A-Za-z0-9_]+)", clean):
        aliases[m.group(1)] = m.group(2).lower()
    for m in re.finditer(r"\b([A-Za-z0-9_]+)\s*=\s*(?:inst|self)\.components\.([A-Za-z0-9_]+)", clean):
        if m.group(1) not in aliases:
            aliases[m.group(1)] = m.group(2).lower()
    return aliases


def _extract_component_stat_exprs(content: str) -> Dict[str, str]:
    rep = PrefabParser(content).parse()
    comp_names = {
        str((comp or {}).get("name") or "").strip().lower()
        for comp in (rep.get("components") or [])
    }
    comp_names.discard("")

    clean = strip_lua_comments(content or "")
    aliases = _extract_component_aliases(clean)
    if not comp_names:
        comp_names = {m.group(1).lower() for m in re.finditer(r"\bcomponents\.([A-Za-z0-9_]+)\b", clean)}

    out: Dict[str, str] = {}
    scores: Dict[str, int] = {}

    method_names = {m for cmap in _STAT_METHODS.values() for m in cmap.keys()}
    extractor = LuaCallExtractor(content)
    for call in extractor.iter_calls(method_names, include_member_calls=True):
        cname = None
        m = re.search(r"\bcomponents\.([A-Za-z0-9_]+)\b", call.full_name)
        if m:
            cname = m.group(1).lower()
        else:
            root = re.split(r"[.:]", call.full_name, 1)[0]
            cname = aliases.get(root)
        if not cname:
            continue
        if comp_names and cname not in comp_names:
            continue
        mapping = _STAT_METHODS.get(cname, {}).get(call.name)
        if not mapping:
            continue
        for stat_key, idx in mapping:
            if idx >= len(call.arg_list):
                continue
            expr = (call.arg_list[idx] or "").strip()
            if not expr:
                continue
            score = _score_stat_expr(expr)
            if (stat_key not in out) or (score >= scores.get(stat_key, 0)):
                out[stat_key] = expr
                scores[stat_key] = score

    for cname in sorted(comp_names):
        prop_map = _STAT_PROPERTIES.get(cname, {})
        if not prop_map:
            continue

        prop_pat = re.compile(rf"\bcomponents\.{re.escape(cname)}\.([A-Za-z0-9_]+)\s*=")
        for m in prop_pat.finditer(clean):
            prop = m.group(1).strip().lower()
            stat_key = prop_map.get(prop)
            if not stat_key:
                continue
            expr = _scan_assignment_expr(clean, m.end())
            if not expr:
                continue
            score = _score_stat_expr(expr)
            if (stat_key not in out) or (score >= scores.get(stat_key, 0)):
                out[stat_key] = expr
                scores[stat_key] = score

        for alias, comp in aliases.items():
            if comp != cname:
                continue
            alias_pat = re.compile(rf"\b{re.escape(alias)}\.([A-Za-z0-9_]+)\s*=")
            for m in alias_pat.finditer(clean):
                prop = m.group(1).strip().lower()
                stat_key = prop_map.get(prop)
                if not stat_key:
                    continue
                expr = _scan_assignment_expr(clean, m.end())
                if not expr:
                    continue
                score = _score_stat_expr(expr)
                if (stat_key not in out) or (score >= scores.get(stat_key, 0)):
                    out[stat_key] = expr
                    scores[stat_key] = score

    if "heat_radius" not in out and "heat_radius_cutoff" in out:
        out["heat_radius"] = out["heat_radius_cutoff"]
        scores["heat_radius"] = scores.get("heat_radius_cutoff", 0)

    return out


def _infer_sources(
    *,
    item_id: str,
    craft_products: Set[str],
    cooking_recipes: Set[str],
    loot_items: Set[str],
    components: Set[str],
    tags: Set[str],
) -> Set[str]:
    sources: Set[str] = set()
    if item_id in craft_products:
        sources.add("craft")
    if item_id in cooking_recipes:
        sources.add("cook")
    if item_id in loot_items:
        sources.add("loot")
    if tags & {"event", "festival"}:
        sources.add("event")
    if tags & {"plant", "tree"} or "pickable" in components:
        sources.add("natural")
    if tags & {"character", "monster", "animal", "smallcreature", "largecreature", "epic"}:
        sources.add("spawn")
    return sources


def build_catalog_v2(
    *,
    engine: Any,
    resource_index: Dict[str, Any],
    tag_overrides_path: Optional[str] = None,
    tuning_mode: str = "value_only",
    include_tuning_trace: bool = False,
) -> Tuple[WagstaffCatalogV2, Optional[Dict[str, Any]]]:

    prefabs = resource_index.get("prefabs") or {}
    prefab_items = prefabs.get("items") or {}
    icon_ids = set(resource_index.get("assets", {}).get("inventory_icons") or [])

    craft_sets = _collect_craft_sets(engine.recipes)
    cooking_sets = _collect_cooking_sets(engine.cooking_recipes or {})
    loot_items = _scan_loot_items(engine)

    cooking_ingredients_src = getattr(engine, "cooking_ingredients", {}) or {}
    cooking_ingredient_ids = {_clean_id(k) for k in cooking_ingredients_src.keys() if _clean_id(k)}

    all_ids = (
        set(prefab_items.keys())
        | icon_ids
        | craft_sets["product_ids"]
        | craft_sets["recipe_ids"]
        | craft_sets["ingredient_ids"]
        | cooking_sets["recipe_ids"]
        | cooking_sets["ingredient_ids"]
        | cooking_ingredient_ids
    )
    all_ids = {i for i in all_ids if _ID_RE.match(i)}

    overrides = load_tag_overrides(tag_overrides_path)
    prefab_stats_cache: Dict[str, Dict[str, str]] = {}

    tuning_trace: Optional[Dict[str, Any]] = {} if include_tuning_trace else None
    tuning = getattr(engine, "tuning", None)

    items_out: Dict[str, Any] = {}
    assets_out: Dict[str, Any] = {}

    for iid in sorted(all_ids):
        pf = prefab_items.get(iid) or {}
        components = set(pf.get("components") or [])
        tags = set(pf.get("tags") or [])
        prefab_files = sorted({str(x) for x in (pf.get("files") or []) if x})
        prefab_assets = [dict(a) for a in (pf.get("assets") or []) if isinstance(a, dict)]
        brains = sorted({str(x) for x in (pf.get("brains") or []) if x})
        stategraphs = sorted({str(x) for x in (pf.get("stategraphs") or []) if x})
        helpers = sorted({str(x) for x in (pf.get("helpers") or []) if x})
        sources = _infer_sources(
            item_id=iid,
            craft_products=craft_sets["product_ids"],
            cooking_recipes=cooking_sets["recipe_ids"],
            loot_items=loot_items,
            components=components,
            tags=tags,
        )
        profile = infer_tags(components=components, tags=tags, sources=sources)
        profile = apply_overrides(iid, profile, overrides)

        assets = _select_asset(pf.get("assets") or [])
        if iid in icon_ids:
            assets["icon"] = f"static/icons/{iid}.png"

        stat_exprs: Dict[str, str] = {}
        stat_scores: Dict[str, int] = {}
        for pfile in prefab_files:
            if pfile not in prefab_stats_cache:
                content = engine.read_file(pfile) or ""
                prefab_stats_cache[pfile] = _extract_component_stat_exprs(content) if content else {}
            for sk, sv in prefab_stats_cache.get(pfile, {}).items():
                score = _score_stat_expr(sv)
                if (sk not in stat_exprs) or (score >= stat_scores.get(sk, 0)):
                    stat_exprs[sk] = sv
                    stat_scores[sk] = score

        stats_out: Dict[str, Any] = {}
        for stat_key, expr in stat_exprs.items():
            trace_key = f"item:{iid}:stat:{stat_key}" if include_tuning_trace else None
            entry = _resolve_stat_expr(
                expr,
                tuning=tuning,
                mode=tuning_mode,
                trace_sink=tuning_trace,
                trace_key=trace_key,
            )
            entry["key"] = stat_key
            stats_out[stat_key] = entry

        items_out[iid] = {
            "id": iid,
            "kind": profile.kind,
            "categories": sorted(profile.categories),
            "behaviors": sorted(profile.behaviors),
            "sources": sorted(profile.sources),
            "slots": sorted(profile.slots),
            "components": sorted(components),
            "tags": sorted(tags),
            "assets": assets or {},
            "prefab_files": prefab_files,
            "prefab_assets": prefab_assets,
            "brains": brains,
            "stategraphs": stategraphs,
            "helpers": helpers,
            "stats": stats_out,
        }
        if assets:
            assets_out[iid] = dict(assets)

    # craft (enrich ingredients)
    craft_doc = engine.recipes.to_dict() if engine.recipes else {}
    craft_recipes = craft_doc.get("recipes") or {}

    for name, rec in craft_recipes.items():
        if not isinstance(rec, dict):
            continue
        for ing in rec.get("ingredients", []) or []:
            if not isinstance(ing, dict):
                continue
            expr = ing.get("amount")
            if isinstance(expr, str) and "TUNING." in expr and tuning is not None:
                key = f"craft:{name}:ingredient:{ing.get('item')}"
                val = _resolve_tuning_field(expr, tuning=tuning, mode=tuning_mode, trace_sink=tuning_trace, trace_key=key)
                ing["amount_value"] = val if isinstance(val, (int, float)) else None
                if tuning_mode == "full" and isinstance(val, dict):
                    ing["amount_trace"] = val.get("trace")
            elif ing.get("amount_num") is not None:
                ing["amount_value"] = ing.get("amount_num")

    # cooking (enrich tuning fields)
    cooking_doc: Dict[str, Any] = {}
    for name, rec in (engine.cooking_recipes or {}).items():
        if not isinstance(rec, dict):
            continue
        out = dict(rec)
        for field in TUNING_FIELDS:
            if field in out:
                key = f"cooking:{name}:{field}"
                out[field] = _resolve_tuning_field(
                    out[field], tuning=tuning, mode=tuning_mode, trace_sink=tuning_trace, trace_key=key
                )
        cooking_doc[name] = out

    cooking_ingredients_doc: Dict[str, Any] = {}
    for iid, raw in (cooking_ingredients_src or {}).items():
        if not isinstance(raw, dict):
            continue
        out = dict(raw)
        out.setdefault("id", str(iid))
        cooking_ingredients_doc[str(iid)] = out

    scripts_zip = getattr(getattr(engine, "source", None), "filename", None)
    scripts_sha = _sha256_12_file(Path(scripts_zip)) if scripts_zip else None

    scripts_dir = getattr(engine, "source", None) if getattr(engine, "mode", "") == "folder" else None
    sources = {
        "resource_index": "wagstaff_resource_index_v1.json",
        "scripts_zip": scripts_zip,
        "scripts_dir": scripts_dir,
    }
    meta = build_meta(
        schema=SCHEMA_VERSION,
        tool="build_catalog_v2",
        sources=sources,
        extra={
            "tuning_mode": tuning_mode,
            "scripts_sha256_12": scripts_sha,
            "scripts_zip": scripts_zip,
            "scripts_dir": scripts_dir,
        },
    )

    stats = {
        "items_total": len(items_out),
        "assets_total": len(assets_out),
        "craft_recipes": len(craft_recipes),
        "cooking_recipes": len(cooking_doc),
        "cooking_ingredients": len(cooking_ingredients_doc),
        "loot_items": len(loot_items),
    }

    catalog = WagstaffCatalogV2(
        schema_version=SCHEMA_VERSION,
        meta=meta,
        items=items_out,
        assets=assets_out,
        craft=craft_doc,
        cooking=cooking_doc,
        cooking_ingredients=cooking_ingredients_doc,
        stats=stats,
    )

    return catalog, tuning_trace
```

### File: core/indexers/farming_defs.py
- mode: full
- size_bytes: 19303
- sha256_12: c55f523da4ff

```py
# -*- coding: utf-8 -*-
"""Farming defs indexer (farm plants + weeds + fertilizers)."""

from __future__ import annotations

import re
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

from core.lua import (
    LuaRaw,
    LuaTableValue,
    find_matching,
    parse_lua_expr,
    parse_lua_table,
    split_top_level,
    strip_lua_comments,
)
from core.parsers import TuningResolver
from core.indexers.shared import _sha256_12_file
from core.schemas.meta import build_meta


SCHEMA_VERSION = 1


def _read(engine: Any, path: str) -> str:
    return engine.read_file(path) or ""


def _parse_locals(src: str) -> Dict[str, Any]:
    clean = strip_lua_comments(src or "")
    out: Dict[str, Any] = {}
    for m in re.finditer(r"^\s*local\s+([A-Za-z0-9_]+)\s*=\s*(.+?)\s*$", clean, flags=re.MULTILINE):
        name = m.group(1)
        rhs = (m.group(2) or "").strip().rstrip(",")
        if not name or not rhs:
            continue
        val = parse_lua_expr(rhs)
        if isinstance(val, LuaRaw):
            out[name] = val.text
        else:
            out[name] = val
    return out


def _substitute_locals(expr: str, locals_map: Dict[str, Any]) -> str:
    if not expr or not locals_map:
        return expr

    def repl(m: re.Match) -> str:
        key = m.group(0)
        if key in locals_map:
            v = locals_map[key]
            if isinstance(v, (int, float)):
                return str(v)
            if isinstance(v, str):
                return v
        return key

    return re.sub(r"\b[A-Za-z_][A-Za-z0-9_]*\b", repl, expr)


def _convert_tuning_table_value(value: Any, *, tuning: Optional[TuningResolver], locals_map: Dict[str, Any]) -> Any:
    if isinstance(value, LuaTableValue):
        if value.map:
            return {str(k): _convert_tuning_table_value(v, tuning=tuning, locals_map=locals_map) for k, v in value.map.items()}
        return [_convert_tuning_table_value(v, tuning=tuning, locals_map=locals_map) for v in value.array]
    if isinstance(value, LuaRaw):
        return _resolve_expr(value.text, tuning=tuning, locals_map=locals_map, tuning_table=None)
    return value


def _resolve_expr(
    expr: Any,
    *,
    tuning: Optional[TuningResolver],
    locals_map: Dict[str, Any],
    tuning_table: Optional[Dict[str, Any]],
) -> Any:
    if expr is None:
        return None
    if isinstance(expr, (int, float, bool)):
        return expr
    if isinstance(expr, LuaRaw):
        expr = expr.text
    if not isinstance(expr, str):
        return expr

    parsed = parse_lua_expr(expr)
    if not isinstance(parsed, LuaRaw):
        return parsed

    raw = _substitute_locals(parsed.text, locals_map).strip()
    if not raw:
        return raw

    m = re.match(r"^TUNING\.([A-Za-z0-9_]+)$", raw)
    if m and tuning_table is not None:
        key = m.group(1)
        if key in tuning_table:
            return _convert_tuning_table_value(tuning_table[key], tuning=tuning, locals_map=locals_map)

    if tuning is not None:
        val = tuning._resolve_ref(raw)
        if val is not None:
            return val

    return raw


def _parse_table_expr(
    rhs: str,
    *,
    tuning: Optional[TuningResolver],
    locals_map: Dict[str, Any],
    tuning_table: Optional[Dict[str, Any]],
) -> Any:
    rhs = (rhs or "").strip().rstrip(",")
    if not (rhs.startswith("{") and rhs.endswith("}")):
        return _resolve_expr(rhs, tuning=tuning, locals_map=locals_map, tuning_table=tuning_table)
    inner = rhs[1:-1]
    tbl = parse_lua_table(inner)
    if tbl.map:
        return {
            str(k): _resolve_expr(v, tuning=tuning, locals_map=locals_map, tuning_table=tuning_table)
            for k, v in tbl.map.items()
        }
    return [_resolve_expr(v, tuning=tuning, locals_map=locals_map, tuning_table=tuning_table) for v in tbl.array]


def _extract_call_args(expr: str, fn_name: str) -> List[str]:
    expr = (expr or "").strip()
    if not expr.startswith(fn_name):
        return []
    open_idx = expr.find("(")
    if open_idx == -1:
        return []
    close_idx = find_matching(expr, open_idx, "(", ")")
    if close_idx is None:
        return []
    inner = expr[open_idx + 1 : close_idx]
    return [s.strip() for s in split_top_level(inner, sep=",") if s.strip()]


def _extract_tuning_tables(src: str, keys: Iterable[str]) -> Dict[str, Any]:
    clean = strip_lua_comments(src or "")
    out: Dict[str, Any] = {}
    for key in keys:
        if not key:
            continue
        m = re.search(rf"\b{re.escape(key)}\s*=\s*\{{", clean)
        if not m:
            continue
        open_idx = clean.find("{", m.end() - 1)
        close_idx = find_matching(clean, open_idx, "{", "}")
        if close_idx is None:
            continue
        inner = clean[open_idx + 1 : close_idx]
        try:
            tbl = parse_lua_table(inner)
        except Exception:
            continue
        out[key] = tbl
    return out


def _parse_top_level_tables(src: str, prefix: str) -> Dict[str, Dict[str, Any]]:
    clean = strip_lua_comments(src or "")
    out: Dict[str, Dict[str, Any]] = {}
    pat = re.compile(rf"\b{re.escape(prefix)}\.([A-Za-z0-9_]+)\s*=\s*\{{")
    for m in pat.finditer(clean):
        name = m.group(1)
        open_idx = clean.find("{", m.end() - 1)
        close_idx = find_matching(clean, open_idx, "{", "}")
        if close_idx is None:
            continue
        inner = clean[open_idx + 1 : close_idx]
        tbl = parse_lua_table(inner)
        row: Dict[str, Any] = {}
        for k, v in tbl.map.items():
            key = str(k)
            row[key] = v
        out[name] = row
    return out


def _parse_field_assignments(src: str, prefix: str) -> Iterable[Tuple[str, str, str]]:
    clean = strip_lua_comments(src or "")
    for line in clean.splitlines():
        if not line.strip():
            continue
        m = re.match(rf"^\s*{re.escape(prefix)}\.([A-Za-z0-9_]+)\.([A-Za-z0-9_]+)\s*=\s*(.+?)\s*$", line)
        if not m:
            continue
        name, field, rhs = m.group(1), m.group(2), m.group(3)
        if not rhs or rhs.strip() == "":
            continue
        yield name, field, rhs.strip()


def _compute_plant_grow_time(
    args: List[str],
    *,
    tuning: Optional[TuningResolver],
    locals_map: Dict[str, Any],
    tuning_table: Optional[Dict[str, Any]],
) -> Dict[str, Any]:
    if len(args) < 4:
        return {}
    germ_min = _resolve_expr(args[0], tuning=tuning, locals_map=locals_map, tuning_table=tuning_table)
    germ_max = _resolve_expr(args[1], tuning=tuning, locals_map=locals_map, tuning_table=tuning_table)
    full_min = _resolve_expr(args[2], tuning=tuning, locals_map=locals_map, tuning_table=tuning_table)
    full_max = _resolve_expr(args[3], tuning=tuning, locals_map=locals_map, tuning_table=tuning_table)

    def _mul(val: Any, factor: float) -> Any:
        if isinstance(val, (int, float)):
            return val * factor
        return val

    total_day = None
    if tuning is not None:
        total_day = tuning._resolve_ref("TUNING.TOTAL_DAY_TIME")
    total_day = total_day if isinstance(total_day, (int, float)) else None

    grow = {
        "seed": [germ_min, germ_max],
        "sprout": [_mul(full_min, 0.5), _mul(full_max, 0.5)],
        "small": [_mul(full_min, 0.3), _mul(full_max, 0.3)],
        "med": [_mul(full_min, 0.2), _mul(full_max, 0.2)],
    }
    if total_day is not None:
        grow["full"] = 4 * total_day
        grow["oversized"] = 6 * total_day
        grow["regrow"] = [4 * total_day, 5 * total_day]
    return grow


def _compute_weed_grow_time(
    args: List[str],
    *,
    tuning: Optional[TuningResolver],
    locals_map: Dict[str, Any],
    tuning_table: Optional[Dict[str, Any]],
) -> Dict[str, Any]:
    if len(args) < 3:
        return {}
    full_min = _resolve_expr(args[0], tuning=tuning, locals_map=locals_map, tuning_table=tuning_table)
    full_max = _resolve_expr(args[1], tuning=tuning, locals_map=locals_map, tuning_table=tuning_table)
    bolting = _resolve_expr(args[2], tuning=tuning, locals_map=locals_map, tuning_table=tuning_table)

    def _mul(val: Any, factor: float) -> Any:
        if isinstance(val, (int, float)):
            return val * factor
        return val

    grow = {}
    if bolting:
        grow["small"] = [_mul(full_min, 0.3), _mul(full_max, 0.3)]
        grow["med"] = [_mul(full_min, 0.3), _mul(full_max, 0.3)]
        grow["full"] = [_mul(full_min, 0.4), _mul(full_max, 0.4)]
    else:
        grow["small"] = [_mul(full_min, 0.6), _mul(full_max, 0.6)]
        grow["med"] = [_mul(full_min, 0.4), _mul(full_max, 0.4)]
    return grow


def _parse_seed_weights(veggies_src: str, tuning: Optional[TuningResolver]) -> Dict[str, Any]:
    clean = strip_lua_comments(veggies_src or "")
    locals_map = _parse_locals(clean)
    m = re.search(r"\bVEGGIES\s*=\s*\{", clean)
    if not m:
        return {}
    open_idx = clean.find("{", m.end() - 1)
    close_idx = find_matching(clean, open_idx, "{", "}")
    if close_idx is None:
        return {}
    inner = clean[open_idx + 1 : close_idx]
    tbl = parse_lua_table(inner)
    out: Dict[str, Any] = {}
    for key, val in tbl.map.items():
        if not isinstance(key, str):
            continue
        if not isinstance(val, LuaRaw):
            continue
        args = _extract_call_args(val.text, "MakeVegStats")
        if not args:
            continue
        weight = _resolve_expr(args[0], tuning=tuning, locals_map=locals_map, tuning_table=None)
        out[key] = weight
    return out


def _parse_plants(
    src: str,
    *,
    tuning: Optional[TuningResolver],
    tuning_table: Optional[Dict[str, Any]],
    seed_weights: Dict[str, Any],
) -> Dict[str, Dict[str, Any]]:
    locals_map = _parse_locals(src)
    defs = _parse_top_level_tables(src, "PLANT_DEFS")

    allowed_fields = {
        "grow_time",
        "moisture",
        "good_seasons",
        "nutrient_consumption",
        "max_killjoys_tolerance",
        "is_randomseed",
        "fireproof",
        "weight_data",
    }

    for name, field, rhs in _parse_field_assignments(src, "PLANT_DEFS"):
        if field not in allowed_fields:
            continue
        defs.setdefault(name, {})
        if field == "grow_time":
            args = _extract_call_args(rhs, "MakeGrowTimes")
            defs[name][field] = _compute_plant_grow_time(
                args, tuning=tuning, locals_map=locals_map, tuning_table=tuning_table
            )
            continue
        defs[name][field] = _parse_table_expr(
            rhs, tuning=tuning, locals_map=locals_map, tuning_table=tuning_table
        )

    for name, data in defs.items():
        if "nutrient_consumption" in data and isinstance(data["nutrient_consumption"], list):
            data["nutrient_restoration"] = [
                True if v == 0 else None for v in data["nutrient_consumption"]
            ]

        data.setdefault("prefab", f"farm_plant_{name}")
        data.setdefault("bank", data.get("prefab"))
        data.setdefault("build", data.get("prefab"))

        if data.get("is_randomseed"):
            data["seed"] = "seeds"
            data["plant_type_tag"] = "farm_plant_randomseed"
            data.setdefault("family_min_count", 0)
        else:
            data["product"] = name
            data["product_oversized"] = f"{name}_oversized"
            data["seed"] = f"{name}_seeds"
            data["plant_type_tag"] = f"farm_plant_{name}"
            data.setdefault(
                "loot_oversized_rot",
                ["spoiled_food", "spoiled_food", "spoiled_food", data["seed"], "fruitfly", "fruitfly"],
            )
            if "family_min_count" not in data:
                data["family_min_count"] = (
                    tuning._resolve_ref("TUNING.FARM_PLANT_SAME_FAMILY_MIN") if tuning else None
                )
            if "family_check_dist" not in data:
                data["family_check_dist"] = (
                    tuning._resolve_ref("TUNING.FARM_PLANT_SAME_FAMILY_RADIUS") if tuning else None
                )

        if name in seed_weights:
            data["seed_weight"] = seed_weights[name]

    return defs


def _parse_weeds(
    src: str,
    *,
    tuning: Optional[TuningResolver],
    tuning_table: Optional[Dict[str, Any]],
) -> Dict[str, Dict[str, Any]]:
    locals_map = _parse_locals(src)
    defs = _parse_top_level_tables(src, "WEED_DEFS")

    allowed_fields = {
        "grow_time",
        "spread",
        "seed_weight",
        "product",
        "nutrient_consumption",
        "moisture",
        "extra_tags",
        "prefab_deps",
    }

    for name, field, rhs in _parse_field_assignments(src, "WEED_DEFS"):
        if field not in allowed_fields:
            continue
        defs.setdefault(name, {})
        if field == "grow_time":
            args = _extract_call_args(rhs, "MakeGrowTimes")
            defs[name][field] = _compute_weed_grow_time(
                args, tuning=tuning, locals_map=locals_map, tuning_table=tuning_table
            )
            continue
        defs[name][field] = _parse_table_expr(
            rhs, tuning=tuning, locals_map=locals_map, tuning_table=tuning_table
        )

    return defs


def _parse_fertilizers(
    src: str,
    *,
    tuning: Optional[TuningResolver],
    tuning_table: Optional[Dict[str, Any]],
) -> Dict[str, Dict[str, Any]]:
    locals_map = _parse_locals(src)
    defs = _parse_top_level_tables(src, "FERTILIZER_DEFS")

    for name, field, rhs in _parse_field_assignments(src, "FERTILIZER_DEFS"):
        defs.setdefault(name, {})
        defs[name][field] = _parse_table_expr(
            rhs, tuning=tuning, locals_map=locals_map, tuning_table=tuning_table
        )

    for data in defs.values():
        for field, val in list(data.items()):
            data[field] = _resolve_expr(
                val, tuning=tuning, locals_map=locals_map, tuning_table=tuning_table
            )

    return defs


def build_farming_defs(engine: Any) -> Dict[str, Any]:
    tuning_src = _read(engine, "scripts/tuning.lua") or _read(engine, "tuning.lua")
    tuning = engine.tuning if getattr(engine, "tuning", None) is not None else TuningResolver(tuning_src or "")
    tuning_table_keys = [
        "SEASONAL_WEED_SPAWN_CAHNCE",
        "POOP_NUTRIENTS",
        "FERTILIZER_NUTRIENTS",
        "GUANO_NUTRIENTS",
        "SPOILED_FOOD_NUTRIENTS",
        "ROTTENEGG_NUTRIENTS",
        "COMPOST_NUTRIENTS",
        "SPOILED_FISH_SMALL_NUTRIENTS",
        "SPOILED_FISH_NUTRIENTS",
        "SOILAMENDER_NUTRIENTS_LOW",
        "SOILAMENDER_NUTRIENTS_MED",
        "SOILAMENDER_NUTRIENTS_HIGH",
        "COMPOSTWRAP_NUTRIENTS",
        "GLOMMERFUEL_NUTRIENTS",
        "MOSQUITOFERTILIZER_NUTRIENTS",
        "TREEGROWTH_NUTRIENTS",
    ]
    tuning_table = _extract_tuning_tables(tuning_src or "", tuning_table_keys)

    plant_src = _read(engine, "scripts/prefabs/farm_plant_defs.lua")
    weed_src = _read(engine, "scripts/prefabs/weed_defs.lua")
    fert_src = _read(engine, "scripts/prefabs/fertilizer_nutrient_defs.lua")
    veggies_src = _read(engine, "scripts/prefabs/veggies.lua")

    seed_weights = _parse_seed_weights(veggies_src, tuning)
    plants = _parse_plants(
        plant_src, tuning=tuning, tuning_table=tuning_table, seed_weights=seed_weights
    )
    weeds = _parse_weeds(weed_src, tuning=tuning, tuning_table=tuning_table)
    fertilizers = _parse_fertilizers(fert_src, tuning=tuning, tuning_table=tuning_table)

    tuning_keys = [
        "FARM_PLANT_RANDOMSEED_WEED_CHANCE",
        "SEED_WEIGHT_SEASON_MOD",
        "SEED_CHANCE_VERYCOMMON",
        "SEED_CHANCE_COMMON",
        "SEED_CHANCE_UNCOMMON",
        "SEED_CHANCE_RARE",
        "FARM_PLANT_CONSUME_NUTRIENT_LOW",
        "FARM_PLANT_CONSUME_NUTRIENT_MED",
        "FARM_PLANT_CONSUME_NUTRIENT_HIGH",
        "FARM_PLANT_DRINK_LOW",
        "FARM_PLANT_DRINK_MED",
        "FARM_PLANT_DRINK_HIGH",
        "FARM_PLANT_DROUGHT_TOLERANCE",
        "FARM_PLANT_KILLJOY_RADIUS",
        "FARM_PLANT_KILLJOY_TOLERANCE",
        "FARM_PANT_OVERCROWDING_MAX_PLANTS",
        "FARM_PLANT_SAME_FAMILY_MIN",
        "FARM_PLANT_SAME_FAMILY_RADIUS",
        "FARM_PLANT_LONG_LIFE_MULT",
        "STARTING_NUTRIENTS_MIN",
        "STARTING_NUTRIENTS_MAX",
        "SOIL_MOISTURE_UPDATE_TIME",
        "SOIL_RAIN_MOD",
        "SOIL_MIN_DRYING_TEMP",
        "SOIL_MAX_DRYING_TEMP",
        "SOIL_MIN_TEMP_DRY_RATE",
        "SOIL_MAX_TEMP_DRY_RATE",
        "SOIL_MAX_MOISTURE_VALUE",
        "FARM_TILL_SPACING",
        "FARM_PLANT_PHYSICS_RADIUS",
        "FARM_PLOW_USES",
        "FARM_HOE_USES",
        "SEASONAL_WEED_SPAWN_CAHNCE",
        "FORGETMELOTS_RESPAWNER_MIN",
        "FORGETMELOTS_RESPAWNER_VAR",
        "FIRE_NETTLE_TOXIN_TEMP_MODIFIER",
        "FIRE_NETTLE_TOXIN_DURATION",
        "WEED_FIRENETTLE_DAMAGE",
        "WEED_TILLWEED_MAX_DEBRIS",
        "WEED_TILLWEED_DEBRIS_TIME_MIN",
        "WEED_TILLWEED_DEBRIS_TIME_VAR",
        "FORMULA_NUTRIENTS_INDEX",
        "COMPOST_NUTRIENTS_INDEX",
        "MANURE_NUTRIENTS_INDEX",
        "POOP_NUTRIENTS",
        "FERTILIZER_NUTRIENTS",
        "GUANO_NUTRIENTS",
        "SPOILED_FOOD_NUTRIENTS",
        "ROTTENEGG_NUTRIENTS",
        "COMPOST_NUTRIENTS",
        "SPOILED_FISH_SMALL_NUTRIENTS",
        "SPOILED_FISH_NUTRIENTS",
        "SOILAMENDER_NUTRIENTS_LOW",
        "SOILAMENDER_NUTRIENTS_MED",
        "SOILAMENDER_NUTRIENTS_HIGH",
        "COMPOSTWRAP_NUTRIENTS",
        "GLOMMERFUEL_NUTRIENTS",
        "MOSQUITOFERTILIZER_NUTRIENTS",
        "TREEGROWTH_NUTRIENTS",
    ]
    tuning_out: Dict[str, Any] = {}
    for key in tuning_keys:
        if key in tuning_table:
            tuning_out[key] = _convert_tuning_table_value(
                tuning_table[key], tuning=tuning, locals_map={}
            )
            continue
        tuning_out[key] = tuning._resolve_ref(f"TUNING.{key}") if tuning else None

    scripts_zip = getattr(getattr(engine, "source", None), "filename", None)
    scripts_sha = _sha256_12_file(Path(scripts_zip)) if scripts_zip else None
    scripts_dir = getattr(engine, "source", None) if getattr(engine, "mode", "") == "folder" else None
    sources = {
        "scripts_zip": scripts_zip,
        "scripts_dir": scripts_dir,
        "farm_plant_defs": "scripts/prefabs/farm_plant_defs.lua",
        "weed_defs": "scripts/prefabs/weed_defs.lua",
        "fertilizer_defs": "scripts/prefabs/fertilizer_nutrient_defs.lua",
        "veggies_defs": "scripts/prefabs/veggies.lua",
        "tuning": "scripts/tuning.lua",
    }
    meta = build_meta(
        schema=SCHEMA_VERSION,
        tool="build_farming_defs",
        sources=sources,
        extra={
            "scripts_sha256_12": scripts_sha,
            "scripts_zip": scripts_zip,
            "scripts_dir": scripts_dir,
        },
    )

    stats = {
        "plants_total": len(plants),
        "weeds_total": len(weeds),
        "fertilizers_total": len(fertilizers),
        "seed_weights_total": len(seed_weights),
    }

    return {
        "schema_version": SCHEMA_VERSION,
        "meta": meta,
        "tuning": tuning_out,
        "seed_weights": seed_weights,
        "plants": plants,
        "weeds": weeds,
        "fertilizers": fertilizers,
        "stats": stats,
    }
```

### File: core/indexers/i18n_index.py
- mode: full
- size_bytes: 6512
- sha256_12: e2e7d3de4162

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""i18n index helpers (core).

Used by devtools to build data/index/wagstaff_i18n_v1.json.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Iterable, Optional, List, Tuple


_NAMES_PREFIX = "STRINGS.NAMES."


def _po_unquote(s: str) -> str:
    """Unquote a PO string literal line segment."""

    s = (s or "").strip()
    if not (s.startswith('"') and s.endswith('"')):
        return ""
    inner = s[1:-1]
    out: List[str] = []
    i = 0
    while i < len(inner):
        ch = inner[i]
        if ch == "\\" and i + 1 < len(inner):
            nxt = inner[i + 1]
            if nxt == "n":
                out.append("\n")
            elif nxt == "t":
                out.append("\t")
            elif nxt == "r":
                out.append("\r")
            elif nxt == '"':
                out.append('"')
            elif nxt == "\\":
                out.append("\\")
            else:
                out.append(nxt)
            i += 2
            continue
        out.append(ch)
        i += 1
    return "".join(out)


def parse_po(text: str) -> Dict[str, str]:
    """Parse a PO file and return mapping: msgctxt -> msgstr.

    Notes
    - Only keep entries with non-empty msgctxt and msgstr.
    - For plural forms, take msgstr[0] only.
    """

    lines = (text or "").splitlines()
    cur: Dict[str, Any] = {}
    last_key: Optional[str] = None
    out: Dict[str, str] = {}

    def commit() -> None:
        nonlocal cur, last_key
        ctx = cur.get("msgctxt")
        msgstr = cur.get("msgstr")
        if isinstance(ctx, str) and ctx and isinstance(msgstr, str) and msgstr:
            out[ctx] = msgstr
        cur = {}
        last_key = None

    for raw in lines:
        line = raw.rstrip("\n")
        s = line.strip()
        if not s:
            commit()
            continue
        if s.startswith("#"):
            continue

        if s.startswith("msgctxt "):
            cur["msgctxt"] = _po_unquote(s[len("msgctxt ") :].strip())
            last_key = "msgctxt"
            continue

        if s.startswith("msgid "):
            cur["msgid"] = _po_unquote(s[len("msgid ") :].strip())
            last_key = "msgid"
            continue

        if s.startswith("msgid_plural "):
            cur["msgid_plural"] = _po_unquote(s[len("msgid_plural ") :].strip())
            last_key = "msgid_plural"
            continue

        if s.startswith("msgstr["):
            rb = s.find("]")
            idx_s = s[len("msgstr[") : rb].strip() if rb != -1 else ""
            try:
                idx = int(idx_s)
            except Exception:
                idx = -1
            if idx == 0:
                rest = s[rb + 1 :].strip() if rb != -1 else ""
                cur["msgstr"] = _po_unquote(rest)
                last_key = "msgstr"
            else:
                last_key = None
            continue

        if s.startswith("msgstr "):
            cur["msgstr"] = _po_unquote(s[len("msgstr ") :].strip())
            last_key = "msgstr"
            continue

        if s.startswith('"') and last_key:
            cur[last_key] = str(cur.get(last_key) or "") + _po_unquote(s)
            continue

    commit()
    return out


def _normalize_key(key: str) -> str:
    return str(key or "").strip().lower()


def extract_name_table(po_text: str) -> Dict[str, str]:
    """Return normalized key -> localized name."""

    ctx_map = parse_po(po_text or "")
    names: Dict[str, str] = {}
    for ctx, val in ctx_map.items():
        if not isinstance(ctx, str) or not ctx.startswith(_NAMES_PREFIX):
            continue
        key = _normalize_key(ctx[len(_NAMES_PREFIX) :])
        if not key:
            continue
        v = str(val or "").strip()
        if not v:
            continue
        names[key] = v
        if "_" in key:
            names.setdefault(key.replace("_", ""), v)
    return names


def build_item_name_map(po_text: str, *, item_ids: Optional[Iterable[str]] = None) -> Dict[str, str]:
    """Build item_id -> localized name mapping from PO content."""

    raw = extract_name_table(po_text or "")
    if not raw:
        return {}
    if item_ids is None:
        return dict(raw)

    out: Dict[str, str] = {}
    for iid in item_ids:
        if not iid:
            continue
        k1 = _normalize_key(iid)
        if not k1:
            continue
        k2 = k1.replace("_", "")
        v = raw.get(k1) or raw.get(k2)
        if v:
            out[str(iid)] = v
    return out


def load_ui_strings(path: Path) -> Dict[str, Dict[str, str]]:
    """Load UI strings JSON: {lang: {key: text}}."""

    p = Path(path)
    if not p.exists() or not p.is_file():
        return {}
    try:
        doc = json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return {}
    if not isinstance(doc, dict):
        return {}

    out: Dict[str, Dict[str, str]] = {}
    for lang, mp in doc.items():
        if not isinstance(mp, dict):
            continue
        l = _normalize_key(lang)
        if not l:
            continue
        out[l] = {str(k): str(v) for k, v in mp.items() if k and v}
    return out


def load_tag_strings(path: Path) -> Tuple[Dict[str, Dict[str, str]], Dict[str, Dict[str, str]]]:
    """Load tag strings JSON: {lang: {tag: {text, source}}}."""

    p = Path(path)
    if not p.exists() or not p.is_file():
        return {}, {}
    try:
        doc = json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return {}, {}
    if not isinstance(doc, dict):
        return {}, {}

    tags: Dict[str, Dict[str, str]] = {}
    meta: Dict[str, Dict[str, str]] = {}
    for lang, mp in doc.items():
        if not isinstance(mp, dict):
            continue
        l = _normalize_key(lang)
        if not l:
            continue
        tags.setdefault(l, {})
        meta.setdefault(l, {})
        for key, val in mp.items():
            k = _normalize_key(key)
            if not k:
                continue
            text = ""
            source = "manual"
            if isinstance(val, dict):
                text = str(val.get("text") or val.get("label") or "").strip()
                source = str(val.get("source") or val.get("src") or "manual").strip().lower() or "manual"
            else:
                text = str(val or "").strip()
            if not text:
                continue
            tags[l][k] = text
            meta[l][k] = source
    return tags, meta
```

### File: core/indexers/mechanism_index.py
- mode: full
- size_bytes: 16549
- sha256_12: 04943ae9991c

```py
# -*- coding: utf-8 -*-
"""Mechanism index builder (core).

Focus: component definitions + prefab/component linkage.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Dict, List, Optional, Set

from core.indexers.shared import _sha256_12_file
from core.parsers import ComponentParser, PrefabParser
from core.schemas.meta import build_meta


SCHEMA_VERSION = 1


def _scan_components(engine: Any) -> Dict[str, Any]:
    files = [
        f
        for f in getattr(engine, "file_list", []) or []
        if str(f).startswith("scripts/components/") and str(f).endswith(".lua")
    ]

    items: Dict[str, Dict[str, Any]] = {}
    for path in files:
        content = engine.read_file(path) or ""
        if not content:
            continue
        parsed = ComponentParser(content, path=path).parse()
        cid = str(parsed.get("id") or "").strip().lower()
        if not cid:
            continue
        items[cid] = parsed

    return {
        "total_files": len(files),
        "items": items,
    }


def _normalize_prefab_assets(assets: Any) -> List[Dict[str, str]]:
    if not isinstance(assets, list):
        return []
    out: List[Dict[str, str]] = []
    seen: Set[str] = set()
    for row in assets:
        if not isinstance(row, dict):
            continue
        asset_type = str(row.get("type") or "").strip()
        path = str(row.get("path") or "").strip()
        if not asset_type or not path:
            continue
        key = f"{asset_type}:{path}"
        if key in seen:
            continue
        out.append({"type": asset_type, "path": path})
        seen.add(key)
    return out


def _component_calls_to_map(calls: Any) -> Dict[str, Dict[str, Set[str]]]:
    out: Dict[str, Dict[str, Set[str]]] = {}
    if not isinstance(calls, list):
        return out
    for row in calls:
        if not isinstance(row, dict):
            continue
        comp = row.get("component") or row.get("name")
        comp = str(comp or "").strip().lower()
        if not comp:
            continue
        entry = out.setdefault(comp, {"methods": set(), "properties": set()})
        for method in row.get("methods") or []:
            if method:
                entry["methods"].add(str(method))
        for prop in row.get("properties") or []:
            if prop:
                entry["properties"].add(str(prop))
    return out


def _component_calls_from_map(calls_map: Dict[str, Dict[str, Set[str]]]) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for comp in sorted(calls_map.keys()):
        entry = calls_map.get(comp) or {}
        out.append(
            {
                "component": comp,
                "methods": sorted(entry.get("methods") or []),
                "properties": sorted(entry.get("properties") or []),
            }
        )
    return out


def _scan_prefab_details(engine: Any, resource_index: Optional[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    prefab_files = [
        f for f in getattr(engine, "file_list", []) if str(f).startswith("scripts/prefabs/") and str(f).endswith(".lua")
    ]

    file_prefabs: Dict[str, List[str]] = {}
    prefabs_meta = (resource_index or {}).get("prefabs") or {}
    files_meta = prefabs_meta.get("files") if isinstance(prefabs_meta, dict) else None
    if isinstance(files_meta, list):
        for row in files_meta:
            if not isinstance(row, dict):
                continue
            path = row.get("path")
            prefabs = row.get("prefabs") or []
            if not path or not isinstance(prefabs, list):
                continue
            file_prefabs[str(path)] = [str(p).strip().lower() for p in prefabs if p]

    store: Dict[str, Dict[str, Any]] = {}
    for path in prefab_files:
        content = engine.read_file(path) or ""
        if not content:
            continue
        parsed = PrefabParser(content, path=path).parse()
        events = [str(e).strip() for e in (parsed.get("events") or []) if str(e).strip()]
        assets = _normalize_prefab_assets(parsed.get("assets"))
        comp_calls = _component_calls_from_map(_component_calls_to_map(parsed.get("components")))
        if not events and not assets and not comp_calls:
            continue

        prefabs = file_prefabs.get(str(path)) or []
        if not prefabs:
            fallback = parsed.get("prefab_name") or Path(str(path)).stem
            if fallback:
                prefabs = [str(fallback).strip().lower()]

        for pid in prefabs:
            if not pid:
                continue
            bucket = store.setdefault(pid, {"events": set(), "assets": {}, "component_calls": {}})
            bucket["events"].update(events)
            for asset in assets:
                key = f"{asset.get('type')}:{asset.get('path')}"
                bucket["assets"][key] = asset
            comp_map = bucket["component_calls"]
            incoming = _component_calls_to_map(comp_calls)
            for comp, data in incoming.items():
                entry = comp_map.setdefault(comp, {"methods": set(), "properties": set()})
                entry["methods"].update(data.get("methods") or set())
                entry["properties"].update(data.get("properties") or set())

    out: Dict[str, Dict[str, Any]] = {}
    for pid, bucket in store.items():
        row: Dict[str, Any] = {}
        events = sorted(bucket.get("events") or [])
        if events:
            row["events"] = events
        assets_map = bucket.get("assets") or {}
        if assets_map:
            row["assets"] = [assets_map[k] for k in sorted(assets_map.keys())]
        calls_map = bucket.get("component_calls") or {}
        if calls_map:
            row["component_calls"] = _component_calls_from_map(calls_map)
        out[pid] = row

    return out


def _merge_list(base: Any, extra: Any) -> List[str]:
    base_list = [str(x) for x in (base or []) if x]
    extra_list = [str(x) for x in (extra or []) if x]
    return sorted(set(base_list) | set(extra_list))


def _merge_component_calls(base: Any, extra: Any) -> List[Dict[str, Any]]:
    base_map = _component_calls_to_map(base)
    extra_map = _component_calls_to_map(extra)
    for comp, data in extra_map.items():
        entry = base_map.setdefault(comp, {"methods": set(), "properties": set()})
        entry["methods"].update(data.get("methods") or set())
        entry["properties"].update(data.get("properties") or set())
    return _component_calls_from_map(base_map)


def _merge_assets(base: Any, extra: Any) -> List[Dict[str, str]]:
    merged: Dict[str, Dict[str, str]] = {}
    for row in _normalize_prefab_assets(base) + _normalize_prefab_assets(extra):
        key = f"{row.get('type')}:{row.get('path')}"
        merged[key] = row
    return [merged[k] for k in sorted(merged.keys())]


def _build_prefab_links(
    resource_index: Optional[Dict[str, Any]],
    prefab_details: Optional[Dict[str, Dict[str, Any]]] = None,
) -> Dict[str, Any]:
    prefabs = (resource_index or {}).get("prefabs") or {}
    items = prefabs.get("items") or {}
    if not isinstance(items, dict):
        items = {}

    out: Dict[str, Dict[str, Any]] = {}
    for iid, row in items.items():
        if not iid or not isinstance(row, dict):
            continue
        entry = {
            "components": sorted({str(x) for x in (row.get("components") or []) if x}),
            "tags": sorted({str(x) for x in (row.get("tags") or []) if x}),
            "brains": sorted({str(x) for x in (row.get("brains") or []) if x}),
            "stategraphs": sorted({str(x) for x in (row.get("stategraphs") or []) if x}),
            "helpers": sorted({str(x) for x in (row.get("helpers") or []) if x}),
            "files": sorted({str(x) for x in (row.get("files") or []) if x}),
        }
        assets = _normalize_prefab_assets(row.get("assets"))
        if assets:
            entry["assets"] = assets
        out[str(iid)] = entry

    if prefab_details:
        for pid, extra in prefab_details.items():
            if not pid or not isinstance(extra, dict):
                continue
            entry = out.setdefault(
                str(pid),
                {
                    "components": [],
                    "tags": [],
                    "brains": [],
                    "stategraphs": [],
                    "helpers": [],
                    "files": [],
                },
            )
            events = _merge_list(entry.get("events"), extra.get("events"))
            if events:
                entry["events"] = events
            assets = _merge_assets(entry.get("assets"), extra.get("assets"))
            if assets:
                entry["assets"] = assets
            component_calls = _merge_component_calls(entry.get("component_calls"), extra.get("component_calls"))
            if component_calls:
                entry["component_calls"] = component_calls
                entry["components"] = _merge_list(
                    entry.get("components"),
                    [row.get("component") for row in component_calls if isinstance(row, dict)],
                )
    return out


def _build_component_usage(prefab_links: Dict[str, Any]) -> Dict[str, List[str]]:
    usage: Dict[str, Set[str]] = {}
    for pid, row in (prefab_links or {}).items():
        comps = row.get("components") if isinstance(row, dict) else None
        if not isinstance(comps, list):
            continue
        for c in comps:
            if not c:
                continue
            usage.setdefault(str(c), set()).add(str(pid))
    return {k: sorted(v) for k, v in usage.items()}


def render_mechanism_index_summary(index: Dict[str, Any]) -> str:
    meta = index.get("meta") or {}
    counts = index.get("counts") or {}
    usage = index.get("component_usage") or {}

    rows: List[Dict[str, Any]] = []
    for cid, prefabs in (usage or {}).items():
        if not cid:
            continue
        n = len(prefabs) if isinstance(prefabs, list) else 0
        rows.append({"component": cid, "prefabs": n})

    rows.sort(key=lambda x: (-int(x.get("prefabs") or 0), str(x.get("component") or "")))
    top_rows = rows[:20]

    lines: List[str] = []
    lines.append("# Wagstaff Mechanism Index Summary")
    lines.append("")
    lines.append("## Meta")
    lines.append("```yaml")
    lines.append(f"schema_version: {index.get('schema_version')}")
    lines.append(f"generated: {meta.get('generated')}")
    lines.append(f"scripts_sha256_12: {meta.get('scripts_sha256_12')}")
    if meta.get("scripts_zip"):
        lines.append(f"scripts_zip: {meta.get('scripts_zip')}")
    if meta.get("scripts_dir"):
        lines.append(f"scripts_dir: {meta.get('scripts_dir')}")
    lines.append("```")
    lines.append("")
    lines.append("## Counts")
    lines.append("```yaml")
    for k, v in counts.items():
        lines.append(f"{k}: {v}")
    lines.append("```")
    lines.append("")
    lines.append("## Top Components by Prefab Usage")
    lines.append("")
    lines.append("| Component | Prefabs |")
    lines.append("| --- | --- |")
    for row in top_rows:
        lines.append(f"| {row.get('component')} | {row.get('prefabs')} |")
    return "\n".join(lines) + "\n"


def render_mechanism_crosscheck_report(
    resource_index: Optional[Dict[str, Any]],
    mechanism_index: Dict[str, Any],
) -> str:
    lines: List[str] = []
    lines.append("# Wagstaff Mechanism Crosscheck Report")
    lines.append("")

    if not resource_index or not isinstance(resource_index, dict):
        lines.append("Resource index not available; crosscheck skipped.")
        return "\n".join(lines) + "\n"

    res_prefabs = (resource_index.get("prefabs") or {}).get("items") or {}
    mech_prefabs = (mechanism_index.get("prefabs") or {}).get("items") or {}

    if not isinstance(res_prefabs, dict):
        res_prefabs = {}
    if not isinstance(mech_prefabs, dict):
        mech_prefabs = {}

    res_prefab_ids = {str(k) for k in res_prefabs.keys() if k}
    mech_prefab_ids = {str(k) for k in mech_prefabs.keys() if k}

    missing_prefabs = sorted(res_prefab_ids - mech_prefab_ids)
    extra_prefabs = sorted(mech_prefab_ids - res_prefab_ids)

    res_components_used: Set[str] = set()
    prefabs_without_components: List[str] = []
    for pid, row in res_prefabs.items():
        comps = row.get("components") if isinstance(row, dict) else None
        if not isinstance(comps, list) or not comps:
            prefabs_without_components.append(str(pid))
            continue
        for c in comps:
            if c:
                res_components_used.add(str(c))

    mech_components = (mechanism_index.get("components") or {}).get("items") or {}
    if not isinstance(mech_components, dict):
        mech_components = {}
    mech_component_ids = {str(k) for k in mech_components.keys() if k}
    mech_component_usage = mechanism_index.get("component_usage") or {}
    mech_component_used = {str(k) for k in mech_component_usage.keys() if k}

    missing_component_defs = sorted(res_components_used - mech_component_ids)
    unused_component_defs = sorted(mech_component_ids - mech_component_used)

    lines.append("## Counts")
    lines.append("```yaml")
    lines.append(f"resource_prefabs: {len(res_prefab_ids)}")
    lines.append(f"mechanism_prefabs: {len(mech_prefab_ids)}")
    lines.append(f"missing_prefabs: {len(missing_prefabs)}")
    lines.append(f"extra_prefabs: {len(extra_prefabs)}")
    lines.append(f"resource_components_used: {len(res_components_used)}")
    lines.append(f"mechanism_components_defined: {len(mech_component_ids)}")
    lines.append(f"missing_component_defs: {len(missing_component_defs)}")
    lines.append(f"unused_component_defs: {len(unused_component_defs)}")
    lines.append(f"prefabs_without_components: {len(prefabs_without_components)}")
    lines.append("```")
    lines.append("")

    def _section(title: str, items: List[str], limit: int = 40) -> None:
        lines.append(f"## {title}")
        if not items:
            lines.append("")
            lines.append("(none)")
            lines.append("")
            return
        lines.append("")
        lines.append("```text")
        for x in items[:limit]:
            lines.append(str(x))
        if len(items) > limit:
            lines.append(f"... ({len(items) - limit} more)")
        lines.append("```")
        lines.append("")

    _section("Missing Prefabs in Mechanism Index", missing_prefabs)
    _section("Extra Prefabs in Mechanism Index", extra_prefabs)
    _section("Missing Component Definitions", missing_component_defs)
    _section("Unused Component Definitions", unused_component_defs)
    _section("Prefabs Without Components (Resource Index)", prefabs_without_components)

    return "\n".join(lines) + "\n"


def build_mechanism_index(
    *,
    engine: Any,
    resource_index: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    components = _scan_components(engine)
    prefab_details = _scan_prefab_details(engine, resource_index)
    prefab_links = _build_prefab_links(resource_index, prefab_details)
    component_usage = _build_component_usage(prefab_links)
    mapping = {
        "prefab_component": [
            {"source": "prefab", "source_id": pid, "target": "component", "target_id": cid}
            for cid, pids in component_usage.items()
            for pid in pids
        ]
    }

    scripts_zip = getattr(getattr(engine, "source", None), "filename", None)
    scripts_sha = _sha256_12_file(Path(scripts_zip)) if scripts_zip else None
    scripts_dir = getattr(engine, "source", None) if getattr(engine, "mode", "") == "folder" else None

    meta = build_meta(
        schema=SCHEMA_VERSION,
        tool="build_mechanism_index",
        sources={
            "resource_index": "wagstaff_resource_index_v1.json",
            "scripts_zip": scripts_zip,
            "scripts_dir": scripts_dir,
        },
        extra={
            "scripts_sha256_12": scripts_sha,
            "scripts_zip": scripts_zip,
            "scripts_dir": scripts_dir,
        },
    )

    return {
        "schema_version": SCHEMA_VERSION,
        "meta": meta,
        "counts": {
            "components_total": len(components.get("items") or {}),
            "prefabs_total": len(prefab_links),
            "components_used": len(component_usage),
            "prefab_component_edges": len(mapping.get("prefab_component") or []),
        },
        "components": components,
        "prefabs": {"items": prefab_links},
        "component_usage": component_usage,
        "links": mapping,
    }
```

### File: core/indexers/resource_index.py
- mode: full
- size_bytes: 23514
- sha256_12: 26d106eaa728

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Resource index builder (core).

Collects a structured inventory of DST scripts and data resources.
"""

from __future__ import annotations

from collections import Counter, defaultdict
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple
import os
import re
import xml.etree.ElementTree as ET
import zipfile

from core.lua import LuaCallExtractor, parse_lua_string, strip_lua_comments
from core.schemas.meta import build_meta


SCHEMA_VERSION = 1
_ID_RE = re.compile(r"^[a-z0-9_]+$")

SCRIPT_KINDS = [
    ("prefab", "scripts/prefabs/"),
    ("prefab_postinit", "scripts/prefabs_postinit/"),
    ("component", "scripts/components/"),
    ("stategraph", "scripts/stategraphs/"),
    ("brain", "scripts/brains/"),
    ("behaviour", "scripts/behaviours/"),
    ("widget", "scripts/widgets/"),
    ("screen", "scripts/screens/"),
    ("map", "scripts/map/"),
    ("scenario", "scripts/scenarios/"),
    ("string", "scripts/strings"),
    ("language", "scripts/languages/"),
    ("tuning", "scripts/tuning.lua"),
    ("recipe", "scripts/recipes"),
    ("tool", "scripts/tools/"),
    ("util", "scripts/util/"),
]


def _classify_script(path: str) -> str:
    p = (path or "").replace("\\", "/")
    for kind, prefix in SCRIPT_KINDS:
        if prefix.endswith(".lua"):
            if p.endswith(prefix):
                return kind
        elif p.startswith(prefix):
            return kind
    return "other"


def _clean_id(raw: Any) -> Optional[str]:
    if not isinstance(raw, str):
        return None
    s = raw.strip().lower()
    if not s or not _ID_RE.match(s):
        return None
    return s


def _scan_scripts(file_list: Iterable[str]) -> Dict[str, Any]:
    files = [str(f) for f in file_list]
    lua_files = [f for f in files if f.endswith(".lua")]

    by_kind: Dict[str, List[str]] = defaultdict(list)
    items: List[Dict[str, str]] = []
    top_dir = Counter()
    second_dir = Counter()

    for f in lua_files:
        kind = _classify_script(f)
        items.append({"path": f, "kind": kind})
        by_kind[kind].append(f)

        clean = f[8:] if f.startswith("scripts/") else f
        parts = clean.split("/")
        if len(parts) == 1:
            top_dir["[root]"] += 1
        else:
            top_dir[parts[0]] += 1
            if len(parts) >= 2:
                second_dir[f"{parts[0]}/{parts[1]}"] += 1

    categories = {k: len(v) for k, v in by_kind.items()}

    top_dirs = [{"dir": d, "count": c} for d, c in top_dir.most_common(40)]
    top_second = [{"dir": d, "count": c} for d, c in second_dir.most_common(60)]

    return {
        "total_files": len(files),
        "lua_files": len(lua_files),
        "categories": categories,
        "top_dirs": top_dirs,
        "top_second_level": top_second,
        "files": items,
        "by_kind": dict(by_kind),
    }


def _parse_prefab_file(content: str) -> Dict[str, Any]:
    clean = strip_lua_comments(content or "")
    extractor = LuaCallExtractor(clean)

    prefabs: Set[str] = set()
    skipped = 0
    for call in extractor.iter_calls("Prefab", include_member_calls=False):
        if not call.arg_list:
            continue
        nm = parse_lua_string(call.arg_list[0])
        if isinstance(nm, str) and nm:
            n = nm.strip().lower()
            if _ID_RE.match(n):
                prefabs.add(n)
            else:
                skipped += 1

    assets: List[Dict[str, str]] = []
    for call in extractor.iter_calls("Asset", include_member_calls=False):
        if len(call.arg_list) < 2:
            continue
        t = parse_lua_string(call.arg_list[0])
        p = parse_lua_string(call.arg_list[1])
        if isinstance(t, str) and isinstance(p, str):
            assets.append({"type": t, "path": p})

    tags: Set[str] = set()
    for call in extractor.iter_calls("AddTag"):
        if call.arg_list:
            tg = parse_lua_string(call.arg_list[0])
            if isinstance(tg, str) and tg:
                tags.add(tg.strip().lower())

    components: Set[str] = set()
    for call in extractor.iter_calls("AddComponent"):
        if call.arg_list:
            cn = parse_lua_string(call.arg_list[0])
            if isinstance(cn, str) and cn:
                components.add(cn.strip().lower())

    brain = None
    m = re.search(r"SetBrain\s*\(\s*require\s*\(\s*['\"](.*?)['\"]\s*\)\s*\)", clean)
    if m:
        brain = m.group(1)

    stategraph = None
    m = re.search(r"SetStateGraph\s*\(\s*['\"](.*?)['\"]\s*\)", clean)
    if m:
        stategraph = m.group(1)

    helpers = sorted(set(re.findall(r"^\s*(Make[A-Za-z0-9_]+)\s*\(", content or "", flags=re.MULTILINE)))

    return {
        "prefabs": prefabs,
        "prefabs_skipped": skipped,
        "assets": assets,
        "tags": tags,
        "components": components,
        "brain": brain,
        "stategraph": stategraph,
        "helpers": helpers,
    }


def _scan_prefabs(engine: Any) -> Dict[str, Any]:
    prefab_files = [
        f for f in getattr(engine, "file_list", []) if str(f).startswith("scripts/prefabs/") and str(f).endswith(".lua")
    ]

    prefab_items: Dict[str, Dict[str, Any]] = {}
    file_entries: List[Dict[str, Any]] = []
    skipped_total = 0

    for path in prefab_files:
        content = engine.read_file(path) or ""
        if not content:
            continue

        parsed = _parse_prefab_file(content)
        prefabs = set(parsed["prefabs"])

        if not prefabs:
            base = str(path).split("/")[-1].rsplit(".lua", 1)[0].strip().lower()
            if _ID_RE.match(base):
                prefabs.add(base)

        file_entries.append(
            {
                "path": path,
                "prefabs": sorted(prefabs),
                "components": sorted(parsed["components"]),
                "tags": sorted(parsed["tags"]),
                "assets_count": len(parsed["assets"]),
            }
        )

        skipped_total += int(parsed.get("prefabs_skipped", 0) or 0)

        for pf in prefabs:
            entry = prefab_items.setdefault(
                pf,
                {
                    "files": [],
                    "components": set(),
                    "tags": set(),
                    "assets": [],
                    "brains": set(),
                    "stategraphs": set(),
                    "helpers": set(),
                },
            )
            if path not in entry["files"]:
                entry["files"].append(path)
            entry["components"].update(parsed["components"])
            entry["tags"].update(parsed["tags"])
            entry["helpers"].update(parsed["helpers"])
            if parsed.get("brain"):
                entry["brains"].add(parsed["brain"])
            if parsed.get("stategraph"):
                entry["stategraphs"].add(parsed["stategraph"])

            asset_keys = {f"{a.get('type')}:{a.get('path')}" for a in entry["assets"]}
            for a in parsed["assets"]:
                key = f"{a.get('type')}:{a.get('path')}"
                if key not in asset_keys:
                    entry["assets"].append(a)
                    asset_keys.add(key)

    # normalize sets -> lists
    for pf, entry in prefab_items.items():
        entry["components"] = sorted(entry["components"])
        entry["tags"] = sorted(entry["tags"])
        entry["brains"] = sorted(entry["brains"])
        entry["stategraphs"] = sorted(entry["stategraphs"])
        entry["helpers"] = sorted(entry["helpers"])

    return {
        "total_files": len(prefab_files),
        "total_prefabs": len(prefab_items),
        "prefabs_skipped": skipped_total,
        "items": prefab_items,
        "files": file_entries,
    }


def _scan_inventory_icons(dst_root: Path, *, id_filter: Optional[Set[str]] = None) -> Tuple[Set[str], List[str]]:
    icons: Set[str] = set()
    xmls: List[str] = []
    data_dir = dst_root / "data"
    img_dir = data_dir / "images"
    bundles_dir = data_dir / "databundles"

    scan_all_images = bool(id_filter)

    def _parse_xml_bytes(label: str, data: bytes) -> None:
        try:
            root = ET.fromstring(data)
        except Exception:
            return
        matched = False
        for el in root.findall(".//Element"):
            name = el.attrib.get("name")
            if name:
                n = name.strip().lower()
                if n.endswith(".tex"):
                    n = n[:-4]
                if not _ID_RE.match(n):
                    continue
                if id_filter is not None and n not in id_filter:
                    continue
                icons.add(n)
                matched = True
        if matched and label and label not in xmls:
            xmls.append(label)

    def _is_inventory_xml(path: str) -> bool:
        base = os.path.basename(path).lower()
        return base.startswith("inventoryimages") and base.endswith(".xml")

    def _is_images_xml(path: str) -> bool:
        p = (path or "").replace("\\", "/").lstrip("/")
        return p.startswith("images/") and p.endswith(".xml")

    if bundles_dir.is_dir():
        for bundle_zip in sorted(bundles_dir.glob("*.zip")):
            if bundle_zip.name.lower() == "scripts.zip":
                continue
            try:
                with zipfile.ZipFile(bundle_zip, "r") as zf:
                    for name in zf.namelist():
                        nrm = (name or "").replace("\\", "/")
                        if not nrm.lower().endswith(".xml"):
                            continue
                        if scan_all_images:
                            if not _is_images_xml(nrm):
                                continue
                        else:
                            if not _is_inventory_xml(nrm):
                                continue
                        try:
                            _parse_xml_bytes(
                                (bundle_zip.relative_to(dst_root).as_posix() + ":" + name),
                                zf.read(name),
                            )
                        except Exception:
                            continue
            except Exception:
                continue

    if img_dir.is_dir():
        if scan_all_images:
            files = sorted(img_dir.rglob("*.xml"))
        else:
            files = sorted(img_dir.glob("inventoryimages*.xml"))
        for p in files:
            try:
                rel = p.relative_to(dst_root).as_posix()
                if scan_all_images or _is_inventory_xml(rel):
                    _parse_xml_bytes(rel, p.read_bytes())
            except Exception:
                continue

    return icons, xmls


def _scan_inventory_icon_traces(dst_root: Path, trace_ids: Set[str]) -> Dict[str, List[Dict[str, str]]]:
    trace_ids = {(_clean_id(x) or "") for x in (trace_ids or set())}
    trace_ids = {x for x in trace_ids if x}
    if not trace_ids:
        return {}

    out: Dict[str, Set[Tuple[str, str]]] = {tid: set() for tid in trace_ids}
    data_dir = dst_root / "data"
    img_dir = data_dir / "images"
    bundles_dir = data_dir / "databundles"

    def _parse_xml_bytes(label: str, data: bytes) -> None:
        try:
            root = ET.fromstring(data)
        except Exception:
            return
        for el in root.findall(".//Element"):
            name = el.attrib.get("name")
            if not name:
                continue
            base = name.strip().lower()
            if base.endswith(".tex"):
                base = base[:-4]
            if not _ID_RE.match(base):
                continue
            for tid in trace_ids:
                if tid in base:
                    out[tid].add((label, name))

    def _is_images_xml(path: str) -> bool:
        p = (path or "").replace("\\", "/").lstrip("/")
        return p.startswith("images/") and p.endswith(".xml")

    if bundles_dir.is_dir():
        for bundle_zip in sorted(bundles_dir.glob("*.zip")):
            if bundle_zip.name.lower() == "scripts.zip":
                continue
            try:
                with zipfile.ZipFile(bundle_zip, "r") as zf:
                    for name in zf.namelist():
                        nrm = (name or "").replace("\\", "/")
                        if not nrm.lower().endswith(".xml"):
                            continue
                        if not _is_images_xml(nrm):
                            continue
                        try:
                            _parse_xml_bytes(
                                (bundle_zip.relative_to(dst_root).as_posix() + ":" + name),
                                zf.read(name),
                            )
                        except Exception:
                            continue
            except Exception:
                continue

    if img_dir.is_dir():
        for p in sorted(img_dir.rglob("*.xml")):
            try:
                rel = p.relative_to(dst_root).as_posix()
                if _is_images_xml(rel):
                    _parse_xml_bytes(rel, p.read_bytes())
            except Exception:
                continue

    result: Dict[str, List[Dict[str, str]]] = {}
    for tid in sorted(out.keys()):
        rows = [{"atlas": a, "element": e} for a, e in sorted(out[tid])]
        result[tid] = rows
    return result


def _collect_known_item_ids(engine: Any, prefabs: Dict[str, Any]) -> Set[str]:
    ids: Set[str] = set()
    prefab_items = prefabs.get("items") or {}
    if isinstance(prefab_items, dict):
        for key in prefab_items.keys():
            cid = _clean_id(key)
            if cid:
                ids.add(cid)

    craft = getattr(engine, "recipes", None)
    craft_recipes = getattr(craft, "recipes", {}) if craft is not None else {}
    if isinstance(craft_recipes, dict):
        for name, rec in craft_recipes.items():
            cid = _clean_id(name)
            if cid:
                ids.add(cid)
            if not isinstance(rec, dict):
                continue
            prod = _clean_id(rec.get("product"))
            if prod:
                ids.add(prod)
            for ing in rec.get("ingredients", []) or []:
                if not isinstance(ing, dict):
                    continue
                item = _clean_id(ing.get("item"))
                if item:
                    ids.add(item)

    cooking_recipes = getattr(engine, "cooking_recipes", {}) or {}
    if isinstance(cooking_recipes, dict):
        for name, rec in cooking_recipes.items():
            cid = _clean_id(name)
            if cid:
                ids.add(cid)
            if not isinstance(rec, dict):
                continue
            for row in (rec.get("card_ingredients") or []):
                if not isinstance(row, (list, tuple)) or not row:
                    continue
                item = _clean_id(row[0])
                if item:
                    ids.add(item)

    cooking_ingredients = getattr(engine, "cooking_ingredients", {}) or {}
    if isinstance(cooking_ingredients, dict):
        for key in cooking_ingredients.keys():
            cid = _clean_id(key)
            if cid:
                ids.add(cid)

    return ids


def _scan_data_dir(
    dst_root: Path,
    *,
    include_files: bool = False,
    max_files: int = 0,
) -> Dict[str, Any]:
    data_dir = dst_root / "data"
    if not data_dir.is_dir():
        return {}

    top_dirs: Dict[str, Dict[str, int]] = defaultdict(lambda: {"files": 0, "bytes": 0})
    ext_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: {"files": 0, "bytes": 0})
    file_list: List[Dict[str, Any]] = []
    total_files = 0
    total_bytes = 0

    for root, _, files in os.walk(data_dir):
        for name in files:
            full = Path(root) / name
            try:
                st = full.stat()
            except Exception:
                continue
            size = int(st.st_size)
            rel = full.relative_to(data_dir).as_posix()
            total_files += 1
            total_bytes += size

            parts = rel.split("/")
            top = parts[0] if parts else "[root]"
            top_dirs[top]["files"] += 1
            top_dirs[top]["bytes"] += size

            ext = Path(name).suffix.lower() or "<no_ext>"
            ext_counts[ext]["files"] += 1
            ext_counts[ext]["bytes"] += size

            if include_files:
                file_list.append(
                    {
                        "path": rel,
                        "bytes": size,
                        "ext": ext,
                        "dir": top,
                    }
                )

    top_dirs_list = [{"dir": k, "files": v["files"], "bytes": v["bytes"]} for k, v in top_dirs.items()]
    top_dirs_list.sort(key=lambda x: x["files"], reverse=True)

    ext_list = [{"ext": k, "files": v["files"], "bytes": v["bytes"]} for k, v in ext_counts.items()]
    ext_list.sort(key=lambda x: x["files"], reverse=True)

    if include_files:
        file_list.sort(key=lambda x: x["path"])
        if max_files and len(file_list) > max_files:
            file_list = file_list[:max_files]

    return {
        "total_files": total_files,
        "total_bytes": total_bytes,
        "top_dirs": top_dirs_list,
        "top_exts": ext_list,
        "files": file_list if include_files else None,
    }


def _scan_bundles(
    dst_root: Path,
    *,
    include_entries: bool = False,
    max_entries: int = 0,
) -> List[Dict[str, Any]]:
    bundles_dir = dst_root / "data" / "databundles"
    if not bundles_dir.is_dir():
        return []

    out: List[Dict[str, Any]] = []
    for zp in sorted(bundles_dir.glob("*.zip")):
        try:
            with zipfile.ZipFile(zp, "r") as zf:
                infos = zf.infolist()
                total_bytes = sum(i.file_size for i in infos)
                entries = None
                if include_entries:
                    entries = [{"path": i.filename, "bytes": int(i.file_size)} for i in infos]
                    if max_entries and len(entries) > max_entries:
                        entries = entries[:max_entries]
                out.append({"file": zp.name, "entries": len(infos), "bytes": total_bytes, "files": entries})
        except Exception:
            out.append({"file": zp.name, "entries": -1, "bytes": -1, "files": None})
    return out


def build_resource_index(
    *,
    engine: Any,
    dst_root: Path,
    include_data_files: bool = False,
    max_data_files: int = 0,
    include_bundle_files: bool = False,
    max_bundle_files: int = 0,
    icon_trace_ids: Optional[Set[str]] = None,
) -> Dict[str, Any]:
    files = list(getattr(engine, "file_list", []) or [])
    scripts = _scan_scripts(files)
    prefabs = _scan_prefabs(engine)
    known_ids = _collect_known_item_ids(engine, prefabs)
    icons, icon_sources = _scan_inventory_icons(dst_root, id_filter=known_ids if known_ids else None)
    icon_traces = _scan_inventory_icon_traces(dst_root, set(icon_trace_ids or []))
    data_scan = _scan_data_dir(dst_root, include_files=include_data_files, max_files=max_data_files)
    bundle_scan = _scan_bundles(dst_root, include_entries=include_bundle_files, max_entries=max_bundle_files)

    meta = build_meta(
        schema=SCHEMA_VERSION,
        tool="build_resource_index",
        sources={
            "dst_root": str(dst_root),
            "scripts_zip": getattr(getattr(engine, "source", None), "filename", None),
            "scripts_dir": getattr(engine, "source", None) if getattr(engine, "mode", "") == "folder" else None,
        },
        extra={
            "engine_mode": getattr(engine, "mode", ""),
            "scripts_file_count": len(files),
            "dst_root": str(dst_root),
            "scripts_zip": getattr(getattr(engine, "source", None), "filename", None),
            "scripts_dir": getattr(engine, "source", None) if getattr(engine, "mode", "") == "folder" else None,
        },
    )

    if data_scan:
        meta["data_file_count"] = data_scan.get("total_files")
        meta["data_total_bytes"] = data_scan.get("total_bytes")

    return {
        "schema_version": SCHEMA_VERSION,
        "meta": meta,
        "scripts": scripts,
        "prefabs": prefabs,
        "assets": {
            "inventory_icons": sorted(icons),
            "inventory_atlases": icon_sources,
            "inventory_icon_traces": icon_traces,
        },
        "data": data_scan,
        "bundles": bundle_scan,
    }


def render_resource_index_summary(index: Dict[str, Any]) -> str:
    meta = index.get("meta") or {}
    scripts = index.get("scripts") or {}
    prefabs = index.get("prefabs") or {}
    assets = index.get("assets") or {}
    data = index.get("data") or {}
    bundles = index.get("bundles") or []

    lines: List[str] = []
    lines.append("# Wagstaff Resource Index Summary")
    lines.append("")
    lines.append("## Meta")
    lines.append("```yaml")
    lines.append(f"generated: {meta.get('generated')}")
    lines.append(f"dst_root: {meta.get('dst_root')}")
    lines.append(f"engine_mode: {meta.get('engine_mode')}")
    if meta.get("scripts_zip"):
        lines.append(f"scripts_zip: {meta.get('scripts_zip')}")
    if meta.get("scripts_dir"):
        lines.append(f"scripts_dir: {meta.get('scripts_dir')}")
    lines.append(f"scripts_file_count: {meta.get('scripts_file_count')}")
    if meta.get("data_file_count") is not None:
        lines.append(f"data_file_count: {meta.get('data_file_count')}")
    if meta.get("data_total_bytes") is not None:
        lines.append(f"data_total_bytes: {meta.get('data_total_bytes')}")
    lines.append("```")

    lines.append("")
    lines.append("## Scripts")
    lines.append("")
    lines.append("```yaml")
    lines.append(f"total_files: {scripts.get('total_files')}")
    lines.append(f"lua_files: {scripts.get('lua_files')}")
    lines.append("```")

    lines.append("")
    lines.append("### Script Categories")
    lines.append("")
    lines.append("| Category | Count |")
    lines.append("|---|---:|")
    for k, v in sorted((scripts.get("categories") or {}).items(), key=lambda x: x[0]):
        lines.append(f"| {k} | {v} |")

    lines.append("")
    lines.append("## Prefabs")
    lines.append("")
    lines.append("```yaml")
    lines.append(f"prefab_files: {prefabs.get('total_files')}")
    lines.append(f"prefabs_total: {prefabs.get('total_prefabs')}")
    lines.append(f"prefabs_skipped: {prefabs.get('prefabs_skipped')}")
    lines.append("```")

    lines.append("")
    lines.append("## Assets")
    lines.append("")
    lines.append("```yaml")
    lines.append(f"inventory_icons: {len(assets.get('inventory_icons') or [])}")
    lines.append(f"inventory_atlases: {len(assets.get('inventory_atlases') or [])}")
    traces = assets.get("inventory_icon_traces") or {}
    if isinstance(traces, dict):
        lines.append(f"inventory_icon_traces: {len(traces)}")
    lines.append("```")

    if data:
        lines.append("")
        lines.append("## Data Summary")
        lines.append("")
        lines.append("```yaml")
        lines.append(f"total_files: {data.get('total_files')}")
        lines.append(f"total_bytes: {data.get('total_bytes')}")
        lines.append("```")

    if bundles:
        lines.append("")
        lines.append("## Bundles")
        lines.append("")
        lines.append("| Bundle | Entries | Bytes |")
        lines.append("|---|---:|---:|")
        for row in bundles:
            lines.append(f"| {row.get('file')} | {row.get('entries')} | {row.get('bytes')} |")

    return "\n".join(lines) + "\n"
```

### File: core/indexers/shared.py
- mode: full
- size_bytes: 2067
- sha256_12: 20b8e9e42b9c

```py
# -*- coding: utf-8 -*-
"""Shared helpers for indexers and scans."""

from __future__ import annotations

import hashlib
import re
from pathlib import Path
from typing import Any, Dict, Optional

from core.lua import find_matching, parse_lua_table


_ID_RE = re.compile(r"^[a-z0-9_]+$")


def _sha256_12_file(path: Path, chunk_size: int = 1024 * 1024) -> Optional[str]:
    try:
        h = hashlib.sha256()
        with path.open("rb") as f:
            while True:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                h.update(chunk)
        return h.hexdigest()[:12]
    except Exception:
        return None


def _is_simple_id(s: str) -> bool:
    return bool(s) and bool(_ID_RE.match(s))


def _extract_strings_names(engine: Any) -> Dict[str, str]:
    """Extract STRINGS.NAMES mapping from scripts/strings.lua."""
    src = (
        engine.read_file("scripts/strings.lua")
        or engine.read_file("strings.lua")
        or ""
    )
    if not src:
        return {}

    m = re.search(r"\bSTRINGS\s*=\s*\{", src)
    if not m:
        return {}
    strings_open = src.find("{", m.end() - 1)
    if strings_open < 0:
        return {}
    strings_close = find_matching(src, strings_open, "{", "}")
    if strings_close is None:
        return {}

    block = src[strings_open: strings_close + 1]
    m2 = re.search(r"(?<![A-Za-z0-9_])NAMES\s*=\s*\{", block)
    if not m2:
        return {}

    names_open = strings_open + m2.end() - 1
    names_close = find_matching(src, names_open, "{", "}")
    if names_close is None:
        return {}

    inner = src[names_open + 1 : names_close]
    try:
        names_tbl = parse_lua_table(inner)
    except Exception:
        return {}

    out: Dict[str, str] = {}
    for k, v in (names_tbl.map or {}).items():
        if not isinstance(k, str):
            continue
        if not isinstance(v, str):
            continue
        kid = k.strip().lower()
        if not _is_simple_id(kid):
            continue
        out[kid] = v
    return out
```

### File: core/lua/__init__.py
- mode: full
- size_bytes: 1089
- sha256_12: e0649c1b83f7

```py
# -*- coding: utf-8 -*-
"""Lua parsing primitives used across the core."""

from core.lua.call_extractor import LuaCall, LuaCallExtractor
from core.lua.expr import LuaRaw, LuaTableValue, lua_to_python, parse_lua_expr, parse_lua_string, parse_lua_table, _NUM_RE
from core.lua.match import _find_matching, find_matching
from core.lua.scan import (
    _is_ident_char,
    _is_ident_start,
    _long_bracket_level,
    _skip_comment,
    _skip_long_bracket,
    _skip_short_string,
    _skip_string_or_long_string,
    strip_lua_comments,
)
from core.lua.split import _split_top_level, split_top_level

__all__ = [
    "LuaCall",
    "LuaCallExtractor",
    "LuaRaw",
    "LuaTableValue",
    "lua_to_python",
    "parse_lua_expr",
    "parse_lua_string",
    "parse_lua_table",
    "find_matching",
    "split_top_level",
    "strip_lua_comments",
    "_NUM_RE",
    "_find_matching",
    "_split_top_level",
    "_is_ident_start",
    "_is_ident_char",
    "_long_bracket_level",
    "_skip_long_bracket",
    "_skip_short_string",
    "_skip_comment",
    "_skip_string_or_long_string",
]
```

### File: core/lua/call_extractor.py
- mode: full
- size_bytes: 5394
- sha256_12: 5588fec4b7be

```py
# -*- coding: utf-8 -*-
"""Lua function call extraction helpers."""

from __future__ import annotations

import bisect
import re
from dataclasses import dataclass
from typing import Iterator, List, Optional, Sequence, Tuple, Union

from core.lua.match import _find_matching
from core.lua.scan import (
    _is_ident_char,
    _is_ident_start,
    _skip_comment,
    _skip_string_or_long_string,
)
from core.lua.split import _split_top_level

__all__ = [
    "LuaCall",
    "LuaCallExtractor",
]


_LUA_KEYWORDS = {
    "and",
    "break",
    "do",
    "else",
    "elseif",
    "end",
    "false",
    "for",
    "function",
    "goto",
    "if",
    "in",
    "local",
    "nil",
    "not",
    "or",
    "repeat",
    "return",
    "then",
    "true",
    "until",
    "while",
}


@dataclass(frozen=True)
class LuaCall:
    name: str
    full_name: str
    start: int
    end: int
    open_paren: int
    close_paren: int
    args: str
    arg_list: List[str]
    line: int
    col: int


class LuaCallExtractor:
    """
    Extract Lua function calls with balanced parentheses, skipping comments/strings/long strings.

    Supports:
    - NAME(...)
    - obj.NAME(...)
    - obj:NAME(...)
    """

    def __init__(self, content: str):
        self.content = content or ""
        self._line_starts: Optional[List[int]] = None

    def iter_calls(
        self,
        names: Union[str, Sequence[str]],
        *,
        include_member_calls: bool = True,
        match_full_name: bool = False,
    ) -> Iterator[LuaCall]:
        if isinstance(names, str):
            targets = {names}
        else:
            targets = set(names)

        text = self.content
        n = len(text)
        i = 0

        while i < n:
            if text.startswith("--", i):
                i = _skip_comment(text, i)
                continue

            nxt = _skip_string_or_long_string(text, i)
            if nxt is not None:
                i = nxt
                continue

            ch = text[i]
            if _is_ident_start(ch):
                # first ident
                j = i + 1
                while j < n and _is_ident_char(text[j]):
                    j += 1
                first = text[i:j]
                if first in _LUA_KEYWORDS:
                    i = j
                    continue

                full = first
                last = first
                k = j

                if include_member_calls:
                    # ".ident" / ":ident" chain
                    while True:
                        kk = k
                        while kk < n and text[kk].isspace():
                            kk += 1
                        if kk < n and text[kk] in ".:":
                            sep = text[kk]
                            kk += 1
                            while kk < n and text[kk].isspace():
                                kk += 1
                            if kk < n and _is_ident_start(text[kk]):
                                jj = kk + 1
                                while jj < n and _is_ident_char(text[jj]):
                                    jj += 1
                                seg = text[kk:jj]
                                full = full + sep + seg
                                last = seg
                                k = jj
                                continue
                        break

                hit = (full in targets) if match_full_name else (last in targets)
                if hit:
                    kk = k
                    while kk < n and text[kk].isspace():
                        kk += 1
                    if kk < n and text[kk] == "(":
                        close = _find_matching(text, kk, "(", ")")
                        if close is not None:
                            args = text[kk + 1 : close]
                            arg_list = self.split_args(args)
                            line, col = self._line_col(i)
                            yield LuaCall(
                                name=last,
                                full_name=full,
                                start=i,
                                end=close + 1,
                                open_paren=kk,
                                close_paren=close,
                                args=args,
                                arg_list=arg_list,
                                line=line,
                                col=col,
                            )
                            i = close + 1
                            continue

                i = k
                continue

            i += 1

    def extract_calls(self, names: Union[str, Sequence[str]], **kwargs: object) -> List[LuaCall]:
        return list(self.iter_calls(names, **kwargs))

    def split_args(self, args: str) -> List[str]:
        return [p for p in _split_top_level(args, ",") if p]

    def _ensure_line_starts(self) -> None:
        if self._line_starts is not None:
            return
        self._line_starts = [0]
        for m in re.finditer("\n", self.content):
            self._line_starts.append(m.end())

    def _line_col(self, pos: int) -> Tuple[int, int]:
        self._ensure_line_starts()
        assert self._line_starts is not None
        idx = bisect.bisect_right(self._line_starts, pos) - 1
        line_start = self._line_starts[idx]
        return idx + 1, (pos - line_start) + 1
```

### File: core/lua/expr.py
- mode: full
- size_bytes: 4804
- sha256_12: 87b0d66a5bd5

```py
# -*- coding: utf-8 -*-
"""Lua expression parsing helpers."""

from __future__ import annotations

from dataclasses import dataclass
import re
from typing import Any, Dict, List, Optional

from core.lua.match import _find_matching
from core.lua.scan import _long_bracket_level, strip_lua_comments
from core.lua.split import _split_top_level

__all__ = [
    "LuaRaw",
    "LuaTableValue",
    "lua_to_python",
    "parse_lua_string",
    "parse_lua_expr",
    "parse_lua_table",
    "_NUM_RE",
]


@dataclass
class LuaRaw:
    """Opaque expression (kept as raw text)."""

    text: str


@dataclass
class LuaTableValue:
    """Lua table constructor parsed into (array, map)."""

    array: List[Any]
    map: Dict[Any, Any]


def lua_to_python(v: Any) -> Any:
    """Recursively convert LuaTableValue/LuaRaw into plain Python types."""
    if isinstance(v, LuaRaw):
        return v.text
    if isinstance(v, LuaTableValue):
        arr = [lua_to_python(x) for x in v.array]
        mp = {lua_to_python(k): lua_to_python(val) for k, val in v.map.items()}
        if mp and arr:
            return {"__array__": arr, **mp}
        if mp:
            return mp
        return arr
    if isinstance(v, list):
        return [lua_to_python(x) for x in v]
    if isinstance(v, dict):
        return {lua_to_python(k): lua_to_python(val) for k, val in v.items()}
    return v


_NUM_RE = re.compile(r"^[+-]?(?:\d+\.\d*|\d*\.\d+|\d+)(?:[eE][+-]?\d+)?$")


def _parse_lua_string(expr: str) -> Optional[str]:
    expr = (expr or "").strip()
    if len(expr) >= 2 and expr[0] == expr[-1] and expr[0] in ("'", '"'):
        body = expr[1:-1]
        body = body.replace(r"\\", "\\").replace(r"\'", "'").replace(r"\"", '"')
        return body
    if expr.startswith("["):
        level = _long_bracket_level(expr, 0)
        if level is not None:
            opener_len = 2 + level
            close_pat = "]" + ("=" * level) + "]"
            end = expr.find(close_pat, opener_len)
            if end != -1:
                return expr[opener_len:end]
    return None


def parse_lua_string(expr: str) -> Optional[str]:
    """Public wrapper: parse a Lua string literal (short or long bracket)."""
    return _parse_lua_string(expr)


def parse_lua_expr(expr: str) -> Any:
    """
    Parse a subset of Lua expressions into Python types:
    - string/long-string -> str
    - number -> int/float
    - true/false/nil -> bool/None
    - table constructor -> LuaTableValue
    - function (...) ... end -> LuaRaw("<function>")
    - identifier/dotted path / everything else -> LuaRaw(expr)
    """
    expr = (expr or "").strip()
    if not expr:
        return LuaRaw("")

    if expr.startswith("function"):
        sig_end = expr.find(")")
        if sig_end != -1 and sig_end < 160:
            return LuaRaw(expr[: sig_end + 1] + " ... end")
        return LuaRaw("<function>")

    if expr == "nil":
        return None
    if expr in ("true", "false"):
        return expr == "true"

    s = _parse_lua_string(expr)
    if s is not None:
        return s

    if _NUM_RE.match(expr):
        try:
            f = float(expr)
            return int(f) if f.is_integer() else f
        except Exception:
            return LuaRaw(expr)

    if expr.startswith("{"):
        close = _find_matching(expr, 0, "{", "}")
        if close is None:
            return LuaRaw(expr)
        inner = expr[1:close]
        return parse_lua_table(inner)

    if re.match(r"^[A-Za-z_][A-Za-z0-9_\.]*$", expr):
        return LuaRaw(expr)

    return LuaRaw(expr)


def parse_lua_table(inner: str) -> LuaTableValue:
    """
    Parse the inside of { ... } (without outer braces).
    Returns LuaTableValue(array, map).
    """
    inner = strip_lua_comments(inner)

    array: List[Any] = []
    mp: Dict[Any, Any] = {}

    for item in _split_top_level(inner, ","):
        item = (item or "").strip()
        if not item:
            continue

        # key = value
        m = re.match(r"^([A-Za-z_][A-Za-z0-9_]*)\s*=\s*(.+)$", item, flags=re.DOTALL)
        if m:
            key = m.group(1)
            mp[key] = parse_lua_expr(m.group(2))
            continue

        # ["key"] = value (also long bracket keys)
        m = re.match(r'^\[\s*([\'"].*?[\'"]|\[=*\[.*?\]=*\])\s*\]\s*=\s*(.+)$', item, flags=re.DOTALL)
        if m:
            key_raw = m.group(1)
            key = _parse_lua_string(key_raw) or LuaRaw(key_raw)
            mp[key] = parse_lua_expr(m.group(2))
            continue

        # [expr] = value
        m = re.match(r"^\[\s*(.+?)\s*\]\s*=\s*(.+)$", item, flags=re.DOTALL)
        if m:
            mp[LuaRaw(m.group(1).strip())] = parse_lua_expr(m.group(2))
            continue

        # array entry
        array.append(parse_lua_expr(item))

    return LuaTableValue(array=array, map=mp)
```

### File: core/lua/match.py
- mode: full
- size_bytes: 1691
- sha256_12: 786839bb2405

```py
# -*- coding: utf-8 -*-
"""Bracket matching helpers for Lua."""

from __future__ import annotations

from typing import List, Optional

from core.lua.scan import (
    _long_bracket_level,
    _skip_comment,
    _skip_long_bracket,
    _skip_string_or_long_string,
)

__all__ = [
    "_find_matching",
    "find_matching",
]


def _find_matching(text: str, open_idx: int, open_ch: str, close_ch: str) -> Optional[int]:
    """Find the matching closing bracket for open_ch at open_idx. Skip strings/comments/long brackets."""
    n = len(text)
    if open_idx >= n or text[open_idx] != open_ch:
        return None

    stack: List[str] = [open_ch]
    i = open_idx + 1
    while i < n and stack:
        if text.startswith("--", i):
            i = _skip_comment(text, i)
            continue
        nxt = _skip_string_or_long_string(text, i)
        if nxt is not None:
            i = nxt
            continue

        ch = text[i]
        if ch in "({[":
            if ch == "[":
                level = _long_bracket_level(text, i)
                if level is not None:
                    i = _skip_long_bracket(text, i, level)
                    continue
            stack.append(ch)
            i += 1
            continue

        if ch in ")}]":
            want = {")": "(", "}": "{", "]": "["}[ch]
            if stack and stack[-1] == want:
                stack.pop()
            i += 1
            continue

        i += 1

    if stack:
        return None
    return i - 1


def find_matching(text: str, open_idx: int, open_ch: str, close_ch: str) -> Optional[int]:
    """Public wrapper for _find_matching."""
    return _find_matching(text, open_idx, open_ch, close_ch)
```

### File: core/lua/scan.py
- mode: full
- size_bytes: 3332
- sha256_12: 19edc5ad75ae

```py
# -*- coding: utf-8 -*-
"""Low-level Lua scanning helpers."""

from __future__ import annotations

from typing import List, Optional

__all__ = [
    "_is_ident_start",
    "_is_ident_char",
    "_long_bracket_level",
    "_skip_long_bracket",
    "_skip_short_string",
    "_skip_comment",
    "_skip_string_or_long_string",
    "strip_lua_comments",
]


def _is_ident_start(ch: str) -> bool:
    return ch == "_" or ("A" <= ch <= "Z") or ("a" <= ch <= "z")


def _is_ident_char(ch: str) -> bool:
    return _is_ident_start(ch) or ("0" <= ch <= "9")


def _long_bracket_level(text: str, i: int) -> Optional[int]:
    """
    If text[i:] starts a Lua long-bracket opener: [=*[ , return '=' count; else None.
    Examples: [[ -> 0, [=[ -> 1, [==[ -> 2
    """
    n = len(text)
    if i >= n or text[i] != "[":
        return None
    j = i + 1
    while j < n and text[j] == "=":
        j += 1
    if j < n and text[j] == "[":
        return j - i - 1
    return None


def _skip_long_bracket(text: str, i: int, level: int) -> int:
    """Skip Lua long-bracket string/comment starting at i. Return next index."""
    n = len(text)
    opener_len = 2 + level
    start = i + opener_len
    close_pat = "]" + ("=" * level) + "]"
    end = text.find(close_pat, start)
    if end == -1:
        return n
    return end + len(close_pat)


def _skip_short_string(text: str, i: int, quote: str) -> int:
    """Skip '...' or "...", supporting backslash escapes. Return next index."""
    n = len(text)
    i += 1
    while i < n:
        ch = text[i]
        if ch == "\\":
            i += 2
            continue
        if ch == quote:
            return i + 1
        i += 1
    return n


def _skip_comment(text: str, i: int) -> int:
    """i points at '-' and text[i:i+2]=='--'. Skip a line or block comment. Return next index."""
    n = len(text)
    if not text.startswith("--", i):
        return i

    # Block comment: --[=*[ ... ]=*]
    if i + 2 < n and text[i + 2] == "[":
        level = _long_bracket_level(text, i + 2)
        if level is not None:
            return _skip_long_bracket(text, i + 2, level)

    # Line comment
    nl = text.find("\n", i + 2)
    return n if nl == -1 else nl + 1


def _skip_string_or_long_string(text: str, i: int) -> Optional[int]:
    """If position i starts a string/long-string, return next index; else None."""
    if i >= len(text):
        return None
    ch = text[i]
    if ch in ("'", '"'):
        return _skip_short_string(text, i, ch)
    if ch == "[":
        level = _long_bracket_level(text, i)
        if level is not None:
            return _skip_long_bracket(text, i, level)
    return None


def strip_lua_comments(text: str) -> str:
    """
    Remove Lua comments while preserving line breaks (keeps line numbers stable).
    Strings/long-strings are preserved.
    """
    if not text:
        return ""
    n = len(text)
    out: List[str] = []
    i = 0
    while i < n:
        if text.startswith("--", i):
            j = _skip_comment(text, i)
            out.append("\n" * text[i:j].count("\n"))
            i = j
            continue
        nxt = _skip_string_or_long_string(text, i)
        if nxt is not None:
            out.append(text[i:nxt])
            i = nxt
            continue
        out.append(text[i])
        i += 1
    return "".join(out)
```

### File: core/lua/split.py
- mode: full
- size_bytes: 3617
- sha256_12: bcd71d71a3c6

```py
# -*- coding: utf-8 -*-
"""Balanced splitting helpers for Lua."""

from __future__ import annotations

from typing import List, Tuple

from core.lua.scan import (
    _is_ident_char,
    _is_ident_start,
    _long_bracket_level,
    _skip_comment,
    _skip_long_bracket,
    _skip_string_or_long_string,
)

__all__ = [
    "_split_top_level",
    "split_top_level",
]


def _split_top_level(text: str, sep: str = ",") -> List[str]:
    """
    Split by sep at top level.

    Top level means:
    - not in (), {}, []
    - not in strings/comments/long-strings
    - not inside Lua blocks (function/if/for/while/repeat/do ... end/until)

    This is critical for safely splitting function call arguments in DST scripts.
    """
    if not text:
        return []

    n = len(text)
    parts: List[str] = []
    start = 0
    i = 0

    bracket_stack: List[str] = []
    block_stack: List[Tuple[str, bool]] = []  # (kind, awaiting_do)

    def _push_block(kind: str) -> None:
        block_stack.append((kind, False))

    def _push_loop(kind: str) -> None:
        block_stack.append((kind, True))

    def _on_do() -> None:
        if block_stack and block_stack[-1][0] in ("for", "while") and block_stack[-1][1]:
            kind, _ = block_stack[-1]
            block_stack[-1] = (kind, False)
        else:
            _push_block("do")

    def _on_end() -> None:
        if block_stack:
            block_stack.pop()

    def _on_until() -> None:
        # close the nearest repeat
        for idx in range(len(block_stack) - 1, -1, -1):
            if block_stack[idx][0] == "repeat":
                del block_stack[idx:]
                return

    while i < n:
        if text.startswith("--", i):
            i = _skip_comment(text, i)
            continue

        nxt = _skip_string_or_long_string(text, i)
        if nxt is not None:
            i = nxt
            continue

        ch = text[i]

        if ch in "({[":
            if ch == "[":
                level = _long_bracket_level(text, i)
                if level is not None:
                    i = _skip_long_bracket(text, i, level)
                    continue
            bracket_stack.append(ch)
            i += 1
            continue

        if ch in ")}]":
            want = {")": "(", "}": "{", "]": "["}[ch]
            if bracket_stack and bracket_stack[-1] == want:
                bracket_stack.pop()
            i += 1
            continue

        # block keywords
        if _is_ident_start(ch):
            j = i + 1
            while j < n and _is_ident_char(text[j]):
                j += 1
            word = text[i:j]
            if word == "function":
                _push_block("function")
            elif word == "if":
                _push_block("if")
            elif word == "for":
                _push_loop("for")
            elif word == "while":
                _push_loop("while")
            elif word == "repeat":
                _push_block("repeat")
            elif word == "do":
                _on_do()
            elif word == "end":
                _on_end()
            elif word == "until":
                _on_until()
            i = j
            continue

        if ch == sep and not bracket_stack and not block_stack:
            parts.append(text[start:i].strip())
            start = i + 1
            i += 1
            continue

        i += 1

    tail = text[start:].strip()
    if tail:
        parts.append(tail)
    return parts


def split_top_level(text: str, sep: str = ",") -> List[str]:
    """Public wrapper for _split_top_level."""
    return _split_top_level(text, sep)
```

### File: core/parsers/__init__.py
- mode: full
- size_bytes: 812
- sha256_12: 89fd1dc20368

```py
# -*- coding: utf-8 -*-
"""Domain parsers for DST scripts."""

from core.parsers.base import BaseParser
from core.parsers.component import ComponentParser
from core.parsers.cooking import CookingIngredientAnalyzer, CookingRecipeAnalyzer, parse_oceanfish_ingredients
from core.parsers.lua_analyzer import LuaAnalyzer
from core.parsers.loot import LootParser
from core.parsers.prefab import PrefabParser
from core.parsers.strings import StringParser
from core.parsers.tuning import TuningResolver
from core.parsers.widget import WidgetParser

__all__ = [
    "BaseParser",
    "ComponentParser",
    "CookingIngredientAnalyzer",
    "CookingRecipeAnalyzer",
    "LuaAnalyzer",
    "LootParser",
    "PrefabParser",
    "StringParser",
    "TuningResolver",
    "WidgetParser",
    "parse_oceanfish_ingredients",
]
```

### File: core/parsers/base.py
- mode: full
- size_bytes: 540
- sha256_12: 68e9935d3a97

```py
# -*- coding: utf-8 -*-
"""Base classes for domain parsers."""

from __future__ import annotations

import re
from typing import List, Optional

from core.lua import strip_lua_comments

__all__ = ["BaseParser"]


class BaseParser:
    def __init__(self, content: str, path: Optional[str] = None):
        self.path = path
        self.content = content or ""
        self.clean = strip_lua_comments(self.content)

    def _extract_requires(self) -> List[str]:
        return re.findall(r'require\s*\(?\s*["\'](.*?)["\']\s*\)?', self.clean)
```

### File: core/parsers/component.py
- mode: full
- size_bytes: 3047
- sha256_12: 880faae7031f

```py
# -*- coding: utf-8 -*-
"""Component parser (scripts/components/*.lua)."""

from __future__ import annotations

import os
import re
from typing import Any, Dict, List, Optional, Set

from core.lua import LuaCallExtractor, parse_lua_string
from core.parsers.base import BaseParser

__all__ = ["ComponentParser"]


def _basename_id(path: Optional[str]) -> str:
    if not path:
        return ""
    base = os.path.basename(path)
    if base.endswith(".lua"):
        base = base[:-4]
    return base.strip().lower()


def _guess_class_name(component_id: str) -> str:
    if not component_id:
        return ""
    parts = [p for p in component_id.split("_") if p]
    if parts:
        return "".join([p[:1].upper() + p[1:] for p in parts])
    return component_id[:1].upper() + component_id[1:]


class ComponentParser(BaseParser):
    """Parse component API surface (methods/fields/events)."""

    def parse(self) -> Dict[str, Any]:
        comp_id = _basename_id(self.path)
        data: Dict[str, Any] = {
            "type": "component",
            "id": comp_id,
            "class_name": None,
            "aliases": [],
            "methods": [],
            "fields": [],
            "events": [],
            "requires": self._extract_requires(),
            "path": self.path,
        }

        aliases: Set[str] = set()
        for m in re.finditer(r"(?m)^\s*(?:local\s+)?([A-Za-z0-9_]+)\s*=\s*Class\b", self.clean):
            aliases.add(m.group(1))

        # prefer an explicit return alias as class name
        class_name = None
        mret = re.search(r"\breturn\s+([A-Za-z0-9_]+)\b", self.clean)
        if mret:
            cand = mret.group(1)
            if cand in aliases:
                class_name = cand

        if class_name is None and aliases:
            class_name = sorted(aliases)[0]

        if not aliases and comp_id:
            guess = _guess_class_name(comp_id)
            aliases.add(guess)
            class_name = guess

        # methods: function Alias:Method(...) / Alias.Method(...)
        methods: Set[str] = set()
        for m in re.finditer(r"\bfunction\s+([A-Za-z0-9_]+)[:\.]([A-Za-z0-9_]+)\s*\(", self.clean):
            obj = m.group(1)
            name = m.group(2)
            if aliases and obj not in aliases:
                continue
            methods.add(name)

        # fields: self.field = ...
        fields = set(re.findall(r"\bself\.([A-Za-z0-9_]+)\s*=", self.clean))

        # events: ListenForEvent("event", ...)
        events: Set[str] = set()
        extractor = LuaCallExtractor(self.content)
        for call in extractor.iter_calls("ListenForEvent"):
            if not call.arg_list:
                continue
            ev = parse_lua_string(call.arg_list[0])
            if isinstance(ev, str) and ev:
                events.add(ev)

        data["class_name"] = class_name
        data["aliases"] = sorted(aliases)
        data["methods"] = sorted(methods)
        data["fields"] = sorted(fields)
        data["events"] = sorted(events)
        return data
```

### File: core/parsers/cooking.py
- mode: full
- size_bytes: 27295
- sha256_12: 33386ba348fc

```py
# -*- coding: utf-8 -*-
"""Cooking recipe and ingredient analyzers."""

from __future__ import annotations

import re
from typing import Any, Dict, Iterable, List, Optional, Set, Tuple

from core.lua import (
    LuaCallExtractor,
    LuaRaw,
    LuaTableValue,
    _NUM_RE,
    _find_matching,
    _is_ident_char,
    _is_ident_start,
    _long_bracket_level,
    _skip_comment,
    _skip_long_bracket,
    _skip_string_or_long_string,
    lua_to_python,
    parse_lua_expr,
    parse_lua_table,
    parse_lua_string,
    strip_lua_comments,
)

__all__ = [
    "CookingRecipeAnalyzer",
    "CookingIngredientAnalyzer",
    "parse_oceanfish_ingredients",
]


def _iter_named_table_blocks(parent_table_body: str) -> Iterable[Tuple[str, str]]:
    """
    Iterate top-level `name = { ... }` blocks inside a parent table body (WITHOUT outer braces).

    This is stricter than a regex: it skips strings/comments and respects nested braces.
    """
    text = parent_table_body or ""
    n = len(text)
    i = 0
    depth = 0

    while i < n:
        if text.startswith("--", i):
            i = _skip_comment(text, i)
            continue

        nxt = _skip_string_or_long_string(text, i)
        if nxt is not None:
            i = nxt
            continue

        ch = text[i]

        if ch == "{":
            depth += 1
            i += 1
            continue
        if ch == "}":
            depth = max(0, depth - 1)
            i += 1
            continue

        if depth == 0:
            # skip whitespace/commas between entries
            if ch.isspace() or ch == ",":
                i += 1
                continue

            if _is_ident_start(ch):
                j = i + 1
                while j < n and _is_ident_char(text[j]):
                    j += 1
                name = text[i:j]

                k = j
                while k < n and text[k].isspace():
                    k += 1
                if k < n and text[k] == "=":
                    k += 1
                    while k < n and text[k].isspace():
                        k += 1
                    if k < n and text[k] == "{":
                        open_idx = k
                        close_idx = _find_matching(text, open_idx, "{", "}")
                        if close_idx is None:
                            i = j
                            continue
                        body = text[open_idx + 1 : close_idx]
                        yield name, body
                        i = close_idx + 1
                        continue

                i = j
                continue

        i += 1


def _find_lua_function_end(text: str, fn_start: int) -> Optional[int]:
    """Return index right after the `end` that closes the function started at fn_start."""
    if fn_start < 0 or fn_start >= len(text):
        return None
    if not text.startswith("function", fn_start):
        return None

    n = len(text)
    i = fn_start

    bracket_stack: List[str] = []
    block_stack: List[Tuple[str, bool]] = []  # (kind, awaiting_do)

    def _push_block(kind: str) -> None:
        block_stack.append((kind, False))

    def _push_loop(kind: str) -> None:
        block_stack.append((kind, True))

    def _on_do() -> None:
        if block_stack and block_stack[-1][0] in ("for", "while") and block_stack[-1][1]:
            kind, _ = block_stack[-1]
            block_stack[-1] = (kind, False)
        else:
            _push_block("do")

    def _on_end() -> None:
        if block_stack:
            block_stack.pop()

    def _on_until() -> None:
        # close the nearest repeat
        for idx in range(len(block_stack) - 1, -1, -1):
            if block_stack[idx][0] == "repeat":
                del block_stack[idx:]
                return

    # consume the initial 'function'
    _push_block("function")
    i += len("function")

    while i < n and block_stack:
        if text.startswith("--", i):
            i = _skip_comment(text, i)
            continue
        nxt = _skip_string_or_long_string(text, i)
        if nxt is not None:
            i = nxt
            continue

        ch = text[i]

        # bracket stack (keep keywords inside parentheses from confusing us less; still scan keywords)
        if ch in "({[":
            if ch == "[":
                level = _long_bracket_level(text, i)
                if level is not None:
                    i = _skip_long_bracket(text, i, level)
                    continue
            bracket_stack.append(ch)
            i += 1
            continue

        if ch in ")}]":
            want = {")": "(", "}": "{", "]": "["}[ch]
            if bracket_stack and bracket_stack[-1] == want:
                bracket_stack.pop()
            i += 1
            continue

        if _is_ident_start(ch):
            j = i + 1
            while j < n and _is_ident_char(text[j]):
                j += 1
            word = text[i:j]

            if word == "function":
                _push_block("function")
            elif word == "if":
                _push_block("if")
            elif word == "for":
                _push_loop("for")
            elif word == "while":
                _push_loop("while")
            elif word == "repeat":
                _push_block("repeat")
            elif word == "do":
                _on_do()
            elif word == "end":
                _on_end()
                if not block_stack:
                    return j
            elif word == "until":
                _on_until()
                if not block_stack:
                    return j

            i = j
            continue

        i += 1

    return None


def _extract_test_return_expr(entry_body: str) -> Optional[str]:
    """Extract the boolean return expression from `test = function(...) return <expr> end`."""
    if not entry_body:
        return None

    m = re.search(r"\btest\s*=\s*function\b", entry_body)
    if not m:
        return None

    fn_start = m.end() - len("function")
    fn_end = _find_lua_function_end(entry_body, fn_start)
    if fn_end is None:
        return None

    fn_src = entry_body[fn_start:fn_end]
    clean = strip_lua_comments(fn_src)

    mret = re.search(r"\breturn\b\s*([\s\S]*?)\bend\b", clean)
    if not mret:
        return None

    expr = mret.group(1).strip()
    expr = re.sub(r"\s+", " ", expr)
    return expr or None


def _parse_rule_constraints(expr: str) -> Dict[str, Any]:
    """Best-effort extraction of common `names.*` / `tags.*` constraints from test-return expr."""
    expr = (expr or "").strip()
    out: Dict[str, Any] = {"raw": expr, "tags": [], "names": [], "names_any": [], "names_sum": [], "unparsed": []}
    if not expr:
        return out

    # Normalize spaces to reduce corner cases
    e = re.sub(r"\s+", " ", expr)

    seen = set()
    sum_seen: Set[Tuple[str, str, int]] = set()
    or_names: Set[str] = set()
    or_spans: List[Tuple[int, int]] = []

    def _add_name_constraint(key: str, op: str, value: Any, text: str) -> None:
        rec = ("names", key, op, str(value))
        if rec in seen:
            return
        seen.add(rec)
        out["names"].append({"key": key, "op": op, "value": value, "text": text})

    def _add_names_sum(keys: List[str], min_val: int, text: str) -> None:
        if len(keys) != 2:
            return
        a = str(keys[0]).strip()
        b = str(keys[1]).strip()
        if not a or not b or a == b:
            return
        key = tuple(sorted((a, b))) + (int(min_val),)
        if key in sum_seen:
            return
        sum_seen.add(key)
        out["names_sum"].append({"keys": [a, b], "min": int(min_val), "text": text})

    # detect parenthesized OR groups: (names.a or names.b)
    paren_pat = re.compile(r"\(([^()]+)\)")
    for m in paren_pat.finditer(e):
        body = m.group(1)
        if not re.match(r"^\s*names\.[A-Za-z0-9_]+(?:\s+or\s+names\.[A-Za-z0-9_]+)+\s*$", body):
            continue
        keys = re.findall(r"\bnames\.([A-Za-z0-9_]+)\b", body)
        if len(keys) < 2:
            continue
        prefix = e[: m.start()].rstrip()
        negated = bool(re.search(r"\bnot$", prefix))
        if negated:
            for key in keys:
                _add_name_constraint(key, "==", 0, f"not names.{key}")
            or_names.update(keys)
            or_spans.append(m.span())
            continue
        out["names_any"].append({"keys": keys, "text": body.strip()})
        or_names.update(keys)
        or_spans.append(m.span())

    # detect inline OR groups: names.a or names.b
    or_pat = re.compile(r"\bnames\.[A-Za-z0-9_]+\b(?:\s+or\s+names\.[A-Za-z0-9_]+\b)+")
    for m in or_pat.finditer(e):
        span = m.span()
        if any(span[0] >= s[0] and span[1] <= s[1] for s in or_spans):
            continue
        body = m.group(0)
        if re.search(r"\bnot\s+names\.", body):
            continue
        prefix = e[: m.start()].rstrip()
        if re.search(r"\bnot$", prefix):
            continue
        keys = re.findall(r"\bnames\.([A-Za-z0-9_]+)\b", body)
        if len(keys) < 2:
            continue
        out["names_any"].append({"keys": keys, "text": body.strip()})
        or_names.update(keys)
        or_spans.append(span)

    # detect OR sum groups: ((names.a and names.a > 1) or (names.b and names.b > 1) or (names.a and names.b))
    sum_pat = re.compile(
        r"\(+\s*names\.(?P<a>[A-Za-z0-9_]+)\s+and\s+names\.(?P=a)\s*(?:>=|>)\s*(?P<n1>[0-9]+)\s*\)+\s+or\s+"
        r"\(+\s*names\.(?P<b>[A-Za-z0-9_]+)\s+and\s+names\.(?P=b)\s*(?:>=|>)\s*(?P<n2>[0-9]+)\s*\)+\s+or\s+"
        r"\(+\s*names\.(?P<x>[A-Za-z0-9_]+)\s+and\s+names\.(?P<y>[A-Za-z0-9_]+)\s*\)+"
    )
    for m in sum_pat.finditer(e):
        a = m.group("a")
        b = m.group("b")
        x = m.group("x")
        y = m.group("y")
        if not a or not b or {x, y} != {a, b}:
            continue
        _add_names_sum([a, b], 2, m.group(0).strip())

    # detect plus sum groups: (names.a + names.b >= N)
    plus_pat = re.compile(
        r"\(?\s*\(?\s*names\.(?P<a>[A-Za-z0-9_]+)\s*(?:or\s*0)?\s*\)?\s*\+\s*"
        r"\(?\s*names\.(?P<b>[A-Za-z0-9_]+)\s*(?:or\s*0)?\s*\)?\s*\)?\s*(?P<op>>=|>)\s*(?P<n>[0-9]+)"
    )
    for m in plus_pat.finditer(e):
        a = m.group("a")
        b = m.group("b")
        op = m.group("op") or ">="
        n = m.group("n") or "0"
        try:
            min_val = int(n)
        except Exception:
            continue
        if op == ">":
            min_val += 1
        _add_names_sum([a, b], min_val, m.group(0).strip())

    # comparisons: tags.X <op> (number|nil|identifier)
    cmp_pat = re.compile(
        r"\b(?P<scope>tags|names)\.(?P<key>[A-Za-z0-9_]+)\s*(?P<op>==|~=|<=|>=|<|>)\s*(?P<rhs>[^\s\)\]]+)"
    )
    for m in cmp_pat.finditer(e):
        scope = m.group("scope")
        key = m.group("key")
        op = m.group("op")
        rhs = m.group("rhs").rstrip(",")
        rhs_norm: Any
        if rhs == "nil":
            rhs_norm = None
        elif _NUM_RE.match(rhs):
            try:
                rhs_norm = float(rhs)
                if isinstance(rhs_norm, float) and rhs_norm.is_integer():
                    rhs_norm = int(rhs_norm)
            except Exception:
                rhs_norm = rhs
        else:
            rhs_norm = rhs

        rec = (scope, key, op, str(rhs_norm))
        if rec in seen:
            continue
        seen.add(rec)
        out[scope].append({"key": key, "op": op, "value": rhs_norm, "text": m.group(0)})

    # presence (truthy): tags.X / names.X
    pres_pat = re.compile(r"\b(?P<scope>tags|names)\.(?P<key>[A-Za-z0-9_]+)\b(?!\s*(==|~=|<=|>=|<|>))")
    for m in pres_pat.finditer(e):
        scope = m.group("scope")
        key = m.group("key")
        rec = (scope, key, ">", 0)
        if rec in seen:
            continue
        seen.add(rec)
        out[scope].append({"key": key, "op": ">", "value": 0, "text": m.group(0)})

    # negated presence: not tags.X / not names.X
    neg_pat = re.compile(r"\bnot\s+(?P<scope>tags|names)\.(?P<key>[A-Za-z0-9_]+)\b")
    for m in neg_pat.finditer(e):
        scope = m.group("scope")
        key = m.group("key")
        rec = (scope, key, "==", 0)
        if rec in seen:
            continue
        seen.add(rec)
        out[scope].append({"key": key, "op": "==", "value": 0, "text": m.group(0)})

    sum_keys: Set[str] = set()
    for g in out.get("names_sum") or []:
        keys_raw = g.get("keys") if isinstance(g, dict) else None
        if not isinstance(keys_raw, list):
            continue
        for k in keys_raw:
            key = str(k).strip()
            if key:
                sum_keys.add(key)

    if or_names:
        filtered: List[Dict[str, Any]] = []
        for c in out["names"]:
            key = str(c.get("key") or "").strip()
            op = str(c.get("op") or "").strip()
            value = c.get("value")
            if key in or_names and op in (">", ">=") and (value is None or float(value) <= 0):
                continue
            filtered.append(c)
        out["names"] = filtered

    if sum_keys and out.get("names"):
        filtered = []
        for c in out["names"]:
            key = str(c.get("key") or "").strip()
            op = str(c.get("op") or "").strip()
            value = c.get("value")
            try:
                rhs = float(value)
            except Exception:
                rhs = None
            positive = op in (">", ">=") and (rhs is None or rhs >= 0)
            if op == "==" and rhs is not None and rhs > 0:
                positive = True
            if key in sum_keys and positive:
                continue
            filtered.append(c)
        out["names"] = filtered

    return out


class CookingRecipeAnalyzer:
    """
    Parse preparedfoods*.lua (data-driven part).

    Extract stable fields for wiki/web:
    - priority/weight/foodtype/hunger/health/sanity/perishtime/cooktime/tags
    - card_def.ingredients -> card_ingredients: list[[item, count], ...]
    - rule constraints (best-effort): `test = function(...) return ... end`
    """

    STABLE_KEYS = (
        "priority",
        "weight",
        "foodtype",
        "hunger",
        "health",
        "sanity",
        "perishtime",
        "cooktime",
        "tags",
    )

    def __init__(self, content: str):
        self.content = content or ""
        self.recipes: Dict[str, Dict[str, Any]] = {}
        if content:
            self._parse()

    def _parse(self) -> None:
        # most files: local foods = { ... }
        m = re.search(r"local\s+foods\s*=\s*\{", self.content)
        if not m:
            return
        open_idx = m.end() - 1
        close_idx = _find_matching(self.content, open_idx, "{", "}")
        if close_idx is None:
            return

        inner = self.content[open_idx + 1 : close_idx]

        for name, body in _iter_named_table_blocks(inner):
            tbl = parse_lua_table(body)
            if not isinstance(tbl, LuaTableValue):
                continue

            mp = tbl.map
            out: Dict[str, Any] = {}

            for key in self.STABLE_KEYS:
                if key in mp:
                    out[key] = lua_to_python(mp[key])

            # card_def.ingredients -> card_ingredients
            card = mp.get("card_def")
            if isinstance(card, LuaTableValue):
                ing = card.map.get("ingredients")
                if isinstance(ing, LuaTableValue):
                    rows: List[List[Any]] = []
                    for r in ing.array:
                        if isinstance(r, LuaTableValue) and len(r.array) >= 2:
                            rows.append([lua_to_python(r.array[0]), lua_to_python(r.array[1])])
                    if rows:
                        out["card_ingredients"] = rows

            # rule constraints (test-return expr)
            test_expr = _extract_test_return_expr(body)
            if test_expr:
                out["rule"] = {
                    "kind": "test_return",
                    "expr": test_expr,
                    "constraints": _parse_rule_constraints(test_expr),
                }

            if out:
                self.recipes[name] = out


_ING_ID_RE = re.compile(r"^[a-z0-9_]+$")


def _clean_ingredient_id(value: Any) -> Optional[str]:
    if value is None:
        return None
    raw = str(value).strip()
    if not raw:
        return None
    raw = raw.lower()
    if not _ING_ID_RE.match(raw):
        return None
    return raw


def _coerce_tag_value(value: Any) -> Optional[float]:
    if isinstance(value, bool):
        return 1.0 if value else 0.0
    if isinstance(value, (int, float)):
        return float(value)
    if isinstance(value, str) and _NUM_RE.match(value):
        try:
            return float(value)
        except Exception:
            return None
    return None


def _parse_tag_table(tags: Any) -> Tuple[Dict[str, float], Dict[str, str]]:
    if not isinstance(tags, LuaTableValue):
        return {}, {}
    out: Dict[str, float] = {}
    expr: Dict[str, str] = {}

    for key, value in tags.map.items():
        k = lua_to_python(key)
        if not isinstance(k, str):
            continue
        k = k.strip().lower()
        if not k:
            continue
        v = lua_to_python(value)
        num = _coerce_tag_value(v)
        if num is None:
            expr[k] = str(v)
        else:
            out[k] = num

    for entry in tags.array:
        k = lua_to_python(entry)
        if not isinstance(k, str):
            continue
        k = k.strip().lower()
        if not k or k in out or k in expr:
            continue
        out[k] = 1.0

    return out, expr


def _extract_table_by_pattern(content: str, pattern: str) -> Optional[LuaTableValue]:
    m = re.search(pattern, content)
    if not m:
        return None
    open_idx = content.find("{", m.end() - 1)
    if open_idx < 0:
        return None
    close_idx = _find_matching(content, open_idx, "{", "}")
    if close_idx is None:
        return None
    inner = content[open_idx + 1 : close_idx]
    try:
        return parse_lua_table(inner)
    except Exception:
        return None


def _find_ingredients_table(content: str) -> Optional[LuaTableValue]:
    patterns = [
        r"(?:^|\b)local\s+ingredients\s*=\s*\{",
        r"(?:^|\b)ingredients\s*=\s*\{",
        r"(?:^|\b)INGREDIENTS\s*=\s*\{",
        r"\bcooking\.ingredients\s*=\s*\{",
    ]
    for pat in patterns:
        tbl = _extract_table_by_pattern(content, pat)
        if isinstance(tbl, LuaTableValue):
            return tbl

    cooking_tbl = _extract_table_by_pattern(content, r"(?:^|\b)local\s+cooking\s*=\s*\{")
    if not isinstance(cooking_tbl, LuaTableValue):
        cooking_tbl = _extract_table_by_pattern(content, r"(?:^|\b)cooking\s*=\s*\{")
    if isinstance(cooking_tbl, LuaTableValue):
        ing = cooking_tbl.map.get("ingredients")
        if isinstance(ing, LuaTableValue):
            return ing

    return None


def _find_named_table(content: str, name: str) -> Optional[LuaTableValue]:
    if not name:
        return None
    pat_name = re.escape(name)
    patterns = [
        rf"(?:^|\b)local\s+{pat_name}\s*=\s*\{{",
        rf"(?:^|\b){pat_name}\s*=\s*\{{",
    ]
    for pat in patterns:
        tbl = _extract_table_by_pattern(content, pat)
        if isinstance(tbl, LuaTableValue):
            return tbl
    return None


def _coerce_lua_bool(expr: str, default: bool = False) -> bool:
    val = parse_lua_expr(expr)
    if isinstance(val, bool):
        return val
    if isinstance(val, (int, float)):
        return bool(val)
    if isinstance(val, LuaRaw):
        raw = val.text.strip().lower()
        if raw == "true":
            return True
        if raw == "false":
            return False
    return default


def parse_oceanfish_ingredients(content: str, *, source: str = "") -> Dict[str, Dict[str, Any]]:
    fish_tbl = _find_named_table(content or "", "FISH_DEFS")
    if not isinstance(fish_tbl, LuaTableValue):
        return {}

    out: Dict[str, Dict[str, Any]] = {}

    def _resolve_tags(value: Any) -> Tuple[Dict[str, float], Dict[str, str]]:
        if isinstance(value, LuaTableValue):
            return _parse_tag_table(value)
        if isinstance(value, LuaRaw):
            key = value.text.strip()
            if key:
                tbl = _find_named_table(content, key)
                if isinstance(tbl, LuaTableValue):
                    return _parse_tag_table(tbl)
            return {}, {}
        if isinstance(value, str):
            val = parse_lua_expr(value)
            if isinstance(val, LuaTableValue):
                return _parse_tag_table(val)
            if isinstance(val, LuaRaw):
                key = val.text.strip()
                if key:
                    tbl = _find_named_table(content, key)
                    if isinstance(tbl, LuaTableValue):
                        return _parse_tag_table(tbl)
        return {}, {}

    for _, entry in (fish_tbl.map or {}).items():
        if not isinstance(entry, LuaTableValue):
            continue
        prefab = lua_to_python(entry.map.get("prefab"))
        if not isinstance(prefab, str):
            continue
        prefab = prefab.strip()
        if not prefab:
            continue
        cooker_val = entry.map.get("cooker_ingredient_value")
        if cooker_val is None:
            continue
        tags, tag_expr = _resolve_tags(cooker_val)
        if not tags and not tag_expr:
            continue

        ing_id = _clean_ingredient_id(f"{prefab}_inv")
        if not ing_id:
            continue

        row: Dict[str, Any] = {"id": ing_id}
        if tags:
            row["tags"] = tags
        if tag_expr:
            row["tags_expr"] = tag_expr
        if source:
            row["sources"] = [source]
        if len(row) > 1:
            out[ing_id] = row

    return out


class CookingIngredientAnalyzer:
    """Parse cooking ingredient definitions and extract tag contributions."""

    def __init__(self, content: str, *, source: str = ""):
        self.content = content or ""
        self.source = source or ""
        self.ingredients: Dict[str, Dict[str, Any]] = {}
        if content:
            self._parse()

    def _parse(self) -> None:
        tbl = _find_ingredients_table(self.content)
        if not isinstance(tbl, LuaTableValue):
            self._parse_add_ingredient_values()
            self._apply_aliases()
            return
        if not (tbl.map or tbl.array):
            self._parse_add_ingredient_values()
            self._apply_aliases()
            return

        for key, value in (tbl.map or {}).items():
            ing_id = _clean_ingredient_id(lua_to_python(key))
            if not ing_id:
                continue

            out: Dict[str, Any] = {"id": ing_id}

            if isinstance(value, LuaTableValue):
                tags, tag_expr = _parse_tag_table(value.map.get("tags"))
                if tags:
                    out["tags"] = tags
                if tag_expr:
                    out["tags_expr"] = tag_expr

                for field in ("name", "atlas", "image", "prefab", "foodtype"):
                    if field in value.map:
                        out[field] = lua_to_python(value.map[field])

            if self.source:
                out["sources"] = [self.source]

            if len(out) > 1:
                self.ingredients[ing_id] = out

        self._apply_aliases()

    def _apply_aliases(self) -> None:
        aliases_tbl = _find_named_table(self.content, "aliases")
        if not isinstance(aliases_tbl, LuaTableValue):
            return
        for alias_key, alias_val in (aliases_tbl.map or {}).items():
            alias = _clean_ingredient_id(lua_to_python(alias_key))
            target = _clean_ingredient_id(lua_to_python(alias_val))
            if not alias or not target:
                continue
            if alias in self.ingredients:
                continue
            src = self.ingredients.get(target)
            if not isinstance(src, dict):
                continue
            out: Dict[str, Any] = {"id": alias}
            if "tags" in src:
                out["tags"] = dict(src["tags"])
            if "tags_expr" in src:
                out["tags_expr"] = dict(src["tags_expr"])
            if "foodtype" in src:
                out["foodtype"] = src["foodtype"]
            if "sources" in src:
                out["sources"] = list(src["sources"])
            if len(out) > 1:
                self.ingredients[alias] = out

    def _parse_add_ingredient_values(self) -> None:
        extractor = LuaCallExtractor(self.content)
        calls = extractor.extract_calls("AddIngredientValues", include_member_calls=False)
        if not calls:
            return

        table_cache: Dict[str, Optional[LuaTableValue]] = {}

        def _resolve_names(expr: str) -> List[str]:
            val = parse_lua_expr(expr)
            if isinstance(val, LuaTableValue):
                names = [lua_to_python(x) for x in val.array]
                return [x for x in names if isinstance(x, str)]
            if isinstance(val, str):
                return [val]
            if isinstance(val, LuaRaw):
                key = val.text.strip()
                if not key:
                    return []
                if key not in table_cache:
                    table_cache[key] = _find_named_table(self.content, key)
                tbl = table_cache.get(key)
                if isinstance(tbl, LuaTableValue):
                    names = [lua_to_python(x) for x in tbl.array]
                    return [x for x in names if isinstance(x, str)]
            return []

        def _set_entry(ing_id: str, tags: Dict[str, float], tag_expr: Dict[str, str]) -> None:
            out: Dict[str, Any] = {"id": ing_id}
            if tags:
                out["tags"] = tags
            if tag_expr:
                out["tags_expr"] = tag_expr
            if self.source:
                out["sources"] = [self.source]
            if len(out) > 1:
                self.ingredients[ing_id] = out

        for call in calls:
            args = [str(a).strip() for a in (call.arg_list or [])]
            if len(args) < 2:
                continue
            names_expr = args[0]
            tags_expr = args[1]
            cancook = _coerce_lua_bool(args[2]) if len(args) >= 3 else False
            candry = _coerce_lua_bool(args[3]) if len(args) >= 4 else False

            names = _resolve_names(names_expr)
            if not names:
                continue
            tags_val = parse_lua_expr(tags_expr)
            tags, tag_expr = _parse_tag_table(tags_val if isinstance(tags_val, LuaTableValue) else None)

            for name in names:
                ing_id = _clean_ingredient_id(name)
                if not ing_id:
                    continue
                _set_entry(ing_id, dict(tags), dict(tag_expr))

                if cancook:
                    cooked_tags = dict(tags)
                    cooked_tags["precook"] = 1.0
                    _set_entry(f"{ing_id}_cooked", cooked_tags, dict(tag_expr))
                if candry:
                    dried_tags = dict(tags)
                    dried_tags["dried"] = 1.0
                    _set_entry(f"{ing_id}_dried", dried_tags, dict(tag_expr))
```

### File: core/parsers/loot.py
- mode: full
- size_bytes: 2175
- sha256_12: 3595dc46ca58

```py
# -*- coding: utf-8 -*-
"""Loot table parser."""

from __future__ import annotations

from typing import Any, Dict

from core.lua import LuaCallExtractor, LuaTableValue, parse_lua_expr, parse_lua_string
from core.parsers.base import BaseParser

__all__ = ["LootParser"]


class LootParser(BaseParser):
    """Parse shared loot tables + simple loot helpers."""

    def parse(self) -> Dict[str, Any]:
        data: Dict[str, Any] = {"type": "loot", "table_name": None, "entries": []}
        extractor = LuaCallExtractor(self.content)

        for call in extractor.iter_calls("SetSharedLootTable"):
            if not call.arg_list:
                continue
            name = parse_lua_string(call.arg_list[0]) or None
            if name:
                data["table_name"] = name
            if len(call.arg_list) >= 2:
                tbl = parse_lua_expr(call.arg_list[1])
                if isinstance(tbl, LuaTableValue):
                    for row in tbl.array:
                        if isinstance(row, LuaTableValue) and len(row.array) >= 2:
                            item = row.array[0]
                            chance = row.array[1]
                            if isinstance(item, str) and isinstance(chance, (int, float)):
                                data["entries"].append({"item": item, "chance": float(chance), "method": "TableData"})

        for call in extractor.iter_calls(["AddRandomLoot", "AddRandomLootTable"]):
            if len(call.arg_list) >= 2:
                item = parse_lua_string(call.arg_list[0])
                w = parse_lua_expr(call.arg_list[1])
                if isinstance(item, str) and isinstance(w, (int, float)):
                    data["entries"].append({"item": item, "weight": float(w), "method": "Random"})

        for call in extractor.iter_calls("AddChanceLoot"):
            if len(call.arg_list) >= 2:
                item = parse_lua_string(call.arg_list[0])
                c = parse_lua_expr(call.arg_list[1])
                if isinstance(item, str) and isinstance(c, (int, float)):
                    data["entries"].append({"item": item, "chance": float(c), "method": "Chance"})

        return data
```

### File: core/parsers/lua_analyzer.py
- mode: full
- size_bytes: 1623
- sha256_12: cecaa2d581fa

```py
# -*- coding: utf-8 -*-
"""LuaAnalyzer facade for selecting domain parsers."""

from __future__ import annotations

from typing import Any, Dict, Optional

from core.parsers.base import BaseParser
from core.parsers.loot import LootParser
from core.parsers.prefab import PrefabParser
from core.parsers.strings import StringParser
from core.parsers.widget import WidgetParser

__all__ = ["LuaAnalyzer"]


class LuaAnalyzer:
    """Facade: choose best strategy based on content + optional path."""

    def __init__(self, content: str, path: Optional[str] = None):
        self.content = content or ""
        self.path = path
        self.parser = self._select_strategy()

    def _select_strategy(self) -> BaseParser:
        p = (self.path or "").replace("\\", "/")
        c = self.content

        if p.startswith("scripts/widgets/") or p.startswith("scripts/screens/"):
            return WidgetParser(c, p)
        if p.startswith("scripts/strings"):
            return StringParser(c, p)
        if p.startswith("scripts/prefabs/"):
            return PrefabParser(c, p)

        if "Class(Widget" in c or "Class(Screen" in c or 'require "widgets/' in c or "require('widgets/" in c:
            return WidgetParser(c, p)
        if "return Prefab" in c or "Prefab(" in c:
            return PrefabParser(c, p)
        if "STRINGS." in c and "STRINGS.CHARACTERS" in c:
            return StringParser(c, p)
        if "SetSharedLootTable" in c or "AddChanceLoot" in c:
            return LootParser(c, p)
        return PrefabParser(c, p)

    def get_report(self) -> Dict[str, Any]:
        return self.parser.parse()
```

### File: core/parsers/prefab.py
- mode: full
- size_bytes: 3102
- sha256_12: f08e0ff280ca

```py
# -*- coding: utf-8 -*-
"""Prefab parser."""

from __future__ import annotations

import re
from typing import Any, Dict, List

from core.lua import LuaCallExtractor, parse_lua_string
from core.parsers.base import BaseParser

__all__ = ["PrefabParser"]


class PrefabParser(BaseParser):
    def parse(self) -> Dict[str, Any]:
        data: Dict[str, Any] = {
            "type": "prefab",
            "assets": [],
            "components": [],
            "helpers": [],
            "stategraph": None,
            "brain": None,
            "events": [],
            "tags": [],
            "prefab_name": None,
        }

        extractor = LuaCallExtractor(self.content)

        for call in extractor.iter_calls("Prefab"):
            if call.arg_list:
                nm = parse_lua_string(call.arg_list[0])
                if nm:
                    data["prefab_name"] = nm
                    break

        for call in extractor.iter_calls("Asset"):
            if len(call.arg_list) >= 2:
                t = parse_lua_string(call.arg_list[0])
                p = parse_lua_string(call.arg_list[1])
                if isinstance(t, str) and isinstance(p, str):
                    data["assets"].append({"type": t, "path": p})

        m = re.search(r"SetBrain\s*\(\s*require\s*\(\s*['\"](.*?)['\"]\s*\)\s*\)", self.clean)
        if m:
            data["brain"] = m.group(1)
        m = re.search(r"SetStateGraph\s*\(\s*['\"](.*?)['\"]\s*\)", self.clean)
        if m:
            data["stategraph"] = m.group(1)

        data["events"] = re.findall(r'EventHandler\s*\(\s*["\']([^"\']+)["\']\s*,', self.clean)
        data["helpers"] = sorted(set(re.findall(r"^\s*(Make[A-Za-z0-9_]+)\s*\(", self.content, flags=re.MULTILINE)))

        tags: List[str] = []
        for call in extractor.iter_calls("AddTag"):
            if call.arg_list:
                tg = parse_lua_string(call.arg_list[0])
                if tg:
                    tags.append(tg)
        data["tags"] = sorted(set(tags))

        comps = set()
        for call in extractor.iter_calls("AddComponent"):
            if call.arg_list:
                cn = parse_lua_string(call.arg_list[0])
                if cn:
                    comps.add(cn)

        for comp_name in sorted(comps):
            comp_data = {"name": comp_name, "methods": [], "properties": []}

            method_pat = re.compile(r"components\." + re.escape(comp_name) + r"[:\.]([A-Za-z0-9_]+)\s*\((.*?)\)", re.DOTALL)
            for m_name, m_args in method_pat.findall(self.clean):
                clean_args = re.sub(r"\s+", " ", m_args).strip()
                if len(clean_args) > 60:
                    clean_args = clean_args[:57] + "..."
                comp_data["methods"].append(f"{m_name}({clean_args})")

            prop_pat = re.compile(r"components\." + re.escape(comp_name) + r"\.([A-Za-z0-9_]+)\s*=\s*([^=\n]+)")
            for p_name, p_val in prop_pat.findall(self.clean):
                comp_data["properties"].append(f"{p_name} = {p_val.strip()}")

            data["components"].append(comp_data)

        return data
```

### File: core/parsers/strings.py
- mode: full
- size_bytes: 610
- sha256_12: bf621ad77af2

```py
# -*- coding: utf-8 -*-
"""Strings parser."""

from __future__ import annotations

import re
from typing import Any, Dict

from core.parsers.base import BaseParser

__all__ = ["StringParser"]


class StringParser(BaseParser):
    def parse(self) -> Dict[str, Any]:
        data: Dict[str, Any] = {"type": "strings", "roots": [], "includes": self._extract_requires()}
        roots = set()
        roots.update(re.findall(r"STRINGS\.([A-Z0-9_]+)\s*=\s*\{", self.clean))
        roots.update(re.findall(r"STRINGS\.([A-Z0-9_]+)\s*=\s*['\"]", self.clean))
        data["roots"] = sorted(roots)
        return data
```

### File: core/parsers/tuning.py
- mode: full
- size_bytes: 12559
- sha256_12: e43dd5a73a7b

```py
# -*- coding: utf-8 -*-
"""Tuning resolver for DST tuning.lua."""

from __future__ import annotations

import math
import re
from typing import Any, Dict, List, Optional, Tuple, Union

from core.lua import _NUM_RE, _split_top_level, find_matching, parse_lua_expr, parse_lua_string, strip_lua_comments

__all__ = ["TuningResolver"]


_ARITH_TOKEN_RE = re.compile(r"\s*(\d+\.\d+|\d+|[A-Za-z_][A-Za-z0-9_\.]*|\*\*|\^|[+\-*/()])\s*")


class TuningResolver:
    """
    Lightweight resolver for DST `scripts/tuning.lua`.

    Goals
    - Parse common constant assignments:
        - `local NAME = <rhs>`  (UPPER_CASE only)
        - `TUNING.NAME = <rhs>`
    - Resolve numeric chains and simple arithmetic expressions.
    - Provide *traceable* resolution (for UI/wiki), not only final numbers.

    Notes
    - This is intentionally conservative: if an expression can't be proven safe and numeric,
      resolution returns None rather than guessing.
    """

    _REF_PAT = re.compile(r"TUNING\.([A-Za-z0-9_]+)|TUNING\[\s*([\'\"])([A-Za-z0-9_]+)\2\s*\]")

    def __init__(self, content: str):
        self.raw_map: Dict[str, Any] = {}
        self.local_map: Dict[str, Any] = {}
        if content:
            self._parse_tuning(content)

    # --------------------------
    # Parsing
    # --------------------------

    def _parse_tuning(self, content: str) -> None:
        clean = strip_lua_comments(content)

        # locals (allow lowercase; many tuning constants depend on lower vars like calories_per_day)
        for m in re.finditer(r"^\s*local\s+([A-Za-z0-9_]+)\s*=\s*(.+?)\s*$", clean, flags=re.MULTILINE):
            name, rhs = m.group(1), m.group(2)
            rhs = rhs.strip().rstrip(",")
            val = self._parse_rhs(rhs)
            if val is not None:
                self.local_map[name] = val

        # TUNING.KEY = rhs
        for m in re.finditer(r"^\s*TUNING\.([A-Z0-9_]+)\s*=\s*(.+?)\s*$", clean, flags=re.MULTILINE):
            key, rhs = m.group(1), m.group(2)
            rhs = rhs.strip().rstrip(",")
            val = self._parse_rhs(rhs)
            self.raw_map[key] = val if val is not None else rhs

        # TUNING = { KEY = rhs, ... }
        for m_table in re.finditer(r"\bTUNING\s*=\s*\{", clean):
            open_idx = clean.find("{", m_table.start())
            close_idx = find_matching(clean, open_idx, "{", "}")
            if close_idx is None:
                continue
            inner = clean[open_idx + 1 : close_idx]
            for m in re.finditer(r"^\s*([A-Z0-9_]+)\s*=\s*(.+?)\s*(?:,|$)", inner, flags=re.MULTILINE):
                key, rhs = m.group(1), m.group(2)
                rhs = rhs.strip().rstrip(",")
                val = self._parse_rhs(rhs)
                if key not in self.raw_map:
                    self.raw_map[key] = val if val is not None else rhs

    def _parse_rhs(self, rhs: str) -> Optional[Any]:
        rhs = (rhs or "").strip().rstrip(",")
        if not rhs:
            return None
        if rhs in ("true", "false"):
            return rhs == "true"
        if rhs == "nil":
            return None

        s = parse_lua_string(rhs)
        if s is not None:
            return s

        if _NUM_RE.match(rhs):
            try:
                f = float(rhs)
                return int(f) if f.is_integer() else f
            except Exception:
                return None

        # keep as raw string expression / symbol
        return rhs

    # --------------------------
    # Resolution (internal)
    # --------------------------

    @staticmethod
    def _norm_key(ref: str) -> str:
        ref = (ref or "").strip()
        return ref[7:] if ref.startswith("TUNING.") else ref

    def _resolve_ref(self, ref: str, depth: int = 8) -> Optional[Union[int, float]]:
        """Resolve a ref/expression to a number (or None)."""
        if depth <= 0:
            return None
        ref = (ref or "").strip()
        if not ref:
            return None

        # numeric literal
        if _NUM_RE.match(ref):
            try:
                f = float(ref)
                return int(f) if f.is_integer() else f
            except Exception:
                return None

        # math.* function calls (limited whitelist)
        m_call = re.match(r"^math\.([A-Za-z_][A-Za-z0-9_]*)\((.*)\)$", ref)
        if m_call:
            fn = m_call.group(1).lower()
            args_raw = m_call.group(2)
            args: List[Optional[Union[int, float]]] = []
            for part in _split_top_level(args_raw, sep=","):
                part = part.strip()
                if not part:
                    continue
                args.append(self._resolve_ref(part, depth - 1))
            # only proceed if all args resolved
            if any(a is None for a in args):
                return None
            vals = [float(a) for a in args if a is not None]
            try:
                if fn == "abs" and len(vals) == 1:
                    return abs(vals[0])
                if fn == "floor" and len(vals) == 1:
                    return math.floor(vals[0])
                if fn == "ceil" and len(vals) == 1:
                    return math.ceil(vals[0])
                if fn == "sqrt" and len(vals) == 1:
                    return math.sqrt(vals[0])
                if fn == "max" and vals:
                    return max(vals)
                if fn == "min" and vals:
                    return min(vals)
                if fn in ("pow",) and len(vals) == 2:
                    return math.pow(vals[0], vals[1])
            except Exception:
                return None
            return None

        # direct symbol (TUNING.X / local X)
        if re.match(r"^[A-Za-z_][A-Za-z0-9_\.]*$", ref):
            key = self._norm_key(ref)
            v = self.raw_map.get(key, self.local_map.get(key))
            if isinstance(v, (int, float)):
                return v
            if isinstance(v, str) and v and v != ref:
                # symbol chain (A -> B) or expression
                if re.match(r"^[A-Za-z_][A-Za-z0-9_\.]*$", v):
                    return self._resolve_ref(v, depth - 1)
                return self._resolve_ref(v, depth - 1)
            return None

        # arithmetic expression (conservative tokenizer)
        py_parts: List[str] = []
        for tok in _ARITH_TOKEN_RE.findall(ref):
            tok = tok.strip()
            if not tok:
                continue

            # Lua exponent
            if tok == "^":
                py_parts.append("**")
                continue
            if tok in {"+", "-", "*", "/", "(", ")", "**"}:
                py_parts.append(tok)
                continue
            if _NUM_RE.match(tok):
                py_parts.append(tok)
                continue

            val = self._resolve_ref(tok, depth - 1)
            if val is None:
                return None
            py_parts.append(str(val))

        expr_py = "".join(py_parts)
        # Safety: only numbers + operators
        if re.search(r"[^0-9\.\+\-\*\/\(\)eE]", expr_py):
            return None
        try:
            out = eval(expr_py, {"__builtins__": {}}, {})
            if isinstance(out, (int, float)):
                if isinstance(out, float) and out.is_integer():
                    return int(out)
                return out
        except Exception:
            return None
        return None

    # --------------------------
    # Public APIs
    # --------------------------

    def explain(self, key: str, max_hops: int = 10) -> Tuple[str, Optional[Union[int, float]]]:
        """Return (chain_text, resolved_value)."""
        key = self._norm_key(key)
        if not key:
            return "", None

        chain: List[str] = []
        visited = set()
        cur = key

        for _ in range(max_hops):
            if cur in visited:
                chain.append(f"{cur} (loop)")
                break
            visited.add(cur)

            v = self.raw_map.get(cur, self.local_map.get(cur))
            if v is None:
                chain.append(cur)
                break

            chain.append(cur)

            if isinstance(v, (int, float)):
                chain.append(str(v))
                return " -> ".join(chain), v

            if isinstance(v, str):
                chain.append(v)
                if re.match(r"^[A-Za-z_][A-Za-z0-9_\.]*$", v):
                    cur = self._norm_key(v)
                    continue
                val = self._resolve_ref(v)
                if val is not None:
                    chain.append(str(val))
                    return " -> ".join(chain), val
                break

            chain.append(str(v))
            break

        # fallback try resolve the symbol itself (handles local->expr cases)
        val = self._resolve_ref(key)
        return " -> ".join(chain) if chain else key, val

    def trace_key(self, key: str, max_hops: int = 16) -> Dict[str, Any]:
        """Structured trace for a single TUNING key."""
        key0 = key
        key = self._norm_key(key)
        steps: List[Dict[str, Any]] = []
        visited = set()
        cur = key

        for _ in range(max_hops):
            if not cur:
                break
            if cur in visited:
                steps.append({"key": cur, "raw": None, "note": "loop"})
                break
            visited.add(cur)

            v = self.raw_map.get(cur, self.local_map.get(cur))
            steps.append({"key": cur, "raw": v})

            if isinstance(v, (int, float)):
                chain = " -> ".join([str(s.get("key") or "") for s in steps] + [str(v)])
                return {"key": key0, "normalized": key, "value": v, "steps": steps, "chain": chain}

            if isinstance(v, str) and re.match(r"^[A-Za-z_][A-Za-z0-9_\.]*$", v):
                cur = self._norm_key(v)
                continue

            # expression or unknown
            if isinstance(v, str):
                val = self._resolve_ref(v)
                return {
                    "key": key0,
                    "normalized": key,
                    "value": val,
                    "steps": steps + [{"key": "<expr>", "raw": v, "value": val}],
                    "chain": " -> ".join([str(s.get("key") or s.get("raw") or "") for s in steps] + [v, str(val)]),
                }
            break

        # fallback
        val = self._resolve_ref(key)
        return {
            "key": key0,
            "normalized": key,
            "value": val,
            "steps": steps,
            "chain": " -> ".join([str(s.get("key") or s.get("raw") or "") for s in steps if s.get("key")] + ([str(val)] if val is not None else [])),
        }

    def trace_expr(self, expr: str) -> Dict[str, Any]:
        """Trace an arbitrary expression containing TUNING refs."""
        expr = (expr or "").strip()
        refs = []
        for m in self._REF_PAT.finditer(expr):
            k = m.group(1) or m.group(3)
            if k and k not in refs:
                refs.append(k)

        ref_traces: Dict[str, Any] = {}
        for k in refs:
            ref_traces[k] = self.trace_key(k)

        value = self._resolve_ref(expr)

        # best-effort normalized expression (TUNING.X -> number)
        expr_resolved = expr
        for k in refs:
            v = ref_traces.get(k, {}).get("value")
            if isinstance(v, (int, float)):
                expr_resolved = re.sub(
                    rf"\bTUNING\.{re.escape(k)}\b",
                    str(v),
                    expr_resolved,
                )
                expr_resolved = re.sub(
                    rf"TUNING\[\s*([\'\"])\s*{re.escape(k)}\s*\1\s*\]",
                    str(v),
                    expr_resolved,
                )

        return {
            "expr": expr,
            "value": value,
            "expr_resolved": expr_resolved,
            "refs": ref_traces,
            "expr_chain": " ; ".join(sorted([rt.get("chain") or "" for rt in ref_traces.values() if rt])),
        }

    def enrich(self, text: str) -> str:
        """Inline enrichment: replace `TUNING.X` in text with `TUNING.X (chain)` when resolvable."""
        if not text or "TUNING" not in text:
            return text

        def repl(m: re.Match) -> str:
            key = m.group(1) or m.group(3)
            if not key:
                return m.group(0)
            chain, val = self.explain(key)
            if val is None:
                return f"TUNING.{key}"
            return f"TUNING.{key} ({chain})"

        return self._REF_PAT.sub(repl, text)
```

### File: core/parsers/widget.py
- mode: full
- size_bytes: 575
- sha256_12: ccd11e91cddd

```py
# -*- coding: utf-8 -*-
"""Widget parser."""

from __future__ import annotations

import re
from typing import Any, Dict

from core.parsers.base import BaseParser

__all__ = ["WidgetParser"]


class WidgetParser(BaseParser):
    def parse(self) -> Dict[str, Any]:
        data: Dict[str, Any] = {"type": "widget", "classes": [], "dependencies": self._extract_requires()}
        for name, parent in re.findall(r"local\s+([A-Za-z0-9_]+)\s*=\s*Class\s*\(\s*([A-Za-z0-9_]+)", self.clean):
            data["classes"].append({"name": name, "parent": parent})
        return data
```

### File: core/schemas/__init__.py
- mode: full
- size_bytes: 47
- sha256_12: 54ea79a6fc22

```py
# -*- coding: utf-8 -*-
"""Schemas package."""
```

### File: core/schemas/catalog_v2.py
- mode: full
- size_bytes: 824
- sha256_12: 1dc18eed6fb8

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Catalog v2 data model."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict


@dataclass
class WagstaffCatalogV2:
    schema_version: int
    meta: Dict[str, Any]
    items: Dict[str, Any]
    assets: Dict[str, Any]
    craft: Dict[str, Any]
    cooking: Dict[str, Any]
    cooking_ingredients: Dict[str, Any]
    stats: Dict[str, Any]

    def to_dict(self) -> Dict[str, Any]:
        return {
            "schema_version": int(self.schema_version),
            "meta": self.meta,
            "items": self.items,
            "assets": self.assets,
            "craft": self.craft,
            "cooking": self.cooking,
            "cooking_ingredients": self.cooking_ingredients,
            "stats": self.stats,
        }
```

### File: core/schemas/meta.py
- mode: full
- size_bytes: 808
- sha256_12: f3a5643f582b

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Shared metadata helpers for index artifacts."""

from __future__ import annotations

from datetime import datetime
from typing import Any, Dict, Optional

from core.version import versions


def now_iso() -> str:
    return datetime.now().astimezone().isoformat(timespec="seconds")


def build_meta(
    *,
    schema: int,
    tool: str,
    sources: Optional[Dict[str, Any]] = None,
    extra: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    meta: Dict[str, Any] = {
        "schema": int(schema),
        "generated": now_iso(),
        "tool": str(tool),
    }
    try:
        meta.update(versions())
    except Exception:
        pass
    if sources:
        meta["sources"] = sources
    if extra:
        meta.update(extra)
    return meta
```

### File: core/tagging.py
- mode: full
- size_bytes: 5775
- sha256_12: 9c009d2318ee

```py
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Tag inference and overrides for catalog v2."""

from __future__ import annotations

from dataclasses import dataclass, field
from fnmatch import fnmatchcase
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Set


KIND_ORDER = ["character", "creature", "structure", "plant", "item", "fx", "unknown"]

CREATURE_TAGS = {
    "monster",
    "animal",
    "smallcreature",
    "largecreature",
    "epic",
    "hostile",
    "bird",
    "animal",
    "scarytoprey",
}

PLANT_TAGS = {"plant", "tree", "crop", "flower", "berrybush", "mushroom"}

STRUCTURE_TAGS = {"structure", "wall", "house", "ruins"}

FX_TAGS = {"fx", "noclick", "notarget"}


COMP_BEHAVIORS = {
    "equippable": "equippable",
    "edible": "edible",
    "stackable": "stackable",
    "burnable": "burnable",
    "perishable": "perishable",
    "repairable": "repairable",
    "fuel": "fuel",
    "tradable": "tradable",
    "hauntable": "hauntable",
    "deployable": "deployable",
}

COMP_CATEGORIES = {
    "weapon": "weapon",
    "armor": "armor",
    "edible": "food",
    "container": "container",
    "inventory": "container",
    "light": "light",
    "fueled": "light",
    "deployable": "deployable",
    "trap": "trap",
    "boat": "boat",
    "farmplanttendable": "farm",
    "tool": "tool",
}

TAG_CATEGORIES = {
    "weapon": "weapon",
    "armor": "armor",
    "food": "food",
    "cookable": "food",
    "magic": "magic",
    "container": "container",
    "boat": "boat",
    "decor": "decor",
    "toy": "toy",
    "cattoy": "toy",
    "light": "light",
    "deploykititem": "deployable",
}


@dataclass
class TagProfile:
    kind: str = "unknown"
    categories: Set[str] = field(default_factory=set)
    behaviors: Set[str] = field(default_factory=set)
    sources: Set[str] = field(default_factory=set)
    slots: Set[str] = field(default_factory=set)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "kind": self.kind,
            "categories": sorted(self.categories),
            "behaviors": sorted(self.behaviors),
            "sources": sorted(self.sources),
            "slots": sorted(self.slots),
        }


def _pick_kind(kind: str, tags: Set[str], components: Set[str]) -> str:
    if "character" in tags:
        return "character"
    if tags & CREATURE_TAGS or {"brain", "health", "combat"} <= components:
        return "creature"
    if tags & STRUCTURE_TAGS:
        return "structure"
    if tags & PLANT_TAGS or "pickable" in components or "crop" in components:
        return "plant"
    if tags & FX_TAGS:
        return "fx"
    if "inventoryitem" in components:
        return "item"
    return kind or "unknown"


def infer_tags(
    *,
    components: Iterable[str],
    tags: Iterable[str],
    sources: Iterable[str],
    kind_hint: Optional[str] = None,
) -> TagProfile:
    comps = {str(c).lower() for c in components if c}
    tgs = {str(t).lower() for t in tags if t}
    srcs = {str(s).lower() for s in sources if s}

    profile = TagProfile(kind=kind_hint or "unknown")
    profile.kind = _pick_kind(profile.kind, tgs, comps)

    for c in comps:
        beh = COMP_BEHAVIORS.get(c)
        if beh:
            profile.behaviors.add(beh)
        cat = COMP_CATEGORIES.get(c)
        if cat:
            profile.categories.add(cat)

    for t in tgs:
        cat = TAG_CATEGORIES.get(t)
        if cat:
            profile.categories.add(cat)

    if profile.kind == "item" and "food" in profile.categories and "edible" not in profile.behaviors:
        profile.categories.add("resource")

    profile.sources.update(srcs)
    return profile


def load_tag_overrides(path: Optional[str]) -> List[Dict[str, Any]]:
    if not path:
        return []
    try:
        import json

        data = json.loads(Path(path).read_text(encoding="utf-8"))
        rules = data.get("rules") or []
        return [r for r in rules if isinstance(r, dict)]
    except Exception:
        return []


def _apply_field(
    profile: TagProfile,
    field: str,
    *,
    add: Optional[Iterable[str]] = None,
    remove: Optional[Iterable[str]] = None,
    set_to: Optional[Iterable[str]] = None,
) -> None:
    if field == "kind":
        if set_to:
            profile.kind = str(list(set_to)[0])
        return

    target: Set[str]
    if field == "categories":
        target = profile.categories
    elif field == "behaviors":
        target = profile.behaviors
    elif field == "sources":
        target = profile.sources
    elif field == "slots":
        target = profile.slots
    else:
        return

    if set_to is not None:
        target.clear()
        for x in set_to:
            if x:
                target.add(str(x))
    if add:
        for x in add:
            if x:
                target.add(str(x))
    if remove:
        for x in remove:
            if x:
                target.discard(str(x))


def apply_overrides(item_id: str, profile: TagProfile, rules: List[Dict[str, Any]]) -> TagProfile:
    if not rules:
        return profile

    iid = str(item_id or "").strip()
    if not iid:
        return profile

    for rule in rules:
        pat = str(rule.get("match") or "").strip()
        if not pat:
            continue
        if pat != iid and not fnmatchcase(iid, pat):
            continue

        add = rule.get("add") or {}
        remove = rule.get("remove") or {}
        set_to = rule.get("set") or {}

        for field in ("kind", "categories", "behaviors", "sources", "slots"):
            _apply_field(
                profile,
                field,
                add=add.get(field),
                remove=remove.get(field),
                set_to=set_to.get(field),
            )
        break

    return profile
```

### File: core/version.py
- mode: full
- size_bytes: 1182
- sha256_12: fd907b450742

```py
# -*- coding: utf-8 -*-
"""Project version helpers."""

from __future__ import annotations

import json
from functools import lru_cache
from pathlib import Path
from typing import Dict


def _project_root() -> Path:
    return Path(__file__).resolve().parents[1]


@lru_cache(maxsize=1)
def _load_version_file() -> Dict[str, str]:
    path = _project_root() / "conf" / "version.json"
    if not path.exists():
        return {}
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}
    if not isinstance(data, dict):
        return {}
    out: Dict[str, str] = {}
    for key in ("project_version", "index_version"):
        val = data.get(key)
        if isinstance(val, str) and val.strip():
            out[key] = val.strip()
    return out


def project_version() -> str:
    return _load_version_file().get("project_version", "unknown")


def index_version() -> str:
    ver = _load_version_file().get("index_version")
    if ver:
        return ver
    return project_version()


def versions() -> Dict[str, str]:
    return {
        "project_version": project_version(),
        "index_version": index_version(),
    }
```

### File: devtools/snapshot.py
- mode: full
- size_bytes: 51404
- sha256_12: 6411307db9d8

```py
#!/usr/bin/env python3
"""Wagstaff-Lab Snapshot (v4.4)

Goal:
- Provide LLM-friendly snapshots by default.
- Provide two primary modes:
  - llm: LLM-friendly snapshot (project overview + core/apps full + non-core interfaces/head)
  - archive: archival snapshot (full content as much as possible + optional zip bundle)
- Provide custom templates via JSON config (growth-friendly).

Default behavior:
- `wagstaff snap` => llm template (LLM-friendly) => writes project_context.txt
- Use --focus to narrow the snapshot scope (repeatable).
- Use section switches to reduce noise.

Template config:
- Default path: conf/snapshot_templates.json
- See the example template file for schema.
"""

from __future__ import annotations

import argparse
import ast
import copy
import fnmatch
import hashlib
import json
import os
import platform
import re
import subprocess
import sys
import time
import textwrap
import zipfile
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

PROJECT_ROOT = Path(__file__).resolve().parent.parent
DEFAULT_CONFIG = PROJECT_ROOT / "conf" / "snapshot_templates.json"

DEFAULT_IGNORE_DIRS = {
    ".git",
    "__pycache__",
    ".pytest_cache",
    "logs",
    "env",
    "venv",
    ".idea",
    ".vscode",
    "node_modules",
    "dist",
    "build",
}

DEFAULT_IGNORE_FILES = {
    "project_context.txt",
    ".DS_Store",
    "id_rsa",
    "id_ed25519",
    "known_hosts",
}

# Conservative binary-ish extensions we almost never want in an LLM snapshot.
DEFAULT_IGNORE_GLOBS = [
    "data/snapshots/**",
    "**/*.swp",
    "**/*.swo",
    "**/*.tmp",
    "**/*.bak",
    "**/*.log",
    "**/*.zip",
    "**/*.tar",
    "**/*.tar.gz",
    "**/*.gz",
    "**/*.7z",
    "**/*.rar",
    "**/*.png",
    "**/*.jpg",
    "**/*.jpeg",
    "**/*.webp",
    "**/*.pdf",
    "**/*.mp4",
    "**/*.mov",
    "**/*.sqlite",
    "**/*.db",
    "**/.env",
    "**/.env.*",
    "**/*.pem",
    "**/*.key",
]

# Snapshot section switches (can be overridden per template).
DEFAULT_SECTIONS = {
    "config": True,
    "env": True,
    "overview": True,
    "tree": True,
    "inventory": True,
    "contents": True,
    "stats": True,
}

# LLM-friendly defaults (less noise).
LLM_SECTIONS = {
    "config": False,
    "env": True,
    "overview": True,
    "tree": True,
    "inventory": True,
    "contents": True,
    "stats": True,
}

# Minimal built-in templates (used if conf/snapshot_templates.json is missing).
_LLM_TEMPLATE = {
    "desc": "Builtin LLM-friendly template",
    "output": "project_context.txt",
    "redact": True,
    "include_reports": True,
    "hash": "embedded",
    "embed_order": "smart",
    "sections": dict(LLM_SECTIONS),
    "inventory": {"enabled": True, "scope": "included", "limit": 700},
    "pinned": [
        "PROJECT_STATUS.json",
        "README.md",
        "conf/settings.ini",
        "docs/guides/DEV_GUIDE.md",
        "docs/management/ROADMAP.md",
        "apps/cli/main.py",
        "apps/cli/registry.py",
    ],
    "max_file_bytes": 200000,
    "max_total_bytes": 1200000,
    "tree": {"max_depth": 8, "max_entries_per_dir": 250},
    "include_globs": [
        "README.md",
        "PROJECT_STATUS.json",
        ".gitignore",
        "conf/**/*.ini",
        "core/**/*.py",
        "apps/**/*.py",
        "devtools/**/*.py",
        "docs/**/*.md",
        "tests/**/*.py",
        "data/reports/**/*.md",
        "data/samples/**/*",
    ],
    "ignore_files": sorted(DEFAULT_IGNORE_FILES),
    "ignore_globs": list(DEFAULT_IGNORE_GLOBS),
    "rules": [
        {"match": "core/**/*.py", "mode": "full"},
        {"match": "apps/**/*.py", "mode": "full"},
        {"match": "devtools/**/*.py", "mode": "interface"},
        {"match": "docs/**/*.md", "mode": "head", "head_lines": 240},
        {"match": "data/reports/**/*.md", "mode": "head", "head_lines": 260},
        {"match": "**/*", "mode": "skip"},
    ],
}

BUILTIN_TEMPLATES = {
    "llm": _LLM_TEMPLATE,
    "core": copy.deepcopy(_LLM_TEMPLATE),
    "archive": {
        "desc": "Builtin archive template",
        "output": "data/snapshots/archive_{timestamp}.md",
        "make_zip": True,
        "zip_output": "data/snapshots/archive_{timestamp}.zip",
        "redact": True,
        "include_reports": True,
        "hash": "all",
        "embed_order": "path",
        "sections": dict(DEFAULT_SECTIONS),
        "inventory": {"enabled": True, "scope": "all", "limit": 2000},
        "max_file_bytes": 500000,
        "max_total_bytes": 20000000,
        "tree": {"max_depth": 30, "max_entries_per_dir": 1000},
        "include_globs": ["**/*"],
        "ignore_files": sorted(DEFAULT_IGNORE_FILES),
        "ignore_globs": list(DEFAULT_IGNORE_GLOBS),
        "rules": [{"match": "**/*", "mode": "full"}],
    },
}



@dataclass
class FileRecord:
    rel_posix: str
    abs_path: Path
    size_bytes: int
    sha256_12: str
    mode: str

    # Rule-derived knobs (cached per file to avoid repeated rule scans)
    head_lines: int = 200
    per_file_max_bytes: int = 200000

    # Render-time metadata
    rendered_bytes: int = 0
    truncated: bool = False
    note: str = ""
    approx_tokens: int = 0


def _now_ts() -> str:
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def _run_cmd(cmd: str) -> str:
    try:
        return subprocess.check_output(
            cmd,
            shell=True,
            text=True,
            cwd=str(PROJECT_ROOT),
            stderr=subprocess.DEVNULL,
        ).strip()
    except Exception:
        return "Unknown"


def get_system_fingerprint() -> str:
    info = []
    info.append(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    info.append(f"User: {os.getenv('USER', 'Unknown')}")
    info.append(f"Host: {platform.node()} ({platform.system()} {platform.release()})")
    info.append(f"Python: {platform.python_version()} ({sys.executable})")
    try:
        import rich  # type: ignore

        ver = getattr(rich, "__version__", "Installed (ver unknown)")
        info.append(f"Rich Ver: {ver}")
    except Exception:
        info.append("Rich Ver: Not Installed")
    return "\n".join(info)


def get_git_status() -> str:
    if not (PROJECT_ROOT / ".git").exists():
        return "Git: Not a repository"
    branch = _run_cmd("git rev-parse --abbrev-ref HEAD")
    commit = _run_cmd("git rev-parse --short HEAD")
    last_msg = _run_cmd("git log -1 --pretty=%B")
    is_dirty = _run_cmd("git status --porcelain") != ""
    dirty_mark = " [DIRTY]" if is_dirty else " [CLEAN]"
    return f"Branch: {branch}{dirty_mark}\nCommit: {commit}\nMessage: {last_msg}"


def _is_probably_binary(path: Path) -> bool:
    try:
        with open(path, "rb") as f:
            chunk = f.read(2048)
        if b"\x00" in chunk:
            return True
        # Heuristic: if too many non-text bytes, treat as binary.
        text_chars = bytearray({7, 8, 9, 10, 12, 13, 27} | set(range(0x20, 0x100)))
        nontext = chunk.translate(None, text_chars)
        return len(nontext) / max(len(chunk), 1) > 0.30
    except Exception:
        return True


_SECRET_KV_RE = re.compile(
    r"(?i)\b(password|passphrase|token|secret|api[_-]?key|client[_-]?secret|access[_-]?key)\b\s*[:=]\s*([^\n\r]+)"
)


def _redact(text: str) -> str:
    # Block private keys
    text = re.sub(
        r"-----BEGIN [A-Z ]*PRIVATE KEY-----.*?-----END [A-Z ]*PRIVATE KEY-----",
        "-----BEGIN PRIVATE KEY-----
<REDACTED>
-----END PRIVATE KEY-----",
        text,
        flags=re.DOTALL,
    )

    def _kv_sub(m: re.Match) -> str:
        key = m.group(1)
        return f"{key}=<REDACTED>"

    text = _SECRET_KV_RE.sub(_kv_sub, text)
    return text


def _sha256_12(path: Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 256), b""):
            h.update(chunk)
    return h.hexdigest()[:12]

DEFAULT_SHA_CACHE_PATH = PROJECT_ROOT / "data" / "snapshots" / ".snapshot_sha_cache.json"


def _load_sha_cache(cache_path: Path) -> Dict[str, Dict[str, Any]]:
    """Load SHA cache from disk (best-effort)."""
    try:
        if cache_path.exists():
            data = json.loads(cache_path.read_text(encoding="utf-8"))
            if isinstance(data, dict):
                # Ensure nested dicts
                out: Dict[str, Dict[str, Any]] = {}
                for k, v in data.items():
                    if isinstance(k, str) and isinstance(v, dict):
                        out[k] = v
                return out
    except Exception:
        pass
    return {}


def _save_sha_cache(cache_path: Path, cache: Dict[str, Dict[str, Any]]) -> None:
    """Persist SHA cache to disk (best-effort)."""
    try:
        _ensure_parent(cache_path)
        cache_path.write_text(json.dumps(cache, ensure_ascii=False, indent=2) + "\n", encoding="utf-8")
    except Exception:
        # Cache is purely an optimization; ignore failures
        return


def _sha256_12_cached(path: Path, rel_posix: str, cache: Dict[str, Dict[str, Any]]) -> str:
    """Compute sha256_12 with a simple stat-based cache."""
    try:
        st = path.stat()
        mtime_ns = int(getattr(st, "st_mtime_ns", int(st.st_mtime * 1e9)))
        size = int(st.st_size)
    except Exception:
        return "Unknown"

    entry = cache.get(rel_posix)
    if isinstance(entry, dict):
        try:
            if int(entry.get("mtime_ns", -1)) == mtime_ns and int(entry.get("size", -2)) == size:
                val = entry.get("sha256_12")
                if isinstance(val, str) and len(val) == 12:
                    return val
        except Exception:
            pass

    try:
        sha = _sha256_12(path)
    except Exception:
        sha = "Unknown"

    cache[rel_posix] = {"mtime_ns": mtime_ns, "size": size, "sha256_12": sha}
    return sha


def _estimate_tokens(text: str) -> int:
    """Rough token estimate (useful for context-window sizing).

    Heuristic:
    - ASCII chars ~ 1 token / 4 chars
    - Non-ASCII chars ~ 1 token / char (works better for CJK)
    """
    if not text:
        return 0
    n = len(text)
    non_ascii = sum(1 for ch in text if ord(ch) > 127)
    ascii_cnt = n - non_ascii
    # ceil-ish without math import
    return int(non_ascii + (ascii_cnt + 3) // 4)


def _posix_rel(path: Path) -> str:
    return path.relative_to(PROJECT_ROOT).as_posix()


def _match_glob(rel_posix: str, pattern: str) -> bool:
    """Glob matching with stable ** semantics.

    Semantics:
    - If pattern has no '/', treat it as a basename pattern.
    - '**' matches 0..N path segments (unlike pathlib.Path.match which may require >=1).
    - Other segments use fnmatch-style wildcards.
    """
    rel_posix = rel_posix.lstrip("/")
    pattern = str(pattern or "").lstrip("/")

    if not pattern:
        return False

    # Basename-only pattern: match only the filename.
    if "/" not in pattern:
        name = rel_posix.rsplit("/", 1)[-1]
        return fnmatch.fnmatchcase(name, pattern)

    path_parts = rel_posix.split("/") if rel_posix else []
    pat_parts = pattern.split("/") if pattern else []

    def rec(i: int, j: int) -> bool:
        if i == len(pat_parts):
            return j == len(path_parts)

        pat = pat_parts[i]

        if pat == "**":
            # Match zero segments
            if rec(i + 1, j):
                return True
            # Match one segment and keep '**' active
            return j < len(path_parts) and rec(i, j + 1)

        if j >= len(path_parts):
            return False

        if fnmatch.fnmatchcase(path_parts[j], pat):
            return rec(i + 1, j + 1)

        return False

    return rec(0, 0)

def _load_templates(config_path: Path) -> Dict[str, Any]:
    if config_path.exists():
        try:
            data = json.loads(config_path.read_text(encoding="utf-8"))
            if isinstance(data, dict) and isinstance(data.get("templates"), dict):
                return data
        except Exception:
            pass
    # Fallback
    return {
        "version": 1,
        "defaults": {
            "mode_to_template": {
                "llm": "llm",
                "core": "llm",
                "archive": "archive",
                "custom": "llm",
            }
        },
        "templates": BUILTIN_TEMPLATES,
    }


def _resolve_template(templates_doc: Dict[str, Any], mode: str, template_name: Optional[str]) -> Tuple[str, Dict[str, Any]]:
    templates = templates_doc.get("templates", {})
    if template_name:
        if template_name not in templates:
            raise SystemExit(f"Unknown template: {template_name}")
        return template_name, templates[template_name]

    defaults = templates_doc.get("defaults", {}).get("mode_to_template", {})
    picked = defaults.get(mode, mode)
    if picked in templates:
        return picked, templates[picked]

    if mode in templates:
        return mode, templates[mode]

    # Compatibility: allow llm/core aliasing when only one exists.
    if mode == "core" and "llm" in templates:
        return "llm", templates["llm"]
    if mode == "llm" and "core" in templates:
        return "core", templates["core"]

    # Fallback to core
    return "core", templates.get("core", BUILTIN_TEMPLATES["core"])


def _ensure_parent(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)


def _dedupe_list(items: Iterable[str]) -> List[str]:
    out: List[str] = []
    seen: set[str] = set()
    for item in items:
        s = str(item)
        if s in seen:
            continue
        seen.add(s)
        out.append(s)
    return out


def _normalize_focus_globs(raw: Iterable[str]) -> List[str]:
    globs: List[str] = []
    for val in raw:
        s = str(val).strip()
        if not s:
            continue
        s = s.replace("\\", "/").lstrip("/")
        if re.search(r"[*?\[]", s):
            globs.append(s)
            continue
        p = (PROJECT_ROOT / s).resolve()
        try:
            rel = p.relative_to(PROJECT_ROOT).as_posix()
        except Exception:
            rel = s
        if p.exists():
            if p.is_dir():
                globs.append(rel.rstrip("/") + "/**")
            else:
                globs.append(rel)
        else:
            globs.append(s)
    return _dedupe_list(globs)


def _simplify_include_globs(include_globs: List[str]) -> List[str]:
    globs = [str(g).strip() for g in (include_globs or []) if str(g).strip()]
    out: List[str] = []
    seen: set[str] = set()
    for g in globs:
        if g in seen:
            continue
        seen.add(g)
        out.append(g)

    # Common redundancy: if include-all is present, additional include globs are unnecessary
    if any(g in {"**", "**/*"} for g in out):
        return ["**/*"]

    return out


def _derive_prunable_ignore_prefixes(ignore_globs: List[str]) -> List[str]:
    """Best-effort directory pruning from ignore globs.

    If an ignore glob is of the form 'path/to/dir/**' (no wildcard in the prefix),
    we can prune that subtree during os.walk for speed.
    """
    prefixes: List[str] = []
    wildcard_re = re.compile(r"[*?\[]")
    for pat in ignore_globs or []:
        pat = str(pat).strip().lstrip("/")
        if not pat:
            continue
        if not pat.endswith("/**") and not pat.endswith("/**/*"):
            continue
        prefix = pat[:-3] if pat.endswith("/**") else pat[:-5]
        if not prefix:
            continue
        if wildcard_re.search(prefix):
            continue
        prefixes.append(prefix)
    # Longer prefixes first to match more specifically
    prefixes.sort(key=len, reverse=True)
    return prefixes


def _iter_candidates(
    include_globs: List[str],
    *,
    ignore_dirs: set[str],
    ignore_files: set[str],
    ignore_globs: List[str],
) -> List[Path]:
    """Iterate candidate files with a single filesystem walk.

    This is substantially faster than multiple Path.glob() passes when include_globs grows.
    """
    include_globs = _simplify_include_globs(include_globs)
    if not include_globs:
        return []

    include_all = include_globs == ["**/*"]

    # Separate basename patterns (no '/') vs full path patterns
    basename_pats: List[str] = []
    path_pats: List[str] = []
    if not include_all:
        for pat in include_globs:
            if "/" in pat:
                path_pats.append(pat)
            else:
                basename_pats.append(pat)

    prunable_prefixes = _derive_prunable_ignore_prefixes(ignore_globs)

    files: List[Path] = []

    for root, dirs, filenames in os.walk(PROJECT_ROOT, topdown=True, followlinks=False):
        # Rel path for pruning (posix)
        try:
            rel_root = Path(root).relative_to(PROJECT_ROOT).as_posix()
        except Exception:
            rel_root = ""

        # Prune by ignore_dirs (name-based)
        if dirs:
            dirs[:] = [d for d in dirs if d not in ignore_dirs]

        # Prune by ignore_globs-derived subtree prefixes
        if rel_root:
            for pref in prunable_prefixes:
                if rel_root == pref or rel_root.startswith(pref + "/"):
                    dirs[:] = []
                    filenames = []
                    break

        for name in filenames:
            if name in ignore_files:
                continue

            abs_path = Path(root) / name
            try:
                rel = abs_path.relative_to(PROJECT_ROOT).as_posix()
            except Exception:
                continue

            # Ignore globs (fast reject)
            ignored = False
            for pat in ignore_globs:
                if _match_glob(rel, pat):
                    ignored = True
                    break
            if ignored:
                continue

            # Include filter
            if include_all:
                files.append(abs_path)
                continue

            ok = False
            if basename_pats:
                for pat in basename_pats:
                    if fnmatch.fnmatchcase(name, pat):
                        ok = True
                        break
            if (not ok) and path_pats:
                for pat in path_pats:
                    if _match_glob(rel, pat):
                        ok = True
                        break
            if ok:
                files.append(abs_path)

    files.sort(key=lambda p: _posix_rel(p))
    return files


def _should_ignore(path: Path, ignore_files: set[str], ignore_globs: List[str], ignore_dirs: set[str]) -> bool:
    """Return True if a file path should be ignored."""
    try:
        rel = _posix_rel(path)
    except Exception:
        return True

    # Ignore by directory segment (relative path only)
    parts = rel.split("/")
    for seg in parts[:-1]:
        if seg in ignore_dirs:
            return True

    if path.name in ignore_files:
        return True

    for pat in ignore_globs:
        if _match_glob(rel, pat):
            return True

    return False


def _pick_rule(rel_posix: str, rules: List[Dict[str, Any]]) -> Dict[str, Any]:
    for r in rules:
        pat = r.get("match")
        if not pat:
            continue
        if _match_glob(rel_posix, pat):
            return r
    return {"mode": "skip"}


def _read_text_limited(path: Path, max_bytes: int) -> Tuple[str, bool]:
    # Returns (text, truncated)
    size = path.stat().st_size
    truncated = size > max_bytes
    with open(path, "rb") as f:
        data = f.read(max_bytes if truncated else size)
    try:
        text = data.decode("utf-8", errors="replace")
    except Exception:
        text = data.decode(errors="replace")
    return text, truncated


def _read_head_lines(path: Path, head_lines: int) -> Tuple[str, bool]:
    lines: List[str] = []
    truncated = False
    try:
        with open(path, "r", encoding="utf-8", errors="replace") as f:
            for i, line in enumerate(f):
                if i >= head_lines:
                    truncated = True
                    break
                lines.append(line.rstrip("\n"))
    except Exception:
        # binary or unreadable
        return "[Unreadable]", True
    return "\n".join(lines), truncated


def _safe_unparse(node: ast.AST) -> str:
    try:
        return ast.unparse(node)  # type: ignore[attr-defined]
    except Exception:
        return "..."


def _format_args(args: ast.arguments) -> str:
    parts: List[str] = []

    def fmt_arg(a: ast.arg, default: Optional[ast.AST]) -> str:
        s = a.arg
        if a.annotation is not None:
            s += f": {_safe_unparse(a.annotation)}"
        if default is not None:
            d = _safe_unparse(default)
            if len(d) > 40:
                d = d[:37] + "..."
            s += f"={d}"
        return s

    # Positional-only
    posonly = getattr(args, "posonlyargs", [])
    if posonly:
        defaults_pad = [None] * (len(posonly) + len(args.args) - len(args.defaults)) + list(args.defaults)
        for a, d in zip(posonly, defaults_pad[: len(posonly)]):
            parts.append(fmt_arg(a, d))
        parts.append("/")

    # Positional or keyword
    all_pos = list(args.args)
    defaults_pad = [None] * (len(posonly) + len(all_pos) - len(args.defaults)) + list(args.defaults)
    # defaults for args start after posonly
    for idx, a in enumerate(all_pos):
        d = defaults_pad[len(posonly) + idx]
        parts.append(fmt_arg(a, d))

    # vararg
    if args.vararg is not None:
        va = "*" + args.vararg.arg
        if args.vararg.annotation is not None:
            va += f": {_safe_unparse(args.vararg.annotation)}"
        parts.append(va)
    elif args.kwonlyargs:
        parts.append("*")

    # kw-only
    for a, d in zip(args.kwonlyargs, args.kw_defaults):
        parts.append(fmt_arg(a, d))

    # kwarg
    if args.kwarg is not None:
        ka = "**" + args.kwarg.arg
        if args.kwarg.annotation is not None:
            ka += f": {_safe_unparse(args.kwarg.annotation)}"
        parts.append(ka)

    return ", ".join(parts)


def _first_doc_line(doc: Optional[str], max_len: int = 120) -> str:
    if not doc:
        return ""
    line = doc.strip().splitlines()[0].strip()
    if len(line) > max_len:
        line = line[: max_len - 3] + "..."
    return line


def _extract_python_interface(path: Path, max_chars: int = 40000) -> str:
    src = path.read_text(encoding="utf-8", errors="replace")
    try:
        tree = ast.parse(src)
    except Exception as e:
        head, _ = _read_head_lines(path, 200)
        return f"# [Interface Extraction Failed]\n# {e}\n\n" + head

    out: List[str] = []
    mod_doc = ast.get_docstring(tree)
    if mod_doc:
        out.append('"""' + _first_doc_line(mod_doc) + '"""')
        out.append("")

    # Constants (simple Assign to ALLCAPS)
    consts: List[str] = []
    for node in tree.body:
        if isinstance(node, ast.Assign):
            for t in node.targets:
                if isinstance(t, ast.Name) and t.id.isupper():
                    try:
                        v = _safe_unparse(node.value)
                        if len(v) > 80:
                            v = v[:77] + "..."
                        consts.append(f"{t.id} = {v}")
                    except Exception:
                        consts.append(f"{t.id} = ...")
    if consts:
        out.append("# Constants")
        out.extend(consts[:40])
        if len(consts) > 40:
            out.append(f"# ... {len(consts)-40} more")
        out.append("")

    # Functions / Classes
    for node in tree.body:
        if isinstance(node, ast.FunctionDef):
            sig = _format_args(node.args)
            ret = f" -> {_safe_unparse(node.returns)}" if node.returns is not None else ""
            doc = _first_doc_line(ast.get_docstring(node))
            out.append(f"def {node.name}({sig}){ret}:")
            out.append(f"    \"\"\"{doc}\"\"\"" if doc else "    ...")
            out.append("")
        elif isinstance(node, ast.ClassDef):
            bases = [
                _safe_unparse(b)
                for b in node.bases
                if not (isinstance(b, ast.Name) and b.id == "object")
            ]
            base_s = f"({', '.join(bases)})" if bases else ""
            doc = _first_doc_line(ast.get_docstring(node))
            out.append(f"class {node.name}{base_s}:")
            out.append(f"    \"\"\"{doc}\"\"\"" if doc else "    ...")

            # Methods
            methods: List[str] = []
            for sub in node.body:
                if isinstance(sub, ast.FunctionDef):
                    if sub.name.startswith("__") and sub.name.endswith("__"):
                        continue
                    msig = _format_args(sub.args)
                    mret = f" -> {_safe_unparse(sub.returns)}" if sub.returns is not None else ""
                    mdoc = _first_doc_line(ast.get_docstring(sub))
                    line = f"    def {sub.name}({msig}){mret}:"
                    if mdoc:
                        line += f"  # {mdoc}"
                    methods.append(line)
            if methods:
                out.extend(methods[:60])
                if len(methods) > 60:
                    out.append(f"    # ... {len(methods)-60} more")
            out.append("")

    text = "\n".join(out).strip() + "\n"
    if len(text) > max_chars:
        return text[: max_chars - 200] + "\n\n# [TRUNCATED: interface too large]\n"
    return text


def _render_tree(root: Path, prefix: str, depth: int, max_depth: int, max_entries: int,
                 ignore_dirs: set[str], ignore_files: set[str], ignore_globs: List[str]) -> str:
    if depth > max_depth:
        return f"{prefix}└── ... (max depth reached)\n"

    try:
        children = [p for p in sorted(root.iterdir(), key=lambda p: p.name.lower())]
    except PermissionError:
        return f"{prefix}└── [Permission Denied]\n"

    # Filter ignored
    filtered: List[Path] = []
    for p in children:
        if p.name in ignore_dirs and p.is_dir():
            continue
        if p.name in ignore_files and p.is_file():
            continue
        rel = _posix_rel(p) if p.exists() else p.name
        if any(_match_glob(rel, pat) for pat in ignore_globs):
            continue
        filtered.append(p)

    omitted = 0
    if len(filtered) > max_entries:
        omitted = len(filtered) - max_entries
        filtered = filtered[:max_entries]

    lines: List[str] = []
    for i, p in enumerate(filtered):
        pointer = "├── " if i < len(filtered) - 1 else "└── "
        lines.append(f"{prefix}{pointer}{p.name}")
        if p.is_dir():
            extension = "│   " if pointer == "├── " else "    "
            sub = _render_tree(
                p,
                prefix=prefix + extension,
                depth=depth + 1,
                max_depth=max_depth,
                max_entries=max_entries,
                ignore_dirs=ignore_dirs,
                ignore_files=ignore_files,
                ignore_globs=ignore_globs,
            )
            lines.append(sub.rstrip("\n"))

    if omitted:
        lines.append(f"{prefix}└── ... ({omitted} entries omitted)")

    return "\n".join(lines) + "\n"


def _extract_registry_tools() -> Optional[List[Dict[str, Any]]]:
    reg_path = PROJECT_ROOT / "apps" / "cli" / "registry.py"
    if not reg_path.exists():
        return None
    try:
        src = reg_path.read_text(encoding="utf-8", errors="replace")
        tree = ast.parse(src)
        for node in tree.body:
            if isinstance(node, ast.Assign):
                for t in node.targets:
                    if isinstance(t, ast.Name) and t.id == "TOOLS":
                        return ast.literal_eval(node.value)  # type: ignore[arg-type]
    except Exception:
        return None
    return None


def _render_tools_overview(tools: Optional[List[Dict[str, Any]]]) -> str:
    if not tools:
        return "(registry tools not available)"

    # Keep it simple to control size.
    headers = ["alias", "file", "type", "desc", "usage"]
    rows: List[List[str]] = []
    for t in tools:
        rows.append([
            str(t.get("alias") or ""),
            str(t.get("file") or ""),
            str(t.get("type") or ""),
            str(t.get("desc") or ""),
            str(t.get("usage") or ""),
        ])

    # Column widths
    widths = [len(h) for h in headers]
    for r in rows:
        for i, cell in enumerate(r):
            widths[i] = min(48, max(widths[i], len(cell)))

    def fmt_row(r: List[str]) -> str:
        out = []
        for i, cell in enumerate(r):
            s = cell.replace("\n", " ").strip()
            if len(s) > widths[i]:
                s = s[: widths[i] - 3] + "..."
            out.append(s.ljust(widths[i]))
        return " | ".join(out)

    lines = [fmt_row(headers), "-+-".join(["-" * w for w in widths])]
    lines.extend(fmt_row(r) for r in rows)
    return "\n".join(lines)


def _render_project_status() -> str:
    status_path = PROJECT_ROOT / "PROJECT_STATUS.json"
    if not status_path.exists():
        return "No project status file found."

    try:
        data = json.loads(status_path.read_text(encoding="utf-8"))
    except Exception:
        return "Error reading project status."

    lines: List[str] = []

    manifesto = data.get("DEV_MANIFESTO") or data.get("guidelines") or []
    if isinstance(manifesto, list) and manifesto:
        lines.append("DEV MANIFESTO:")
        for rule in manifesto:
            lines.append(f"* {rule}")
        lines.append("-" * 20)

    objective = data.get("OBJECTIVE") or data.get("objective")
    lines.append(f"OBJECTIVE: {objective}")

    tasks_todo = data.get("TASKS_TODO")
    tasks_done = data.get("TASKS_DONE")
    if isinstance(tasks_todo, list) or isinstance(tasks_done, list):
        todo_list = tasks_todo if isinstance(tasks_todo, list) else []
        done_list = tasks_done if isinstance(tasks_done, list) else []
        if todo_list:
            lines.append("\nTASKS TODO:")
            for t in todo_list:
                lines.append(f"- {t}")
        if done_list:
            lines.append("\nTASKS DONE:")
            for t in done_list:
                lines.append(f"- {t}")
    else:
        tasks = data.get("tasks", [])
        if isinstance(tasks, list) and tasks:
            lines.append("\nTASKS:")
            for i, t in enumerate(tasks):
                if isinstance(t, dict):
                    mark = "[x]" if t.get("status") == "done" else "[ ]"
                    desc = t.get("desc")
                    lines.append(f"{i}. {mark} {desc}")
                else:
                    lines.append(f"- {t}")

    logs = data.get("RECENT_LOGS") or data.get("logs") or []
    if isinstance(logs, list) and logs:
        lines.append("\nRECENT LOGS:")
        for l in logs[-5:]:
            lines.append(f"- {l}")

    return "\n".join(lines)


def _render_file_inventory(records: List[FileRecord], limit: int = 500) -> str:
    # Inventory is useful for LLM context even if contents are truncated.
    headers = ["mode", "bytes", "sha256_12", "path"]

    rows: List[List[str]] = []
    for r in records[:limit]:
        rows.append([
            r.mode + ("*" if r.truncated else ""),
            str(r.size_bytes),
            r.sha256_12,
            r.rel_posix,
        ])
    if len(records) > limit:
        rows.append(["...", "", "", f"({len(records)-limit} more omitted)"])

    widths = [len(h) for h in headers]
    for row in rows:
        for i, cell in enumerate(row):
            widths[i] = min(80, max(widths[i], len(cell)))

    def fmt_row(row: List[str]) -> str:
        cells: List[str] = []
        for i, cell in enumerate(row):
            s = cell
            if len(s) > widths[i]:
                s = s[: widths[i] - 3] + "..."
            cells.append(s.ljust(widths[i]))
        return " | ".join(cells)

    lines = [fmt_row(headers), "-+-".join(["-" * w for w in widths])]
    lines.extend(fmt_row(r) for r in rows)
    return "\n".join(lines)


def _write_zip(zip_path: Path, records: List[FileRecord]) -> None:
    _ensure_parent(zip_path)
    with zipfile.ZipFile(zip_path, "w", compression=zipfile.ZIP_DEFLATED) as z:
        # Include a manifest
        manifest = {
            "generated": datetime.now().isoformat(timespec="seconds"),
            "project_root": str(PROJECT_ROOT),
            "files": [
                {
                    "path": r.rel_posix,
                    "mode": r.mode,
                    "size_bytes": r.size_bytes,
                    "sha256_12": r.sha256_12,
                }
                for r in records
                if r.mode != "skip"
            ],
        }
        z.writestr("manifest.json", json.dumps(manifest, ensure_ascii=False, indent=2))

        # Add raw files
        for r in records:
            if r.mode == "skip":
                continue
            # Only archive repo files (text), skip binaries defensively.
            if _is_probably_binary(r.abs_path):
                continue
            z.write(r.abs_path, arcname=r.rel_posix)


def main() -> None:
    parser = argparse.ArgumentParser(description="Wagstaff-Lab snapshot generator (llm/archive/custom via templates).")
    parser.add_argument("--mode", choices=["llm", "core", "archive", "custom"], default="llm")
    parser.add_argument("--template", help="Template name from conf/snapshot_templates.json")
    parser.add_argument("--config", default=str(DEFAULT_CONFIG), help="Path to snapshot template config JSON")
    parser.add_argument("--output", help="Override output path")
    parser.add_argument("--list-templates", action="store_true", help="List available templates and exit")
    parser.add_argument("--focus", action="append", help="Limit snapshot to specific paths/globs (repeatable)")
    parser.add_argument("--no-redact", action="store_true", help="Disable secret redaction")
    parser.add_argument("--zip", action="store_true", help="Force creating zip bundle when supported")
    parser.add_argument("--hash", choices=["all", "embedded", "none"], help="Override hashing mode (all/embedded/none)")
    parser.add_argument("--no-env", action="store_true", help="Disable environment diagnostics section")
    parser.add_argument("--no-overview", action="store_true", help="Disable project overview section")
    parser.add_argument("--no-tree", action="store_true", help="Disable project tree section")
    parser.add_argument("--no-inventory", action="store_true", help="Disable file inventory section")
    parser.add_argument("--no-contents", action="store_true", help="Disable file contents section")
    parser.add_argument("--no-stats", action="store_true", help="Disable snapshot stats section")
    parser.add_argument("--verbose", action="store_true", help="Include template config section")
    parser.add_argument("--plan", action="store_true", help="Print a JSON plan summary and exit (no snapshot written)")

    args = parser.parse_args()

    t0 = time.perf_counter()

    cfg_path = Path(args.config)
    templates_doc = _load_templates(cfg_path)

    if args.list_templates:
        tpls = templates_doc.get("templates", {})
        print("Available templates:")
        for name, t in sorted(tpls.items(), key=lambda x: x[0]):
            print(f"- {name}: {t.get('desc', '')}")
        return

    tpl_name, tpl = _resolve_template(templates_doc, args.mode, args.template)

    ts = _now_ts()

    out_str = args.output or str(tpl.get("output", "project_context.txt"))
    out_str = out_str.replace("{timestamp}", ts)
    output_path = PROJECT_ROOT / out_str

    make_zip = bool(tpl.get("make_zip", False)) or bool(args.zip)
    zip_output = tpl.get("zip_output")
    zip_path: Optional[Path] = None
    if make_zip:
        if isinstance(zip_output, str) and zip_output:
            zip_path = PROJECT_ROOT / zip_output.replace("{timestamp}", ts)
        else:
            zip_path = output_path.with_suffix(".zip")

    # Template controls
    focus_globs = _normalize_focus_globs(args.focus or [])
    include_globs = _simplify_include_globs(list(tpl.get("include_globs") or []))
    if focus_globs:
        include_globs = _simplify_include_globs(
            ["PROJECT_STATUS.json", "README.md", "conf/settings.ini"] + focus_globs
        )

    ignore_dirs = set(DEFAULT_IGNORE_DIRS) | set(tpl.get("ignore_dirs") or [])
    ignore_files = set(tpl.get("ignore_files") or list(DEFAULT_IGNORE_FILES))
    ignore_globs = list(tpl.get("ignore_globs") or list(DEFAULT_IGNORE_GLOBS))

    # Always ignore generated artifacts to avoid snapshot recursion
    ignore_files.add(output_path.name)
    if zip_path is not None:
        ignore_files.add(zip_path.name)
        try:
            ignore_globs.append(zip_path.relative_to(PROJECT_ROOT).as_posix())
        except Exception:
            pass

    # Optional reports toggle (hard exclude)
    if not bool(tpl.get("include_reports", True)):
        ignore_globs.append("data/reports/**")

    redact_enabled = bool(tpl.get("redact", True)) and (not args.no_redact)

    max_file_bytes = int(tpl.get("max_file_bytes", 200000))
    max_total_bytes = int(tpl.get("max_total_bytes", 1200000))

    tree_cfg = tpl.get("tree") or {}
    tree_max_depth = int(tree_cfg.get("max_depth", 8))
    tree_max_entries = int(tree_cfg.get("max_entries_per_dir", 250))

    rules = list(tpl.get("rules") or [])
    if focus_globs:
        focus_rules = [{"match": pat, "mode": "full"} for pat in focus_globs]
        rules = focus_rules + rules

    # Hashing / ordering / inventory knobs
    hash_mode = str(args.hash or tpl.get("hash") or tpl.get("hash_mode") or "all").strip().lower()
    if hash_mode not in {"all", "embedded", "none"}:
        hash_mode = "all"

    embed_order = str(tpl.get("embed_order", "path")).strip().lower()
    if embed_order not in {"path", "mode", "smart"}:
        embed_order = "path"

    pinned = list(tpl.get("pinned") or [])
    if focus_globs:
        pinned = _dedupe_list(focus_globs + pinned)

    tpl_sections = tpl.get("sections")
    if isinstance(tpl_sections, dict):
        sections = dict(DEFAULT_SECTIONS)
        for key, val in tpl_sections.items():
            if key in sections:
                sections[key] = bool(val)
    else:
        sections = dict(LLM_SECTIONS if args.mode in {"llm", "core"} else DEFAULT_SECTIONS)

    if args.verbose:
        sections["config"] = True
    if args.no_env:
        sections["env"] = False
    if args.no_overview:
        sections["overview"] = False
    if args.no_tree:
        sections["tree"] = False
    if args.no_inventory:
        sections["inventory"] = False
    if args.no_contents:
        sections["contents"] = False
    if args.no_stats:
        sections["stats"] = False

    inv_cfg = tpl.get("inventory") or {}
    inv_enabled = bool(inv_cfg.get("enabled", True)) and sections["inventory"]
    inv_scope = str(inv_cfg.get("scope", "all")).strip().lower()
    inv_limit = int(inv_cfg.get("limit", 700))

    sha_cache_path = DEFAULT_SHA_CACHE_PATH
    if isinstance(tpl.get("hash_cache"), str) and tpl.get("hash_cache"):
        sha_cache_path = PROJECT_ROOT / str(tpl.get("hash_cache"))

    sha_cache: Dict[str, Dict[str, Any]] = {}
    if hash_mode in {"all", "embedded"}:
        sha_cache = _load_sha_cache(sha_cache_path)

    if not args.plan:
        print(f"📸 Generating snapshot: mode={args.mode}, template={tpl_name}, output={output_path}")

    # 1) Collect candidates (single walk, already filtered by ignore_files/ignore_globs/ignore_dirs)
    t_scan0 = time.perf_counter()
    candidates = _iter_candidates(
        include_globs,
        ignore_dirs=ignore_dirs,
        ignore_files=ignore_files,
        ignore_globs=ignore_globs,
    )
    t_scan_ms = int((time.perf_counter() - t_scan0) * 1000)

    records: List[FileRecord] = []
    t_rec0 = time.perf_counter()
    for p in candidates:
        rel = _posix_rel(p)
        rule = _pick_rule(rel, rules)
        mode = str(rule.get("mode", "skip")).strip() or "skip"
        if mode not in {"full", "interface", "head", "skip"}:
            mode = "skip"

        try:
            size = p.stat().st_size
        except Exception:
            continue

        head_lines = int(rule.get("head_lines", 200))
        per_file = int(rule.get("max_file_bytes", max_file_bytes))

        sha = "-"
        if hash_mode == "all" and mode != "skip":
            sha = _sha256_12_cached(p, rel, sha_cache)

        records.append(
            FileRecord(
                rel_posix=rel,
                abs_path=p,
                size_bytes=size,
                sha256_12=sha,
                mode=mode,
                head_lines=head_lines,
                per_file_max_bytes=per_file,
            )
        )

    t_rec_ms = int((time.perf_counter() - t_rec0) * 1000)

    # Plan mode: emit JSON and exit (no file I/O except optional SHA cache)
    if args.plan:
        by_mode: Dict[str, Dict[str, int]] = {}
        for r in records:
            d = by_mode.setdefault(r.mode, {"count": 0, "size_bytes": 0})
            d["count"] += 1
            d["size_bytes"] += int(r.size_bytes)

        non_skip = [r for r in records if r.mode != "skip"]
        top_large = sorted(non_skip, key=lambda r: r.size_bytes, reverse=True)[:20]

        plan = {
            "mode": args.mode,
            "template": tpl_name,
            "config_file": str(cfg_path),
            "output": str(output_path.relative_to(PROJECT_ROOT)),
            "zip": {
                "enabled": bool(zip_path is not None),
                "output": str(zip_path.relative_to(PROJECT_ROOT)) if zip_path is not None else None,
            },
            "redact_enabled": redact_enabled,
            "hash_mode": hash_mode,
            "embed_order": embed_order,
            "sections": sections,
            "focus": focus_globs,
            "limits": {"max_file_bytes": max_file_bytes, "max_total_bytes": max_total_bytes},
            "tree": {"max_depth": tree_max_depth, "max_entries_per_dir": tree_max_entries},
            "include_globs": include_globs,
            "ignore_dirs": sorted(ignore_dirs),
            "ignore_files": sorted(ignore_files),
            "ignore_globs": ignore_globs,
            "rules_count": len(rules),
            "total_candidates": len(candidates),
            "included_records": len(records),
            "by_mode": by_mode,
            "timing_ms": {"scan": t_scan_ms, "records": t_rec_ms, "total": int((time.perf_counter() - t0) * 1000)},
            "top_largest_non_skip": [
                {"path": r.rel_posix, "mode": r.mode, "size_bytes": r.size_bytes} for r in top_large
            ],
        }

        if hash_mode == "all":
            _save_sha_cache(sha_cache_path, sha_cache)

        print(json.dumps(plan, ensure_ascii=False, indent=2))
        return

    # 2) Build report
    report: List[str] = []
    report.append("# Wagstaff-Lab Project Snapshot")
    report.append("")
    report.append(f"- Generated: {datetime.now().isoformat(timespec='seconds')}")
    report.append(f"- Mode: {args.mode}")
    report.append(f"- Template: {tpl_name}")
    if focus_globs:
        report.append(f"- Focus: {', '.join(focus_globs)}")

    section_idx = 0

    def add_section(title: str) -> None:
        nonlocal section_idx
        section_idx += 1
        report.append(f"\n## {section_idx}. {title}")

    eff = {
        "mode": args.mode,
        "template": tpl_name,
        "config_file": str(cfg_path),
        "output": str(output_path.relative_to(PROJECT_ROOT)),
        "zip": {
            "enabled": bool(zip_path is not None),
            "output": str(zip_path.relative_to(PROJECT_ROOT)) if zip_path is not None else None,
        },
        "redact_enabled": redact_enabled,
        "hash_mode": hash_mode,
        "embed_order": embed_order,
        "limits": {"max_file_bytes": max_file_bytes, "max_total_bytes": max_total_bytes},
        "tree": {"max_depth": tree_max_depth, "max_entries_per_dir": tree_max_entries},
        "glob_semantics": {"**": "matches 0..N path segments"},
        "include_globs": include_globs,
        "ignore_dirs": sorted(ignore_dirs),
        "ignore_files": sorted(ignore_files),
        "ignore_globs": ignore_globs,
        "rules": rules,
        "sections": sections,
        "focus": focus_globs,
        "inventory": {"enabled": inv_enabled, "scope": inv_scope, "limit": inv_limit},
        "pinned": pinned,
    }

    if sections["config"]:
        add_section("Effective Template Config")
        report.append("```json")
        report.append(json.dumps(eff, ensure_ascii=False, indent=2))
        report.append("```")

    if sections["env"]:
        add_section("Environment Diagnostics")
        report.append("```yaml")
        report.append(get_system_fingerprint())
        report.append("-" * 20)
        report.append(get_git_status())
        report.append("```")

    if sections["overview"]:
        add_section("Project Overview")
        report.append("### Toolbox (apps/cli/registry.py)")
        report.append("```text")
        report.append(_render_tools_overview(_extract_registry_tools()))
        report.append("```")

        report.append("\n### Project Context (PROJECT_STATUS.json)")
        report.append("```text")
        report.append(_render_project_status())
        report.append("```")

    if sections["tree"]:
        add_section("Project Structure")
        report.append("```text")
        report.append(
            _render_tree(
                PROJECT_ROOT,
                prefix="",
                depth=0,
                max_depth=tree_max_depth,
                max_entries=tree_max_entries,
                ignore_dirs=ignore_dirs,
                ignore_files=ignore_files,
                ignore_globs=ignore_globs,
            ).rstrip("\n")
        )
        report.append("```")

    if inv_enabled:
        add_section("File Inventory")
        report.append("(mode: full/interface/head/skip; '*' means truncated when rendered)\n")
        report.append("```text")
        inv_records: List[FileRecord]
        if inv_scope in {"included", "non_skip", "nonskip"}:
            inv_records = [r for r in records if r.mode != "skip"]
        else:
            inv_records = records
        report.append(_render_file_inventory(inv_records, limit=inv_limit))
        report.append("```")

    t_render_ms = 0
    budget = max_total_bytes
    embedded_files = 0
    approx_total_tokens = 0

    if sections["contents"]:
        add_section("File Contents")

        # 3) Emit contents within budget
        t_render0 = time.perf_counter()

        embed_records = [r for r in records if r.mode != "skip"]

        def mode_prio(m: str) -> int:
            return {"full": 0, "interface": 1, "head": 2}.get(m, 9)

        if embed_order == "mode":
            embed_records.sort(key=lambda r: (mode_prio(r.mode), r.rel_posix))
        elif embed_order == "smart":
            def pin_rank(r: FileRecord) -> int:
                if not pinned:
                    return 10_000
                for i, pat in enumerate(pinned):
                    if pat and _match_glob(r.rel_posix, str(pat)):
                        return i
                return 10_000
            embed_records.sort(key=lambda r: (pin_rank(r), mode_prio(r.mode), r.rel_posix))
        # else: "path" keeps as-is (already sorted by path)

        for rec in embed_records:
            if _is_probably_binary(rec.abs_path):
                rec.note = "[skipped: binary]"
                continue

            mode = rec.mode
            content = ""
            truncated = False

            if mode == "head":
                content, truncated = _read_head_lines(rec.abs_path, head_lines=rec.head_lines)
            elif mode == "interface":
                if rec.abs_path.suffix.lower() == ".py":
                    content = _extract_python_interface(rec.abs_path)
                    truncated = False
                else:
                    content, truncated = _read_head_lines(rec.abs_path, head_lines=rec.head_lines)
            elif mode == "full":
                content, truncated = _read_text_limited(rec.abs_path, max_bytes=rec.per_file_max_bytes)
            else:
                continue

            if redact_enabled:
                content = _redact(content)

            # Optionally hash only embedded files
            if hash_mode == "embedded" and rec.sha256_12 == "-":
                rec.sha256_12 = _sha256_12_cached(rec.abs_path, rec.rel_posix, sha_cache)

            # Rough byte count for budget
            render_blob = content.encode("utf-8", errors="replace")
            needed = len(render_blob)

            # Keep a small overhead for section headers
            needed += 200

            if budget - needed < 0:
                rec.note = "[omitted: total budget exceeded]"
                continue

            budget -= needed
            rec.rendered_bytes = needed
            rec.truncated = truncated

            tok = _estimate_tokens(content)
            rec.approx_tokens = tok
            approx_total_tokens += tok

            embedded_files += 1

            report.append(f"\n### File: {rec.rel_posix}")
            report.append(f"- mode: {mode}")
            report.append(f"- size_bytes: {rec.size_bytes}")
            report.append(f"- sha256_12: {rec.sha256_12}")
            if truncated:
                report.append("- note: TRUNCATED")
            if rec.note and rec.note not in {"[skipped: binary]"}:
                report.append(f"- note: {rec.note}")
            report.append("")

            # code fence lang
            lang = rec.abs_path.suffix.lstrip(".")
            if lang == "ini":
                lang = "toml"
            if mode == "interface":
                lang = "py"

            report.append(f"```{lang}")
            report.append(content.rstrip("\n"))
            report.append("```")

        t_render_ms = int((time.perf_counter() - t_render0) * 1000)

    if sections["stats"]:
        add_section("Snapshot Stats")
        report.append("```yaml")
        report.append(f"total_candidates: {len(candidates)}")
        report.append(f"included_records: {len(records)}")
        report.append(f"embedded_files: {embedded_files}")
        report.append(f"hash_mode: {hash_mode}")
        report.append(f"embed_order: {embed_order}")
        report.append(f"timing_ms: {{scan: {t_scan_ms}, records: {t_rec_ms}, render: {t_render_ms}, total: {int((time.perf_counter() - t0) * 1000)}}}")
        report.append(f"approx_total_tokens: {approx_total_tokens}")
        report.append(f"max_total_bytes: {max_total_bytes}")
        report.append(f"bytes_remaining: {budget}")
        report.append("```")

    # 4) Write output
    _ensure_parent(output_path)
    output_path.write_text("\n".join(report) + "\n", encoding="utf-8")
    print(f"✅ Snapshot written: {output_path}")

    # Persist SHA cache (optimization only)
    if hash_mode in {"all", "embedded"}:
        _save_sha_cache(sha_cache_path, sha_cache)

    # 5) Optional zip bundle
    if zip_path is not None:
        try:
            _write_zip(zip_path, records)
            print(f"✅ Zip bundle written: {zip_path}")
        except Exception as e:
            print(f"⚠️  Zip bundle failed: {e}")


if __name__ == "__main__":
    main()
```

### File: devtools/__init__.py
- mode: interface
- size_bytes: 48
- sha256_12: 730483be1b04

```py
"""Devtools package."""
```

### File: devtools/build_cache.py
- mode: interface
- size_bytes: 2553
- sha256_12: dadaa666e6b2

```py
# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent
DEFAULT_CACHE_PATH = PROJECT_ROOT / 'data' / 'index' / '.build_cache.json'

def load_cache(path: Optional[Path]=None) -> Dict[str, Any]:
    ...

def save_cache(cache: Dict[str, Any], path: Optional[Path]=None) -> None:
    ...

def file_sig(path: Path) -> Dict[str, Any]:
    ...

def files_sig(paths: Iterable[Path], *, label: str='') -> Dict[str, Any]:
    ...

def paths_sig(paths: Iterable[Path]) -> List[Dict[str, Any]]:
    ...

def dir_sig(path: Path, *, suffixes: Optional[Iterable[str]]=None, glob: str='**/*', label: str='') -> Dict[str, Any]:
    ...
```

### File: devtools/build_catalog_index.py
- mode: interface
- size_bytes: 2031
- sha256_12: 4dd79b3fb540

```py
"""Build catalog index (compact listing + indexes)."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent

def main() -> int:
    ...
```

### File: devtools/build_catalog_sqlite.py
- mode: interface
- size_bytes: 24035
- sha256_12: 31af5764fb1a

```py
"""Build SQLite catalog from wagstaff_catalog_v2.json."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent
DB_SCHEMA_VERSION = 4

def _load_json(path: Path) -> Dict[str, Any]:
    ...

def _json_dumps(value: Any) -> str:
    ...

def _load_trace(path: Optional[Path]) -> Dict[str, Any]:
    ...

def _as_list(value: Any) -> List[str]:
    ...

def _iter_map_rows(obj: Any) -> Iterable[Tuple[str, str]]:
    ...

def _iter_craft_meta_rows(craft_obj: Any) -> Iterable[Tuple[str, str]]:
    ...

def _iter_item_rows(items_obj: Any, assets_obj: Any) -> Iterable[Tuple[str, str, str, str, str, str, str, str, str, str, str, str]]:
    ...

def _iter_asset_rows(assets_obj: Any) -> Iterable[Tuple[str, str, str, str, str, str, str, str]]:
    ...

def _iter_item_list_rows(items_obj: Any) -> Iterable[Tuple[str, str]]:
    ...

def _iter_item_stat_rows(items_obj: Any) -> Iterable[Tuple[str, str, str, str, str, str, str]]:
    ...

def _iter_join_rows(items_obj: Any, field: str) -> Iterable[Tuple[str, str]]:
    ...

def _iter_craft_recipe_rows(craft_obj: Dict[str, Any]) -> Iterable[Tuple[str, str, str, str, str, str, str, str, str]]:
    ...

def _iter_craft_ingredient_rows(craft_obj: Dict[str, Any]) -> Iterable[Tuple[str, str, Optional[float], Optional[float], str]]:
    ...

def _iter_cooking_rows(cooking_obj: Any) -> Iterable[Tuple[str, float, float, str, str, str, str, str, str, str, str, str]]:
    ...

def _iter_cooking_ingredient_rows(cooking_obj: Any) -> Iterable[Tuple[str, str, str, str, str]]:
    ...

def _iter_catalog_index_rows(items: List[Dict[str, Any]]) -> Iterable[Tuple[str, str, str, str, int, int, str, str, str, str, str, str, str]]:
    ...

def _build_sqlite(catalog_path: Path, out_path: Path, icon_index_path: Optional[Path], tuning_trace_path: Optional[Path]) -> None:
    ...

def main() -> int:
    ...
```

### File: devtools/build_catalog_v2.py
- mode: interface
- size_bytes: 6479
- sha256_12: af3b1c977d49

```py
"""Build Wagstaff catalog v2 (item-centric)."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent

def _resolve_dst_root(arg: str | None) -> str | None:
    ...

def _load_resource_index(path: Path) -> dict:
    ...

def main() -> int:
    ...

def _render_summary(catalog: WagstaffCatalogV2) -> str:
    ...
```

### File: devtools/build_farming_defs.py
- mode: interface
- size_bytes: 3327
- sha256_12: e8b2ff3ebd60

```py
"""Build farming defs index from DST scripts."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent

def _resolve_dst_root(arg: str | None) -> str | None:
    ...

def main() -> int:
    ...
```

### File: devtools/build_i18n_index.py
- mode: interface
- size_bytes: 9722
- sha256_12: adbc55ab6ae0

```py
"""Build Wagstaff i18n index (names + UI strings)."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent
_PO_CANDIDATES = {'zh': ['scripts/languages/chinese_s.po', 'languages/chinese_s.po']}

def _resolve_dst_root(arg: Optional[str]) -> Optional[str]:
    ...

def _read_po_from_zip(zip_path: Path, candidates: List[str]) -> Tuple[Optional[str], Optional[str], Optional[str]]:
    ...

def _read_po_from_dir(dir_path: Path, candidates: List[str]) -> Tuple[Optional[str], Optional[str], Optional[str]]:
    ...

def _file_sig(path: Path) -> str:
    ...

def _zip_sig(zip_path: Path, inner: str) -> str:
    ...

def _resolve_po(*, lang: str, po_path: Optional[str], scripts_zip: Optional[str], scripts_dir: Optional[str], dst_root: Optional[str]) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[str]]:
    """Return (po_source, inner, text, sig)."""

def _load_item_ids(catalog_path: Path, icon_index_path: Optional[Path]) -> List[str]:
    ...

def main() -> int:
    ...
```

### File: devtools/build_icons.py
- mode: interface
- size_bytes: 29929
- sha256_12: 51fdeac1b40a

```py
"""build_icons.py (v1.3) - reduce missing by supporting:"""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent

def _norm(p: str) -> str:
    ...

def _expand(p: Optional[str]) -> Optional[str]:
    ...

def _ensure_dir(p: Path) -> None:
    ...

def _default_catalog_path() -> Path:
    ...

def _default_out_dir() -> Path:
    ...

def _default_index_path() -> Path:
    ...

def _databundles_dir(dst_root: str) -> Path:
    ...

def _data_dir(dst_root: str) -> Path:
    ...

def _auto_bundles(dst_root: str) -> List[Path]:
    ...

def _read_ini_dst_root(ini_path: str) -> Optional[str]:
    ...

def _strip_tex_suffix(name: str) -> str:
    ...

def _is_inventory_atlas(path: str, extra_globs: Sequence[str]) -> bool:
    ...

def _is_images_xml(path: str) -> bool:
    ...

def _to_pil(img_obj):
    """Convert decoder output to PIL.Image.Image."""

def _unpremultiply_rgba(pil_img):
    """Unpremultiply alpha using numpy (fast)."""

class Bundle:
    ...

class BundleFS:
    ...
    def close(self) -> None:
    def resolve(self, path: str, *, base_dir: Optional[str]=None) -> Optional[Tuple[int, str]]:
    def read(self, resolved: Tuple[int, str]) -> Optional[bytes]:
    def scan_paths(self) -> Iterable[str]:

class LocalFS:
    """Read loose files under data_dir (DST_ROOT/data)."""
    def _build_index(self) -> None:
    def resolve(self, path: str, *, base_dir: Optional[str]=None) -> Optional[Path]:
    def read(self, resolved: Path) -> Optional[bytes]:
    def scan_inventory_atlas_xmls(self, extra_globs: Sequence[str], *, include_all_images: bool=False) -> List[str]:  # Return paths relative to data_dir, like 'images/inventoryimages.xml'

class ResourceFS:
    """Combined resolver:"""
    def resolve(self, path: str, *, base_dir: Optional[str]=None) -> Optional[Tuple[str, Union[Tuple[int, str], Path]]]:
    def read(self, resolved: Tuple[str, Union[Tuple[int, str], Path]]) -> Optional[bytes]:

def _parse_atlas_xml(xml_bytes: bytes) -> Tuple[Optional[str], Dict[str, Dict[str, float]]]:
    ...

def _crop_uv(tex_img_pil, uv: Dict[str, float], *, invert_v: bool=True):
    """Crop using atlas UVs."""

def _collect_catalog_ids(catalog_path: Path) -> Set[str]:
    ...

def main() -> None:
    ...
```

### File: devtools/build_index_manifest.py
- mode: interface
- size_bytes: 5866
- sha256_12: b3087b84a361

```py
"""Build an index manifest for generated artifacts."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent
INDEX_ARTIFACTS = [{'id': 'resource_index', 'kind': 'resource', 'format': 'json', 'path': 'data...
META_KEYS = ('schema', 'project_version', 'index_version', 'generated', 'tool', 'db_schem...

def _parse_value(value: Any) -> Any:
    ...

def _extract_meta(meta: Dict[str, Any]) -> Dict[str, Any]:
    ...

def _load_json_meta(path: Path) -> Dict[str, Any]:
    ...

def _load_sqlite_meta(path: Path) -> Dict[str, Any]:
    ...

def _build_manifest(*, include_missing: bool) -> Dict[str, Any]:
    ...

def main() -> int:
    ...
```

### File: devtools/build_mechanism_index.py
- mode: interface
- size_bytes: 22780
- sha256_12: 75ed6486d405

```py
"""Build mechanism index (components + prefab links)."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent
DB_SCHEMA_VERSION = 4

def _resolve_dst_root(arg: str | None) -> str | None:
    ...

def _write_sqlite(doc: dict, path: Path) -> None:
    ...

def _validate_index(doc: dict) -> List[str]:
    ...

def _load_json_file(path: Path) -> Dict[str, Any]:
    ...

def _component_ids(doc: Dict[str, Any]) -> Set[str]:
    ...

def _prefab_ids(doc: Dict[str, Any]) -> Set[str]:
    ...

def _link_pairs(doc: Dict[str, Any]) -> Set[Tuple[str, str]]:
    ...

def _counts(doc: Dict[str, Any]) -> Dict[str, int]:
    ...

def _print_section(title: str, items: List[str], limit: int) -> None:
    ...

def _run_build(args: argparse.Namespace) -> int:
    ...

def _run_validate(args: argparse.Namespace) -> int:
    ...

def _run_diff(args: argparse.Namespace) -> int:
    ...

def main(argv: Optional[List[str]]=None) -> int:
    ...
```

### File: devtools/build_resource_index.py
- mode: interface
- size_bytes: 5320
- sha256_12: a47bde51cd90

```py
"""Build resource index from DST scripts + data folders."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent

def _resolve_dst_root(arg: str | None) -> str | None:
    ...

def main() -> int:
    ...
```

### File: devtools/catalog_quality.py
- mode: interface
- size_bytes: 14996
- sha256_12: 7e13eec0b0fb

```py
"""Catalog quality/coverage report."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent

def _load_json(path: Path) -> Dict[str, Any]:
    ...

def _collect_component_keys() -> Dict[str, Set[str]]:
    ...

def _norm_id(x: str) -> str:
    ...

def _trace_keys(trace_doc: Dict[str, Any]) -> Set[str]:
    ...

def _stat_trace_key(item_id: str, stat_key: str, entry: Dict[str, Any]) -> str:
    ...

def _sample(items: Set[str], limit: int=20) -> List[str]:
    ...

def build_report(*, catalog_doc: Dict[str, Any], icon_index: Dict[str, str], i18n_doc: Dict[str, Any], trace_doc: Dict[str, Any]) -> Dict[str, Any]:
    ...

def render_report_md(doc: Dict[str, Any]) -> str:
    ...

def main() -> int:
    ...
```

### File: devtools/portal_hub.py
- mode: interface
- size_bytes: 24138
- sha256_12: 602f6d81f1a3

```py
"""Portal hub: aggregate mgmt + quality + report + index status for CLI/web."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent
REPORT_DIR = PROJECT_ROOT / 'data' / 'reports'
PORTAL_MANIFEST_PATH = REPORT_DIR / 'wagstaff_portal_manifest.json'
PORTAL_INDEX_PATH = REPORT_DIR / 'portal_index.html'

def build_portal_manifest() -> Dict[str, Any]:
    ...

def render_portal_html(doc: Dict[str, Any]) -> str:
    ...

def write_portal_artifacts() -> Dict[str, Any]:
    ...

def _serve_portal(host: str, port: int, open_browser: bool) -> None:
    ...

def main() -> int:
    ...
```

### File: devtools/quality_gate.py
- mode: interface
- size_bytes: 13291
- sha256_12: ea888a6abf12

```py
"""Quality gate for Wagstaff artifacts (lightweight validation + report)."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent

def _load_json(path: Path) -> Optional[Dict[str, Any]]:
    ...

def _as_dict(v: Any) -> Dict[str, Any]:
    ...

def _as_list(v: Any) -> List[Any]:
    ...

def _ratio(n: int, d: int) -> float:
    ...

def _check_catalog(doc: Dict[str, Any], min_items: int) -> Tuple[Dict[str, Any], List[Tuple[str, str]]]:
    ...

def _check_catalog_index(doc: Dict[str, Any], items_total: int) -> Tuple[Dict[str, Any], List[Tuple[str, str]]]:
    ...

def _check_icon_index(doc: Dict[str, Any], min_icons: int) -> Tuple[Dict[str, Any], List[Tuple[str, str]]]:
    ...

def _check_i18n(doc: Dict[str, Any], items_total: int, min_ratio: float) -> Tuple[Dict[str, Any], List[Tuple[str, str]]]:
    ...

def _check_tuning_trace(doc: Dict[str, Any]) -> Tuple[Dict[str, Any], List[Tuple[str, str]]]:
    ...

def render_report(*, inputs: Dict[str, str], summary: Dict[str, Any], issues: List[Tuple[str, str]]) -> str:
    ...

def main() -> int:
    ...
```

### File: devtools/raw_scan.py
- mode: interface
- size_bytes: 29214
- sha256_12: 4066504f9eac

```py
"""DST raw data coverage scan."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent
REPORT_DIR = PROJECT_ROOT / 'data' / 'reports'
_ID_RE = re.compile('^[a-z0-9_]+$')

def _now_iso() -> str:
    ...

def _norm_id(val: str) -> str:
    ...

def _scan_inventory_icons(dst_root: Path) -> Tuple[Set[str], List[str]]:
    ...

def _scan_data_dir(dst_root: Path, *, top_n_ext: int=20, top_n_files: int=30, include_files: bool=False, max_files: int=0) -> Dict[str, Any]:
    ...

def _scan_data_bundles(dst_root: Path, *, include_entries: bool=False, max_entries: int=0) -> List[Dict[str, Any]]:
    ...

def _scan_scripts_overview(engine: WagstaffEngine) -> Dict[str, Any]:
    ...

def _collect_prefab_data(engine: WagstaffEngine) -> Dict[str, Any]:
    ...

def _coverage_counts(base: Set[str], target: Set[str]) -> Dict[str, Any]:
    ...

def _sample_missing(base: Set[str], target: Set[str], limit: int=40) -> Dict[str, Any]:
    ...

def _safe_str(val: Optional[str]) -> Optional[str]:
    ...

def _craft_sets(engine: WagstaffEngine) -> Dict[str, Set[str]]:
    ...

def _cooking_sets(engine: WagstaffEngine) -> Dict[str, Any]:
    ...

def build_report(engine: WagstaffEngine, *, dst_root: Path, top_n: int=25, data_full: bool=False, data_max_files: int=0, bundle_full: bool=False, bundle_max_files: int=0) -> Dict[str, Any]:
    ...

def render_md(doc: Dict[str, Any]) -> str:
    ...

def main() -> None:
    ...
```

### File: devtools/report_hub.py
- mode: interface
- size_bytes: 24028
- sha256_12: 0c3c7791def8

```py
"""Unified report hub (build/list/open)."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent
REPORT_DIR = PROJECT_ROOT / 'data' / 'reports'
MANIFEST_PATH = REPORT_DIR / 'wagstaff_report_manifest.json'
INDEX_PATH = REPORT_DIR / 'index.html'
REPORT_SPECS = [{'id': 'quality_gate', 'title': 'Quality Gate', 'kind': 'quality', 'files': ...

def _rel_path(path: Path) -> str:
    ...

def build_report_manifest() -> Dict[str, Any]:
    ...

def render_index_html(manifest: Dict[str, Any]) -> str:
    ...

def _resolve_dst_root(arg: Optional[str]) -> Optional[str]:
    ...

def _run_tool(args: List[str]) -> bool:
    ...

def _build_asset_registry(engine: WagstaffEngine, out_path: Path) -> None:
    ...

def _build_recipe_distribution(engine: WagstaffEngine, out_path: Path) -> None:
    ...

def build_reports(*, run_quality: bool, run_scan: bool, run_stats_gap: bool, dst_root: Optional[str]) -> None:
    ...

def write_manifest_and_index() -> Dict[str, Any]:
    ...

def _serve_reports(host: str, port: int, open_browser: bool) -> None:
    ...

def main() -> int:
    ...
```

### File: devtools/report_utils.py
- mode: interface
- size_bytes: 7650
- sha256_12: 8b2e940b23e4

```py
"""Shared helpers for report + portal hubs."""

def now_iso() -> str:
    ...

def read_text(path: Path) -> str:
    ...

def load_json(path: Path) -> Dict[str, Any]:
    ...

def file_info(path: Path) -> Dict[str, Any]:
    ...

def web_path(path: Path, base_dir: Path) -> Optional[str]:
    ...

def summarize_quality_gate(path: Path) -> Dict[str, Any]:
    ...

def summarize_catalog_quality(path: Path) -> Dict[str, Any]:
    ...

def summarize_index_manifest(path: Path) -> Dict[str, Any]:
    ...

def render_markdown(md_text: str) -> str:
    ...

def _render_inline(text: str) -> str:
    ...

def _split_table_row(line: str) -> List[str]:
    ...

def _is_table_sep(line: str) -> bool:
    ...

def _parse_table(lines: List[str], start: int) -> Optional[Tuple[int, List[str], List[List[str]]]]:
    ...

def _slug(text: str) -> str:
    ...
```

### File: devtools/sampler.py
- mode: interface
- size_bytes: 14614
- sha256_12: 44c1ca6632d6

```py
"""Wagstaff-Lab Sample Pack Generator"""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent
OUT_DIR = PROJECT_ROOT / 'data' / 'samples'
REPORT_DIR = PROJECT_ROOT / 'data' / 'reports'
DEFAULT_CATEGORIES = ['STRINGS', 'Widgets', 'Brains', 'Stategraphs', 'LootTables', 'Components']

def _now_iso() -> str:
    ...

def _now_ts() -> str:
    ...

def _sha256_12_text(text: str) -> str:
    ...

def _safe_read_asset_registry(path: Path) -> Dict[str, List[str]]:
    """Parse data/reports/asset_registry.md into {Category: [file1, file2, ...]}."""

def _file_size(engine: WagstaffEngine, path: str) -> int:
    ...

def _choose_files(engine: WagstaffEngine, category: str, n: int, registry_map: Dict[str, List[str]], rng: random.Random) -> List[str]:
    ...

def _pick_snippet_lines(lines: List[str], patterns: Sequence[re.Pattern], snippet_blocks: int, context_lines: int, rng: random.Random) -> List[Tuple[int, int]]:
    """Return a list of (start_idx, end_idx_exclusive) ranges."""

def _render_snippet(lines: List[str], start: int, end: int) -> str:
    ...

def _cap_text(text: str, max_chars: int) -> Tuple[str, bool]:
    ...

class FileSample:
    ...

def build_sample_pack(categories: List[str], n_files_per_category: int, head_lines: int, snippet_blocks: int, context_lines: int, max_chars_per_file: int, max_total_chars: int, seed: int) -> Tuple[str, Dict[str, object]]:
    ...

def main() -> None:
    ...
```

### File: devtools/serve_webcraft.py
- mode: interface
- size_bytes: 6049
- sha256_12: 180c7d164d0a

```py
"""Run WebCraft server (FastAPI + Uvicorn)."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent

def _detect_lan_ip() -> str:
    """Best-effort LAN IP discovery (no external network required)."""

def _find_sqlite_peer(path: Path) -> Path | None:
    ...

def main() -> None:
    ...
```

### File: devtools/snapshot_gui.py
- mode: interface
- size_bytes: 66112
- sha256_12: 7940229d33ef

```py
"""snapshot_gui.py"""

# Constants
MODE_FULL = 'full'
MODE_INTERFACE = 'interface'
MODE_HEAD = 'head'
MODE_SKIP = 'skip'
MODE_LABELS = {MODE_FULL: 'Full', MODE_INTERFACE: 'Interface', MODE_HEAD: 'Head', MODE_SKIP...
MODE_COLOR_TAG = {MODE_FULL: 'mode_full', MODE_INTERFACE: 'mode_interface', MODE_HEAD: 'mode_h...
PROFILE_MANUAL = 'Manual'
PROFILE_LLM_BEST = 'LLM Best (balanced)'
PROFILE_LLM_DEEP = 'LLM Deep (more code)'
PROFILE_ARCHIVE = 'Archive (full + zip)'
PROFILE_AUDIT = 'Audit (inventory all)'
PROFILE_ORDER = [PROFILE_MANUAL, PROFILE_LLM_BEST, PROFILE_LLM_DEEP, PROFILE_ARCHIVE, PROFILE...
_WILDCARD_CHARS_RE = re.compile('[*?\\[]')

def _find_project_root(start: Path) -> Path:
    """Best-effort project root discovery."""

def _guess_snapshot_py(project_root: Path) -> Optional[Path]:
    """Locate snapshot.py inside the project."""

def _load_snapshot_defaults(project_root: Path, snapshot_py: Optional[Path]) -> Tuple[set[str], set[str], List[str], Dict[str, Any]]:
    """Try to import snapshot.py to reuse DEFAULT_IGNORE_* and BUILTIN_TEMPLATES."""

class ModeSpec:
    ...
    def to_rule(self) -> Dict[str, Any]:

def _is_glob_like(pat: str) -> bool:
    ...

def _as_posix_rel(project_root: Path, abs_path: Path) -> str:
    ...

def _specificity_key(match_pat: str) -> Tuple[int, int, int]:
    """Sorting key for rules: more specific first."""

def _safe_int(s: str, default: int) -> int:
    ...

class SnapshotGUI(tk.Tk):
    ...
    def _build_styles(self) -> None:
    def _build_ui(self) -> None:
    def _status_text(self) -> str:
    def _rebuild_tree(self) -> None:
    def _on_open_node(self, event: tk.Event) -> None:
    def _populate_children_if_needed(self, item: str) -> None:
    def _populate_children(self, parent_item: str) -> None:
    def _effective_spec(self, rel: str) -> ModeSpec:  # Inheritance:
    def _update_item_display(self, item: str) -> None:
    def _update_loaded_subtree(self, item: str) -> None:
    def _on_select(self, event: tk.Event) -> None:
    def _selected_rels(self) -> List[str]:
    def _apply_mode(self, mode: str) -> None:
    def _clear_override(self) -> None:
    def _on_right_click(self, event: tk.Event) -> None:
    def _apply_profile_replace(self) -> None:  # Apply selected profile and REPLACE current overrides/settings.
    def _apply_profile_merge(self) -> None:  # Apply selected profile and MERGE into current overrides/settings.
    def _apply_profile(self, *, merge: bool) -> None:
    def _open_pinned_editor(self) -> None:  # Edit pinned paths (one per line).
    def _build_template_dict(self) -> Dict[str, Any]:  # Convert current GUI state into snapshot.py template dict.
    def _load_config_doc(self) -> Dict[str, Any]:
    def _merge_and_write_config(self, tpl_name: str, tpl_dict: Dict[str, Any]) -> None:
    def _save_config(self) -> None:
    def _preview_json(self) -> None:
    def _plan_snapshot(self) -> None:  # Run snapshot.py in --plan mode and show a compact summary.
    def _run_snapshot(self) -> None:
    def _try_load_existing_gui_template(self) -> None:  # Best-effort load:

def main() -> None:
    ...
```

### File: devtools/stats_gap_inspect.py
- mode: interface
- size_bytes: 13516
- sha256_12: 99fce71b728c

```py
"""Stats gap inspection for prefab assignments (heuristic)."""

# Constants
PROJECT_ROOT = Path(__file__).resolve().parent.parent

def _load_json(path: Path) -> Dict[str, Any]:
    ...

def _collect_component_keys() -> Dict[str, Set[str]]:
    ...

def _read_zip(zip_obj: Optional[zipfile.ZipFile], name: str) -> str:
    ...

def _read_file(path: Path) -> str:
    ...

def _load_prefab_source(*, zip_obj: Optional[zipfile.ZipFile], zip_paths: Set[str], rel_path: str) -> str:
    ...

def _extract_aliases(clean: str, comp: str) -> Set[str]:
    ...

def _classify_expr(expr: str) -> Set[str]:
    ...

def _inspect_component(*, comp: str, content: str, raw_lines: List[str]) -> List[Dict[str, Any]]:
    ...

def _summarize_item(records: List[Dict[str, Any]]) -> Dict[str, Any]:
    ...

def _build_report(*, catalog_doc: Dict[str, Any], components: List[str], scripts_zip: Optional[Path], max_records: int) -> Dict[str, Any]:
    ...

def _render_md(report: Dict[str, Any]) -> str:
    ...

def main() -> int:
    ...
```

### File: devtools/validators.py
- mode: interface
- size_bytes: 12630
- sha256_12: 3dd3a4cef1b1

```py
"""Validation helpers for Wagstaff artifacts (library-only)."""

# Constants
DB_SCHEMA_VERSION = 4

def _parse_int(value: Any) -> int | None:
    ...

def _fetch_tables(conn: sqlite3.Connection) -> Set[str]:
    ...

def _table_columns(conn: sqlite3.Connection, table: str) -> Set[str]:
    ...

def _load_meta(conn: sqlite3.Connection) -> Dict[str, Any]:
    ...

def _validate_meta(meta: Dict[str, Any], label: str) -> Tuple[Dict[str, Any], List[Tuple[str, str]]]:
    ...

def _validate_columns(*, label: str, tables: Set[str], conn: sqlite3.Connection, required: Dict[str, Set[str]], optional: Dict[str, Set[str]]) -> List[Tuple[str, str]]:
    ...

def validate_sqlite_v4(path: Path, *, kind: str) -> Tuple[Dict[str, Any], List[Tuple[str, str]]]:
    ...

def validate_mechanism_index(doc: Dict[str, Any]) -> Dict[str, List[str]]:
    ...
```

### File: data/reports/asset_registry.md
- mode: head
- size_bytes: 2096
- sha256_12: 45c762a51673

```md
# Wagstaff Asset Registry

| Category | Total Definitions | Top File |
|----------|-------------------|----------|
| Prefabs | 2381 | `scripts/prefabs/meats.lua` |
| Widgets | 2096 | `scripts/widgets/controls.lua` |
| LootTables | 183 | `scripts/prefabs/deciduoustrees.lua` |
| Brains | 103 | `scripts/prefabs/pigman.lua` |
| STRINGS | 45 | `scripts/strings.lua` |

## Detailed Breakdown

### Prefabs
- `scripts/prefabs/meats.lua`: 30
- `scripts/prefabs/boat.lua`: 19
- `scripts/prefabs/rocks.lua`: 13
- `scripts/prefabs/archive_props.lua`: 13
- `scripts/prefabs/explode_small.lua`: 10
- `scripts/prefabs/staff.lua`: 9
- `scripts/prefabs/planted_tree.lua`: 8
- `scripts/prefabs/hound.lua`: 8
- `scripts/prefabs/wagstaff_npc.lua`: 8
- `scripts/prefabs/spider.lua`: 8

### Widgets
- `scripts/widgets/controls.lua`: 35
- `scripts/screens/playerhud.lua`: 32
- `scripts/screens/lobbyscreen.lua`: 16
- `scripts/screens/multiplayermainscreen.lua`: 15
- `scripts/widgets/redux/templates.lua`: 14
- `scripts/widgets/statusdisplays.lua`: 14
- `scripts/screens/serverlistingscreen.lua`: 14
- `scripts/screens/redux/multiplayermainscreen.lua`: 14
- `scripts/screens/servercreationscreen.lua`: 14
- `scripts/screens/modconfigurationscreen.lua`: 13

### LootTables
- `scripts/prefabs/deciduoustrees.lua`: 7
- `scripts/prefabs/evergreens.lua`: 7
- `scripts/prefabs/moon_altar.lua`: 6
- `scripts/prefabs/farm_plants.lua`: 5
- `scripts/prefabs/merm.lua`: 4
- `scripts/prefabs/veggies.lua`: 4
- `scripts/prefabs/perdshrine.lua`: 4
- `scripts/prefabs/pigman.lua`: 4
- `scripts/prefabs/monkey.lua`: 4
- `scripts/prefabs/oceantree.lua`: 4

### Brains
- `scripts/prefabs/pigman.lua`: 4
- `scripts/prefabs/merm.lua`: 2
- `scripts/prefabs/wobster.lua`: 2
- `scripts/brains/wobysmallbrain.lua`: 2
- `scripts/prefabs/ghost.lua`: 2
- `scripts/prefabs/slurtle.lua`: 2
- `scripts/prefabs/spider.lua`: 2
- `scripts/prefabs/monkey.lua`: 2
- `scripts/brains/catcoonbrain.lua`: 1
- `scripts/brains/pigbrain.lua`: 1

### STRINGS
- `scripts/strings.lua`: 37
- `scripts/skin_strings.lua`: 7
- `scripts/strings_pretranslated.lua`: 1
```

### File: data/reports/catalog_index_summary.md
- mode: head
- size_bytes: 198
- sha256_12: 58b64665db95

```md
# Wagstaff Catalog Index Summary

## Meta
```yaml
schema_version: 1
catalog_schema: 2
scripts_sha256_12: b975ad93d5d8
```

## Counts
```yaml
items_total: 3085
items_with_icon: 1647
icon_only: 0
```
```

### File: data/reports/catalog_quality_report.md
- mode: head
- size_bytes: 3351
- sha256_12: 02158681fd99

```md
# Catalog Quality Report

## Counts
```yaml
items_total: 3085
assets_total: 1096
icons_total: 3106
all_ids_total: 4968
items_with_stats: 1392
stats_total: 4138
```

## Stats Coverage (by component)
```yaml
equippable: 143/255 (56.08%)
sanityaura: 142/190 (74.74%)
rechargeable: 18/23 (78.26%)
combat: 287/366 (78.42%)
stackable: 278/332 (83.73%)
heater: 31/34 (91.18%)
locomotor: 306/333 (91.89%)
planardamage: 98/103 (95.15%)
waterproofer: 30/31 (96.77%)
edible: 168/173 (97.11%)
finiteuses: 125/128 (97.66%)
workable: 450/458 (98.25%)
health: 340/346 (98.27%)
weapon: 153/153 (100.00%)
perishable: 139/139 (100.00%)
fueled: 88/88 (100.00%)
armor: 20/20 (100.00%)
insulator: 17/17 (100.00%)
hunger: 13/13 (100.00%)
sanity: 1/1 (100.00%)
```

## Stats Missing Items (sample)
```yaml
equippable: missing=112 sample=[axe, balloon, balloonparty, balloonparty_buff, balloonparty_confetti_balloon, balloonparty_confetti_cloud, batbat, batbat_bats, batbat_fantasy_fx, beeswax_spray]
combat: missing=79 sample=[babybeefalo, beehive, bernie_active, birds, boatrace_seastack, boatrace_seastack_monkey, butterfly, canary_poisoned, carrat, carrat_planted]
stackable: missing=54 sample=[bee, bilesplat, blowdart_fire, blowdart_pipe, blowdart_sleep, blowdart_walrus, blowdart_yellow, bomb_lunarplant, bomb_lunarplant_explode_fx, boneshard]
sanityaura: missing=48 sample=[alterguardian_phase1, alterguardian_phase1_lunarrift, alterguardian_phase1_lunarrift_gestalt, alterguardian_phase2, alterguardian_phase3, alterguardian_phase4_lunarrift, alterguardian_phase4_lunarrift_erupt_fx, alterguardian_phase4_lunarrift_slam_fx, bearger, beequeen]
locomotor: missing=27 sample=[bee, bilesplat, birds, bomb_lunarplant, bomb_lunarplant_explode_fx, buff_firefrenzy, butterfly, cannonball_rock, cannonball_rock_item, chum]
workable: missing=8 sample=[campfire, carnival_plaza, coldfire, junk_pile_big, junk_pile_blueprint, junk_pile_side, lureplant, quagmire_campfire]
health: missing=6 sample=[dropperweb, gingerbreadpig, gingerdeadpig, glommer, spiderhole, spiderhole_rock]
edible: missing=5 sample=[chum_aoe, chumpiece, manrabbit_tail, pigskin, slurper_pelt]
planardamage: missing=5 sample=[beefalo, beefalo_carry, player_common, winona_catapult, winona_catapult_item]
rechargeable: missing=5 sample=[battlesongs, beef_bell, pocketwatch_common, shadow_beef_bell, wortox_soul]
finiteuses: missing=3 sample=[spider_repellent, spider_whistle, spiderden_bedazzler]
heater: missing=3 sample=[campfirefire, fire, lavalight]
waterproofer: missing=1 sample=[raincoat]
```

## Tuning Trace Coverage
```yaml
items_tuning_exprs: 3009
items_with_trace: 3009
cooking_tuning_exprs: 254
cooking_with_trace: 254
```

## i18n Coverage
```yaml
zh:
  names: 1873
  items: 1873/3085
  all_ids: 1873/4968
```

## Top Stats (by frequency)
```yaml
work_left: 450
health_max: 341
combat_damage: 284
walk_speed: 278
stack_size: 278
attack_period: 196
attack_range: 176
edible_hunger: 168
run_speed: 156
edible_health: 153
weapon_damage: 153
sanity_aura: 142
perish_time: 139
uses: 125
uses_max: 124
planar_damage_base: 98
edible_sanity: 93
fuel_level: 88
equip_slot: 79
weapon_range_min: 67
weapon_range_max: 54
equip_walk_speed_mult: 52
attack_range_max: 47
dapperness: 36
waterproof: 30
equip_magic_dapperness: 25
heater_exothermic: 22
heater_endothermic: 22
equip_stack: 22
area_damage: 21
```
```

### File: data/reports/catalog_v2_summary.md
- mode: head
- size_bytes: 341
- sha256_12: 40df99deacde

```md
# Wagstaff Catalog v2 Summary

## Meta
```yaml
schema_version: 2
tuning_mode: value_only
scripts_zip: /home/steam/dontstarvetogether_dedicated_server/data/databundles/scripts.zip
scripts_dir: None
```

## Counts
```yaml
items_total: 3085
assets_total: 1096
craft_recipes: 878
cooking_recipes: 68
cooking_ingredients: 155
loot_items: 206
```
```

### File: data/reports/dst_raw_coverage.md
- mode: head
- size_bytes: 12001
- sha256_12: 25bf603e5f6b
- note: TRUNCATED

```md
# DST Raw Coverage Report

## Meta
```yaml
generated: 2026-01-20T21:43:07+08:00
dst_root: /home/steam/dontstarvetogether_dedicated_server
engine_mode: zip
scripts_zip: /home/steam/dontstarvetogether_dedicated_server/data/databundles/scripts.zip
scripts_file_count: 3915
```

## Counts

| Metric | Count |
|---|---:|
| total_files | 3915 |
| lua_files | 3901 |
| prefab_files | 1533 |
| prefabs_total | 2353 |
| prefabs_declared | 2058 |
| prefabs_fallback | 295 |
| item_prefabs | 813 |
| strings_names | 2758 |
| icons | 3106 |
| craft_recipes | 878 |
| craft_products | 851 |
| craft_ingredients | 246 |
| cooking_recipes | 68 |
| cooking_ingredients | 33 |
| tuning_keys | 5810 |
| tuning_expr_in_cooking_fields | 254 |

## Icon Sources

- `data/databundles/images.zip:images/inventoryimages2.xml`
- `data/databundles/images.zip:images/inventoryimages.xml`
- `data/databundles/images.zip:images/inventoryimages3.xml`
- `data/databundles/images.zip:images/inventoryimages1.xml`
- `data/databundles/images.zip:images/inventoryimages4.xml`

## Scripts Overview

```yaml
total_files: 3915
lua_files: 3901
```

### Categories

| Category | Count |
|---|---:|
| behaviours | 28 |
| brains | 184 |
| components | 782 |
| prefabs | 1533 |
| prefabs_postinit | 0 |
| recipes | 3 |
| screens | 134 |
| stategraphs | 250 |
| strings | 3 |
| tuning | 1 |
| widgets | 268 |

### Top Directories (scripts)

| Dir | Files |
|---|---:|
| prefabs | 1533 |
| components | 782 |
| map | 444 |
| widgets | 268 |
| stategraphs | 250 |
| [root] | 214 |
| brains | 184 |
| screens | 134 |
| scenarios | 50 |
| behaviours | 28 |
| util | 7 |
| languages | 2 |
| tools | 2 |
| nis | 2 |
| cameras | 1 |

### Top 2nd-level Directories

| Dir | Files |
|---|---:|
| map/static_layouts | 340 |
| widgets/redux | 88 |
| screens/redux | 71 |
| map/rooms | 42 |
| map/tasks | 10 |
| map/tasksets | 4 |
| map/levels | 4 |
| components/digester.lua | 1 |
| prefabs/reviver_cupid_beat_fx.lua | 1 |
| prefabs/structure_collapse_fx.lua | 1 |
| components/cookbookupdater.lua | 1 |
| prefabs/cutreeds.lua | 1 |
| prefabs/hermitcrab_relocation_kit.lua | 1 |
| components/vanish_on_sleep.lua | 1 |
| map/placement.lua | 1 |
| prefabs/yotd_boats.lua | 1 |
| prefabs/mushtree_webbed.lua | 1 |
| components/flotationdevice.lua | 1 |
| brains/spatbrain.lua | 1 |
| prefabs/chum.lua | 1 |
| widgets/listcursor.lua | 1 |
| components/wintertreegiftable.lua | 1 |
| components/projectedeffects.lua | 1 |
| prefabs/shadowchesspieces.lua | 1 |
| prefabs/monkeyhut.lua | 1 |
| prefabs/boat_leak.lua | 1 |
| map/settings.lua | 1 |
| components/pocketwatch.lua | 1 |
| components/shard_players.lua | 1 |
| components/winonateleportpadmanager.lua | 1 |
| prefabs/canopyshadows.lua | 1 |
| prefabs/alterguardian_lunar_fissures.lua | 1 |
| prefabs/hitsparks_fx.lua | 1 |
| stategraphs/SGwilson.lua | 1 |
| brains/catcoonbrain.lua | 1 |
| brains/krampusbrain.lua | 1 |
| components/furnituredecor.lua | 1 |
| scenarios/chest_ghosts.lua | 1 |
| prefabs/atrium_gate_pulsesfx.lua | 1 |
| components/klaussackkey.lua | 1 |
| brains/rockybrain.lua | 1 |
| prefabs/quagmire_parkspike.lua | 1 |
| map/caves_retrofit_land.lua | 1 |
| widgets/inventorybar.lua | 1 |
| components/moistureabsorberuser.lua | 1 |
| components/yotc_racestats.lua | 1 |
| stategraphs/SGgnarwail.lua | 1 |
| prefabs/ancienttrees.lua | 1 |
| widgets/nineslice.lua | 1 |
| scenarios/icebox_summer.lua | 1 |
| components/constructionbuilderuidata.lua | 1 |
| components/boatphysics.lua | 1 |
| prefabs/carpentry_station.lua | 1 |
| prefabs/rope.lua | 1 |
| prefabs/gelblob_attach_fx.lua | 1 |
| components/areaaware.lua | 1 |
| prefabs/lavaarena_fossilizing.lua | 1 |
| brains/wagdrone_rollingbrain.lua | 1 |
| brains/hostedbrain.lua | 1 |
| prefabs/eyeflame.lua | 1 |

## Data Directory Summary

```yaml
total_files: 10479
total_bytes: 4131504650
```

### Top Directories (by file count)

| Dir | Files | Bytes |
|---|---:|---:|
| anim | 6738 | 1483594678 |
| haptics | 1663 | 5080782 |
| images | 1314 | 1020148148 |
| bigportraits | 359 | 268490889 |
| sound | 201 | 1024031036 |
| levels | 164 | 82163930 |
| fx | 21 | 3161892 |
| databundles | 8 | 119625000 |
| minimap | 6 | 16891411 |
| data_structure.txt | 1 | 492951 |
| unsafedata_readme.txt | 1 | 364 |
| scripts_readme.txt | 1 | 1226 |
| movies | 1 | 107822304 |
| unsafedata | 1 | 39 |

### Top Extensions (by file count)

| Ext | Files | Bytes |
|---|---:|---:|
| .zip | 3489 | 1008765824 |
| .dyn | 3256 | 594453787 |
| .wav | 1663 | 5080782 |
| .tex | 1486 | 1390402400 |
| .xml | 377 | 447462 |
| .fsb | 146 | 1019247968 |
| .fev | 55 | 4783068 |
| .txt | 4 | 494608 |
| .ogv | 1 | 107822304 |
| <no_ext> | 1 | 39 |
| .bin | 1 | 6408 |

### Largest Files

| Bytes | Path |
|---:|---|
| 107822304 | `movies/intro.ogv` |
| 92971936 | `sound/meta4.fsb` |
| 81267424 | `sound/meta5.fsb` |
| 60823360 | `sound/together.fsb` |
| 53953598 | `databundles/scripts.zip` |
| 51251552 | `sound/music.fsb` |
| 51063616 | `sound/rifts3.fsb` |
| 50928320 | `sound/rifts.fsb` |
| 39105920 | `sound/monkeyisland.fsb` |
| 32872412 | `databundles/images.zip` |
| 28552960 | `sound/quagmire.fsb` |
| 26288896 | `sound/turnoftides_amb.fsb` |
| 25700768 | `sound/waterlogged2_amb.fsb` |
| 24906368 | `sound/music_frontend.fsb` |
| 23942496 | `sound/sanity.fsb` |
| 22406432 | `sound/music_frontend_yotg.fsb` |
| 22398272 | `sound/turnoftides.fsb` |
| 20810560 | `sound/moonstorm.fsb` |
| 20797984 | `sound/hookline_2.fsb` |
| 20125472 | `sound/lava_arena.fsb` |
| 20029792 | `sound/sfx.fsb` |
| 18855360 | `sound/qol1.fsb` |
| 17317184 | `sound/forest_stream.fsb` |
| 16850560 | `sound/grotto_sfx.fsb` |
| 16807231 | `databundles/fonts.zip` |
| 16473024 | `sound/wurt.fsb` |
| 15810442 | `databundles/anim_dynamic.zip` |
| 14053216 | `sound/amb_stream.fsb` |
| 13661344 | `sound/waterlogged1_amb.fsb` |
| 9879584 | `sound/rifts4.fsb` |

## Data Bundles (databundles/*.zip)

| Bundle | Entries | Bytes |
|---|---:|---:|
| anim_dynamic.zip | 3162 | 16280072 |
| bigportraits.zip | 351 | 59377 |
| fonts.zip | 24 | 16839540 |
| images.zip | 715 | 180473036 |
| klump.zip | 1 | 109 |
| scripts.zip | 3915 | 268830384 |
| shaders.zip | 67 | 254728 |

## Coverage

| Metric | Base | Target | Direct | Normalized |
|---|---:|---:|---:|---:|
| prefab_names | 2353 | 2758 | 1243 | 1245 |
| item_prefab_names | 813 | 2758 | 607 | 607 |
| item_prefab_icons | 813 | 3106 | 542 | 542 |
| icons_vs_prefabs | 3106 | 2353 | 672 | 673 |
| craft_products_vs_items | 851 | 813 | 296 | 296 |
| craft_ingredients_vs_items | 246 | 813 | 172 | 172 |
| cooking_ingredients_vs_items | 33 | 813 | 17 | 17 |

## Top Components (by prefab count)

| Component | Prefabs |
|---|---:|
| inspectable | 1569 |
| inventoryitem | 813 |
| lootdropper | 754 |
| workable | 458 |
| timer | 373 |
| combat | 366 |
| health | 346 |
```

### File: data/reports/mechanism_crosscheck_report.md
- mode: head
- size_bytes: 1824
- sha256_12: f33209a4ec09

```md
# Wagstaff Mechanism Crosscheck Report

## Counts
```yaml
resource_prefabs: 2353
mechanism_prefabs: 2353
missing_prefabs: 0
extra_prefabs: 0
resource_components_used: 708
mechanism_components_defined: 781
missing_component_defs: 0
unused_component_defs: 73
prefabs_without_components: 427
```

## Missing Prefabs in Mechanism Index

(none)

## Extra Prefabs in Mechanism Index

(none)

## Missing Component Definitions

(none)

## Unused Component Definitions

```text
aoeweapon_base
aoeweapon_leap
area_trigger
area_unlock
batspawner
beaverness
boatai
boatcrew
builder_replica
carnivalhostsummon
combat_replica
constructionsite_replica
container_replica
custombuildmanager
deathloothandler
debugger
decay
discoverable
diseaseable
dryer
equippable_replica
firebug
fishingrod_replica
follower_replica
freezable
freezefx
ghostbabysitter
grabbable
health_replica
healthbar
hideandseekhidingspot
highlight
hunger_replica
inventory_replica
inventoryitem_replica
inventoryitemmoisture
itemmimic
lordfruitflytrigger
lunarhailbuildup
lunarhailmanager
... (33 more)
```

## Prefabs Without Components (Resource Index)

```text
reviver_cupid_glow_fx
reviver_cupid_beat_fx
collapse_big
collapse_small
canopyshadows
atrium_gate_pulsesfx
grotto_war_sfx
atrium_gate_explodesfx
lavaarena_fossilizing
eyeflame
brokentool
lantern_posts_defs
rose_petals_fx
clockwork_common
wake_small
attunable_classified
skilltree_wendy
skilltree_wurt
daywalker2_items
torchfire_nautical
armor_lavaarena
torchfire_common
waxwell_shadowstriker
acidraindrop
acidsmoke_endless
skilltree_wathgrithr
quagmire_book_fertilizer
ice_splash
player_float_fx
quagmire_grill
quagmire_fern
container_opener
messagebottletreasure_marker
moonstormmarker_big
monstormmarker_debug
frostbreath
confetti_fx
torchfire_pronged
cane_victorian_fx
lunarhaildrop
... (387 more)
```
```

### File: data/reports/mechanism_index_summary.md
- mode: head
- size_bytes: 821
- sha256_12: 7fcd5cd604ab

```md
# Wagstaff Mechanism Index Summary

## Meta
```yaml
schema_version: 1
generated: 2026-01-20T20:23:38+08:00
scripts_sha256_12: b975ad93d5d8
scripts_zip: /home/steam/dontstarvetogether_dedicated_server/data/databundles/scripts.zip
```

## Counts
```yaml
components_total: 781
prefabs_total: 2353
components_used: 708
prefab_component_edges: 13304
```

## Top Components by Prefab Usage

| Component | Prefabs |
| --- | --- |
| inspectable | 1569 |
| inventoryitem | 813 |
| lootdropper | 754 |
| workable | 458 |
| timer | 373 |
| combat | 366 |
| health | 346 |
| locomotor | 333 |
| stackable | 332 |
| tradable | 273 |
| equippable | 255 |
| hauntable | 215 |
| knownlocations | 195 |
| fuel | 194 |
| sanityaura | 190 |
| updatelooper | 183 |
| edible | 173 |
| entitytracker | 166 |
| sleeper | 165 |
| weapon | 153 |
```

### File: data/reports/quality_gate_report.md
- mode: head
- size_bytes: 1581
- sha256_12: 46feb9bc8059

```md
# Quality Gate Report

## Inputs
```yaml
catalog: data/index/wagstaff_catalog_v2.json
catalog_index: data/index/wagstaff_catalog_index_v1.json
icon_index: data/index/wagstaff_icon_index_v1.json
i18n: data/index/wagstaff_i18n_v1.json
tuning_trace: data/index/wagstaff_tuning_trace_v1.json
mechanism: data/index/wagstaff_mechanism_index_v1.json
catalog_sqlite: data/index/wagstaff_catalog_v2.sqlite
mechanism_sqlite: data/index/wagstaff_mechanism_index_v1.sqlite
```

## Summary
```yaml
catalog_schema_version: 2
catalog_items_total: 3085
catalog_assets_total: 1096
catalog_items_with_stats: 1392
catalog_stats_total: 4138
catalog_stats_ratio: 0.45121555915721234
catalog_index_items_total: 3085
catalog_index_has_indexes: True
catalog_index_counts_items: 3085
icon_icons_total: 3106
i18n_langs: ['zh']
i18n_coverage: {'zh': {'names': 1873, 'ratio': 0.6071312803889789}}
trace_total: 3269
trace_items: 3009
trace_cooking: 254
mechanism_schema_version: 1
mechanism_components_total: 781
mechanism_prefabs_total: 2353
mechanism_prefab_component_edges: 13304
mechanism_validation_errors: 0
mechanism_validation_warnings: 0
sqlite_catalog_exists: True
sqlite_catalog_tables_total: 22
sqlite_catalog_db_schema_version: 4
sqlite_catalog_schema_version: 2
sqlite_catalog_has_fts: True
sqlite_catalog_has_tuning_trace: True
sqlite_mechanism_exists: True
sqlite_mechanism_tables_total: 15
sqlite_mechanism_db_schema_version: 4
sqlite_mechanism_schema_version: 1
sqlite_mechanism_has_links_relation: True
issues_total: 0
issues_fail: 0
issues_warn: 0
```

## Issues
- PASS: no issues detected
```

### File: data/reports/recipe_distribution.md
- mode: head
- size_bytes: 1655
- sha256_12: e177df3496c5

```md
# Wagstaff Recipe Distribution

## Function Usage
- **Recipe2**: 881
- **DeconstructRecipe**: 111
- **Recipe**: 56
- **DoRecipeClick**: 11
- **AddCookerRecipe**: 9
- **SetRecipeUnlocked**: 4
- **ClearAllUnlockedRecipes**: 3
- **UpdateRecipes**: 2
- **OnRecipeDirty**: 2
- **SetRecipe**: 2
- **SetupRecipeIngredientDetails**: 1
- **AddRecipeCard**: 1
- **TestRecipes**: 1
- **RemoveAllRecipes**: 1
- **CanBlueprintRandomRecipe**: 1
- **CleanupDupRecipes**: 1
- **IsRecipeValidForFilter**: 1
- **IsRecipeValidForStation**: 1
- **DeclareLimitedCraftingRecipe**: 1
- **ShouldHintRecipe**: 1
- **PickRandomRecipe**: 1

## File Hotspots (Top 20)
- `scripts/recipes.lua`: 997 recipes
- `scripts/prefabs/quagmire.lua`: 52 recipes
- `scripts/cooking.lua`: 10 recipes
- `scripts/prefabs/hermithouse.lua`: 7 recipes
- `scripts/widgets/redux/craftingmenu_widget.lua`: 3 recipes
- `scripts/prefabs/cookingrecipecard.lua`: 3 recipes
- `scripts/prefabs/hermitcrab_teashop.lua`: 2 recipes
- `scripts/widgets/recipepopup.lua`: 2 recipes
- `scripts/widgets/redux/craftingmenu_pinslot.lua`: 2 recipes
- `scripts/widgets/redux/quagmire_recipebook.lua`: 1 recipes
- `scripts/widgets/controllercrafting_singletab.lua`: 1 recipes
- `scripts/components/quagmire_recipeprices.lua`: 1 recipes
- `scripts/modutil.lua`: 1 recipes
- `scripts/widgets/ingredientui.lua`: 1 recipes
- `scripts/components/quagmire_recipebook.lua`: 1 recipes
- `scripts/prefabs/blueprint.lua`: 1 recipes
- `scripts/widgets/controllercrafting.lua`: 1 recipes
- `scripts/widgets/craftslot.lua`: 1 recipes
- `scripts/quagmire_recipebook.lua`: 1 recipes
- `scripts/widgets/quagmire_recipepopup.lua`: 1 recipes
```

### File: data/reports/resource_index_summary.md
- mode: head
- size_bytes: 1247
- sha256_12: 495c92f7d778

```md
# Wagstaff Resource Index Summary

## Meta
```yaml
generated: 2026-01-20T17:08:26+08:00
dst_root: /home/steam/dontstarvetogether_dedicated_server
engine_mode: zip
scripts_zip: /home/steam/dontstarvetogether_dedicated_server/data/databundles/scripts.zip
scripts_file_count: 3915
data_file_count: 10479
data_total_bytes: 4131504650
```

## Scripts

```yaml
total_files: 3915
lua_files: 3901
```

### Script Categories

| Category | Count |
|---|---:|
| behaviour | 28 |
| brain | 184 |
| component | 782 |
| language | 2 |
| map | 444 |
| other | 211 |
| prefab | 1533 |
| recipe | 2 |
| scenario | 50 |
| screen | 134 |
| stategraph | 250 |
| string | 3 |
| tool | 2 |
| tuning | 1 |
| util | 7 |
| widget | 268 |

## Prefabs

```yaml
prefab_files: 1533
prefabs_total: 2353
prefabs_skipped: 10
```

## Assets

```yaml
inventory_icons: 1080
inventory_atlases: 67
inventory_icon_traces: 0
```

## Data Summary

```yaml
total_files: 10479
total_bytes: 4131504650
```

## Bundles

| Bundle | Entries | Bytes |
|---|---:|---:|
| anim_dynamic.zip | 3162 | 16280072 |
| bigportraits.zip | 351 | 59377 |
| fonts.zip | 24 | 16839540 |
| images.zip | 715 | 180473036 |
| klump.zip | 1 | 109 |
| scripts.zip | 3915 | 268830384 |
| shaders.zip | 67 | 254728 |
```

### File: data/reports/stats_gap_inspect.md
- mode: head
- size_bytes: 1124
- sha256_12: e0eead6dd5a4

```md
# Stats Gap Inspection

## Meta
```yaml
generated: 2026-01-16 17:02:09
scripts_zip: /home/steam/dontstarvetogether_dedicated_server/data/databundles/scripts.zip
components: ['equippable', 'rechargeable', 'heater']
```

## equippable
```yaml
missing: 112
with_records: 0
with_function: 0
with_conditional: 0
```

Sample items with no detected assignments:
```text
- axe
- balloon
- balloonparty
- balloonparty_buff
- balloonparty_confetti_balloon
- balloonparty_confetti_cloud
- batbat
- batbat_bats
- batbat_fantasy_fx
- beeswax_spray
- bishop
- bishop_nightmare
- boomerang
- bootleg
- brush
- bugnet
- bullkelp_root
- carnivalgame_feedchicks_food
- carnivalgame_feedchicks_nest
- carnivalgame_feedchicks_station
```

## rechargeable
```yaml
missing: 5
with_records: 0
with_function: 0
with_conditional: 0
```

Sample items with no detected assignments:
```text
- battlesongs
- beef_bell
- pocketwatch_common
- shadow_beef_bell
- wortox_soul
```

## heater
```yaml
missing: 3
with_records: 0
with_function: 0
with_conditional: 0
```

Sample items with no detected assignments:
```text
- campfirefire
- fire
- lavalight
```
```

### File: docs/architecture/WEBCRAFT_NETWORK_STACK.md
- mode: head
- size_bytes: 1868
- sha256_12: 69134d3ef311

```md
# WebCraft 网络基建（FastAPI/ASGI）

目标：把 Web/GUI Craft 从“临时 stdlib server”升级为可扩展的系统级服务基座。

## 选型
- ASGI 框架：FastAPI
- 服务器：Uvicorn
- 中间件：GZip、可选 CORS
- 数据源：`data/index/wagstaff_catalog_v2.{json,sqlite}`（item-centric） + `data/index/wagstaff_mechanism_index_v1.{json,sqlite}`（机制索引）

## 目录结构
- apps/webcraft/
  - app.py          FastAPI app factory
  - api.py          REST API 路由（/api/v1）
  - catalog_store.py  catalog 装载 + 内存索引
  - planner.py      craft planner（inventory -> craftable/missing）
  - ui.py           单页 UI（零构建）
  - settings.py     配置结构

- devtools/serve_webcraft.py  开发/部署启动器

## 启动
```bash
python3 devtools/serve_webcraft.py --host 0.0.0.0 --port 20000 --no-open
```

## 反向代理挂载（root_path）
若挂载在 /webcraft：
```bash
python3 devtools/serve_webcraft.py --root-path /webcraft --host 0.0.0.0 --port 20000
```

UI 与 API 都使用相同 root_path 前缀，前端以相对路径访问 `/api/v1/...`，避免 0.0.0.0 导致的跨机器交互失败。

## API
- GET /api/v1/meta
- GET /api/v1/craft/filters
- GET /api/v1/craft/tabs
- GET /api/v1/craft/tags
- GET /api/v1/craft/recipes/search?q=...
- GET /api/v1/craft/recipes/{name}
- POST /api/v1/craft/plan
- POST /api/v1/craft/missing
- GET /api/v1/mechanism/meta
- GET /api/v1/mechanism/components
- GET /api/v1/mechanism/components/{component_id}
- GET /api/v1/mechanism/prefabs
- GET /api/v1/mechanism/prefabs/{prefab_id}

## 后续扩展建议
- 引入 “station/tech/skill tree” 规则：只扩展 planner + API，不改 UI 架构
- catalog 已优先 SQLite v4 表结构（JSON 作为 fallback），后续仅需补充 v4 扩展表
- 加 websocket/SSE：用于索引重建、长任务进度推送
```

### File: docs/archived/COOKING_UPGRADE.md
- mode: head
- size_bytes: 2022
- sha256_12: 118ef24f1c4c

```md
# Cooking 探索/模拟升级方案（归档）

目的：将 Cooking 页面升级为“百科（主）/探索/模拟”三入口体验，并提供可解释的结果与排序公式。

## 1. 机制理解（DST Cookpot）

- 料理由 `test(ingredients, tags)` 决定，`ingredients` 为槽位物品，`tags` 为食材属性汇总（meat/veggie/fruit/sweet/monster/egg/dairy 等）。
- 约束分两类：
  - 特定食材：需要/排除指定物品（names.X）。
  - 属性约束：对 tags 数值进行比较（tags.X >= 1.5 等）。
- 匹配后的选择规则：优先按 priority，高优先级内按 weight 抽取；无匹配时为 wetgoop。

## 2. 目标体验

- 入口清晰：百科 / 探索 / 模拟三种模式。
- 结果可解释：显示排序公式、结果依据与缺失项（接近可做）。
- 视觉结构：卡片为主，可切换高密度列表。
- 探索/模拟耦合：首个槽位填入即触发探索提示。

## 3. 数据契约与解析

- 新增 `cooking_ingredients` 索引：
  - `id`
  - `tags`（数值化）
  - `tags_expr`（无法解析的表达式）
  - `sources`（来源脚本）
- 来源：`scripts/ingredients.lua` / `scripts/cooking.lua`。

## 4. 结果与排序说明

排序公式（默认）：

```
score = priority * 1000 + weight * 100 - missing_penalty
missing_penalty = Σ(缺失 tag 值 * 10) + Σ(缺失食材数 * 50)
```

可解释字段：
- priority / weight
- tags / names 约束的满足情况
- 接近可做：缺失标签/缺失食材与差距值

## 5. API 设计（WebCraft）

- `/api/v1/cooking/explore`
  - 输入：`slots`（允许少于 4）
  - 输出：`cookable` 与 `near_miss`，含排序分数与缺失解释
- `/api/v1/cooking/simulate`
  - 输入：`slots`（必须 4）
  - 输出：`result`、`candidates`、公式说明与缺失解释

## 6. UI 约束

- 探索/模拟页必须显示：
  - 排序公式说明
  - 结果卡片（可做/接近可做）
  - 结果详情中的约束解释
- 移动端避免信息被固定头部遮挡。
```

### File: docs/archived/WEBCRAFT_UI_MODULARIZATION.md
- mode: head
- size_bytes: 2829
- sha256_12: b7ef1f4059e6

```md
# WebCraft UI 模块化改造计划

目标：在不更换技术栈的前提下，拆分 WebCraft UI 结构，降低 `apps/webcraft/ui.py` 体积与耦合，减少跨页 JS 依赖导致的回归，同时保持 API/数据结构稳定。

## 约束与原则

- 遵循 `docs/guides/DEV_GUIDE.md`（UI 静态资源必须落盘 `apps/webcraft/static/`，对外挂载 `/static/app`）。
- 不引入构建链，不改变 API 与数据产物结构。
- 迁移过程按“低风险 → 高改动”渐进执行。
- 每阶段结束都需手动验证 `/catalog` `/craft` `/cooking` 运行无 JS 错误。

## 范围

- 拆分 CSS/JS/模板结构，建立 core + pages 分层。
- `ui.py` 变薄，仅负责模板注入与资源引用。

非目标：
- 不引入 React/Vue/Vite 等构建体系。
- 不改动后端 API 形态与路径。

## 分阶段计划

### Phase 0：脚手架与入口对齐（低风险）

- 创建目录：
  - `apps/webcraft/static/css/`
  - `apps/webcraft/static/js/core/`
  - `apps/webcraft/static/js/pages/`
- 添加空占位资源：
  - `static/css/base.css`
  - `static/js/app.js`
- 在模板中引入 `/static/app/css/base.css` 与 `/static/app/js/app.js`（先不移除内联 CSS/JS）。

输出：
- 资源路径与加载链路建立。

### Phase 1：CSS 拆分

- 把 `_SHARED_CSS` 拆为：
  - `base.css`（跨页面通用）
  - `catalog.css` / `craft.css` / `cooking.css`（页面样式）
- `ui.py` 内联 CSS 逐步移除，仅保留极少量页面必要样式（如无则清空）。

输出：
- UI 样式可分文件维护，避免大段内联。

### Phase 2：JS 拆分（核心）

- 抽取通用工具到 `static/js/core/`（api/dom/i18n/icons/shared）。
- 页面逻辑拆入 `static/js/pages/`（catalog/craft/cooking）。
- `static/js/app.js` 通过 `body` class 或 `data-page` 初始化对应页面。
- 禁止页面之间互相引用函数。

输出：
- 跨页函数缺失问题根治。

### Phase 3：模板化 HTML（可选但推荐）

- 新增 `apps/webcraft/templates/*.html`。
- `ui.py` 读取模板并注入 `APP_ROOT` 等变量。

输出：
- HTML 结构可单独编辑，diff 变小。

## 验证清单

- `/` `/catalog` `/craft` `/cooking` 均可正常访问。
- console 无 `ReferenceError` / `Uncaught` 报错。
- i18n（label mode / tags）仍可正常切换与显示。
- 仍支持 `root_path` 部署路径。

## 风险与回滚

- 风险：资源路径写错导致页面空白；JS 拆分造成函数缺失。
- 回滚：保留 Phase 0 的路径引入不影响现状；出现问题可先恢复内联版本。

## 记录

- 每阶段完成需更新：
  - `PROJECT_STATUS.json`（RECENT_LOGS）
  - `docs/guides/DEV_GUIDE.md`（如新增结构约束）

## 状态

- 2026-01-17：Phase 1-3 完成，CSS/JS/模板迁移落地，`ui.py` 保留模板渲染。
```

### File: docs/guides/CLI_GUIDE.md
- mode: head
- size_bytes: 3291
- sha256_12: e861f5485b74

```md
# Wagstaff CLI Role Plan (v4.0.0-dev)

Goal: make the CLI an engineering console with clear roles and low cognitive load.

Install entrypoint (once per env): `python -m pip install -e ".[cli]"`.

## 1. Role Layers

- **Dash (entry)**
  - Purpose: project overview and runtime snapshot (objective, tasks, artifacts, freshness, quality).
  - Command: `wagstaff dash`

- **Health (env/artifact checks)**
  - Purpose: verify config and key artifacts (info-only, no blocking).
  - Command: `wagstaff doctor`

- **Query (knowledge lookup)**
  - Purpose: query recipes/cooking/prefab analysis for day-to-day lookup.
  - Command: `wagstaff wiki`

- **Explore (source analysis)**
  - Purpose: inspect source structure and Lua parsing for parser development.
  - Command: `wagstaff exp`

- **Mgmt (project management)**
  - Purpose: show/sync milestones and active tasks.
  - Command: `wagstaff mgmt`
  - Tip: `wagstaff mgmt check` for DEV_GUIDE emphasis
  - i18n: `--lang` or `WAGSTAFF_LANG=zh|en`


- **Build (index outputs)**
  - Purpose: generate data/index + data/reports artifacts.
  - Commands:
    - `wagstaff resindex` resource index
    - `wagstaff catalog2` catalog v2
    - `wagstaff catalog-sqlite` catalog sqlite v4
    - `wagstaff catindex` compact catalog index
    - `wagstaff i18n` i18n index
    - `wagstaff icons` icons + icon index
    - `wagstaff farming-defs` farming defs
    - `wagstaff mechanism-index build` mechanism index
    - Tip: `wagstaff mechanism-index validate` / `wagstaff mechanism-index diff`
    - `wagstaff index-manifest` index manifest

- **Quality (coverage checks)**
  - Purpose: quality gate (default info-only); reports generated via report hub.
  - Command:
    - `wagstaff quality` (includes sqlite + mechanism checks)
  - Report refresh: `wagstaff report build --quality`

- **Ops (service)**
  - Purpose: run WebCraft for UI validation.
  - Command: `wagstaff web`

- **Server (DST ops)**
  - Purpose: start/stop/update/backup/restore DST servers via screen.
  - Command: `wagstaff server` (interactive: `wagstaff server ui`)

- **Utility (support)**
  - Purpose: snapshots, reports, macro scans.
  - Commands: `wagstaff snap` / `wagstaff report` / `wagstaff samples`
  - Report hub: `wagstaff report build --all` / `wagstaff report build --stats-gap` / `wagstaff report list` / `wagstaff report open`
  - Portal: `wagstaff portal build` / `wagstaff portal list` / `wagstaff portal open`

## 2. Command Overview

- Entry
  - `wagstaff` / `wagstaff dash`
- Build
  - `wagstaff resindex` / `wagstaff catalog2` / `wagstaff catalog-sqlite` / `wagstaff catindex`
  - `wagstaff i18n` / `wagstaff icons` / `wagstaff farming-defs` / `wagstaff mechanism-index build`
  - `wagstaff index-manifest`
- Quality
  - `wagstaff quality`
- Service
  - `wagstaff web`
- Server
  - `wagstaff server` / `wagstaff server ui`
- Query + Analysis
  - `wagstaff wiki` / `wagstaff exp`
- Management
  - `wagstaff mgmt`
- Utilities
  - `wagstaff snap` / `wagstaff report` / `wagstaff portal` / `wagstaff samples`

## 3. Operating Principles

- CLI is for input/output orchestration; core parsing belongs in `core/`.
- Build artifacts always land in `data/`; CLI consumes `data/index` + `data/reports`.
- `wagstaff dash` is the default entry point (high density, non-blocking).
```

### File: docs/management/PROJECT_MANAGEMENT.md
- mode: head
- size_bytes: 5613
- sha256_12: 51a74bbf8cf3

```md
# Wagstaff-Lab 项目管理总览 (v4.0.0-dev)

本文件是**执行层面的单一管理入口**。ROADMAP 仅保留长期方向，SPEC 仅描述数据/接口契约，PROJECT_STATUS 保持运行快照与近期记录。

## 0. 管理约定

- **战略方向**：`docs/management/ROADMAP.md`
- **vNext 重构规划**：`docs/management/VNEXT_REFACTOR_PLAN.md`
- **数据契约**：`docs/specs/CATALOG_V2_SPEC.md`
- **执行管理**：`docs/management/PROJECT_MANAGEMENT.md`（本文件）
- **运行快照**：`PROJECT_STATUS.json`

说明：不再使用 pm 工具，统一以文档与 JSON 状态文件管理进度。
工具化入口：`wagstaff mgmt status|sync|dump`。
建议：`wagstaff mgmt check` 作为变更前的必跑检查。

## 1. L0 目标（North Star）

对 DST 资源实现**可迁移、可检索、可解释**的全面理解与展示，产物可被 Web/CLI/后续数据库直接消费。

## 2. L1 里程碑（Milestones）

- **M3.0 架构拆分与入口统一**（完成）
  - core/apps 分层、CLI dispatcher、pyproject 入口、Makefile 任务体系
- **M3.1 Catalog v2 基线与 WebCraft 接入**（完成）
  - Catalog v2、icon index、tuning trace、WebCraft UI/接口对接
- **M3.2 质量与覆盖率提升**（进行中）
  - stats 解析覆盖、i18n 覆盖、质量报告持续迭代
- **M3.3 WebCraft 体验深化**（规划中）
  - 参考标杆：Food Guide（模拟/探索/统计）、DST Item List（双语+调试 ID）、Wiki Craft 表格化呈现
  - 三入口体验：Catalog/Craft/Cooking 结构一致，探索/模拟/百科模式清晰
  - 解释性输出：规则/条件/trace 可视化，配方链路与用途说明
  - catalog 分页/缓存与搜索改造（已落地）
  - Cooking 升级方案归档：`docs/archived/COOKING_UPGRADE.md`
- **M3.4 存储升级准备**（规划中）
  - SQLite/Parquet 迁移计划与 schema 对齐
  - SQLite catalog 派生产物与 WebCraft SQLite 优先加载（已落地）
- **M3.5 服务器运维集成**（完成）
  - wagstaff server 与独立运维模块

## 3. L2 需求分层（Pillars → Epics）

### 数据基础
- **E1 资源索引**：scripts + data 扫描（完成）
- **E2 Catalog v2 构建**：items + craft + cooking + assets（完成）
- **E3 Tuning trace**：链路索引 + 按需加载（完成）
- **E4 Icon pipeline**：静态 icon + 动态回退（完成）

### 数据质量
- **E5 stats 覆盖扩展**：更多组件/属性/方法（进行中）
- **E6 i18n 覆盖扩展**：names/desc/quotes + UI 词条（进行中）
- **E7 质量报告与门禁**：report_hub + quality_gate（完成）

### WebCraft 应用
- **E8 Catalog 列表/检索**：items + indexes（完成）
- **E9 Item 详情与 stats 展示**（完成）
- **E10 i18n UI**：语言切换（完成）
- **E11 Trace UI 按需加载**（完成）
- **E12 交互体验增强**（规划中）
  - 料理探索：食材驱动筛选 + 模拟结果 + 规则可解释
  - 列表密度：表格/卡片切换，中英文/调试 ID 同屏，快捷复制
  - 导航一致：统一 list/detail 框架，支持 URL 状态、键盘/移动端操作
  - 料理食材索引：解析 ingredients/cooking 定义，落盘 cooking_ingredients 标签与来源
  - 探索/模拟入口：百科为主，探索/模拟耦合；排序公式 + 接近可做解释

### 工程化与工具链
- **E13 CLI 统一入口**（完成）
- **E14 任务入口规范**（完成）
- **E15 Snapshot 规范**（完成）

### 服务器运维
- **E16 DST server 管理**（完成）

## 4. L3 当前任务（Active）

- **T-101**：stats 覆盖扩展（组件属性/方法解析补全）
- **T-102**：i18n 覆盖提升（names/desc/quotes + UI 文案）
- **T-103**：Catalog 质量报告迭代（覆盖率与缺口追踪）
- **T-104**：Cooking ingredient tags 解析与 catalog 落盘（ingredients.lua / cooking.lua）
- **T-105**：Cooking 探索/模拟重做（可做/接近可做 + 解释卡片 + 高密度切换）
- **T-106**：Mechanism index v1（组件解析 + prefab 链路）
- **T-107**：Mechanism SQLite schema v1（links 表 + 统一映射策略）
- **T-108**：Mechanism summary + consistency 校验（JSON/SQLite 对齐）
- **T-109**：Mechanism crosscheck 报告（resource_index 对齐 + 缺口清单）
- **T-112**：Mechanism JSON schema 文件（machine-readable）
- **T-113**：Mechanism build 严格校验开关（--strict）
- **T-114**：SQLite v4 schema 设计（DDL + 索引 + 迁移策略，见 `docs/specs/SQLITE_V4_SPEC.md`）

## 5. 最近完成（摘要）

- wagstaff server 接入，运维模块独立化
- WebCraft UI 模块化：模板迁移至 `apps/webcraft/templates/`，CSS/JS 拆分到 `apps/webcraft/static/`
- Catalog index v1 规范与 WebCraft API 契约补齐
- pyproject 入口统一、bin/installer 清理完成
- 新增耕种机制索引产物与机制报告（farming defs）
- 索引清单落盘：新增 `index-manifest` 生成器与 Makefile 入口
- SQLite 产物增加 `db_schema_version=4`，为 v4 结构演进预留标记
- core 收口：`klei_atlas_tex` 迁移至 `core/assets/`，配置加载迁移至 `core/config/`
- 索引全量重建并生成 `wagstaff_index_manifest.json`
- SQLite v4：catalog/mechanism 构建器升级 + WebCraft v4 表优先加载
- mechanism-index 收口：validate/diff 合并为子命令，旧脚本内退

## 6. 下一步建议（短期）

1. 以 stats 解析覆盖为主线，补齐关键组件（equippable/rechargeable/heater 等）
2. i18n 覆盖率提升，补齐 UI 词条并完善多语言元数据
3. 质量报告指标化：新增缺失原因统计与趋势对比
```

### File: docs/management/VNEXT_REFACTOR_PLAN.md
- mode: head
- size_bytes: 7679
- sha256_12: 5f3cbf715663

```md
# Wagstaff-Lab vNext 重构规划（破兼容版）

目标：清理历史包袱，重建 core 架构与索引流水线，为下一代能力打底（机制解析、模拟、分析工具、可视化、服务器管理增强）。

## 0. 背景与约束

- 版本兼容性非重点，可进行破坏性重构。
- 当前痛点：core 过于耦合（解析/领域逻辑/索引混杂），索引构建链路难复用，机制解析与模拟缺乏统一入口。
- vNext 必须提供明确的“机制解析 → 索引 → 分析 → 可视化”的链路。

## 0.1 已确认方向

- **机制解析优先级**：组件解析优先（Components > StateGraph > Brain）。
- **产物形态**：JSON 与 SQLite 同步落盘。
- **版本策略**：v4 破兼容重构。
- **旧 analyzer.py**：彻底移除，不保留兼容层。
- **应用层节奏**：应用层慢慢做，core 重构优先。
- **WebCraft UI**：保留 Cooking 模拟工具；Catalog/Craft/Cooking 图鉴页重做。
- **索引引用方式**：统一 id（以 prefab 为核心）+ 映射表兜底。

## 1. 重构原则（抛弃历史包袱）

- 允许移除旧 API 与旧文件结构，不做兼容层“补丁”。
- 以模块职责为中心，避免单文件巨型模块。
- 以数据契约驱动索引与工具；一切产物可验证、可追溯。
- 解析与索引分离：解析做“事实抽取”，索引做“结构化输出”。
- 抽离配置/IO：engine 专注挂载与读取；解析/索引不直接读磁盘。

## 2. vNext 目标产物

- **catalog v3**：更细粒度的物品/机制索引，覆盖组件、状态机、掉落、行为、技能、资源链路。
- **mechanism index**：DST 机制解析产物（组件属性/方法、状态机、脑图、事件流、配方规则）。
- **component index**：组件定义与接口索引（字段/方法/默认值/事件）。
- **simulation index（轻量）**：以种植为主的简化模拟输入输出与参数表。
- **analysis reports**：覆盖率、机制差异、脚本演进对比等。
- **storage**：JSON 与 SQLite 同步落盘，机制索引与 catalog 同步支持查询。
  - JSON 主键统一以 prefab id 为核心，附 `links` 映射表兜底。
  - Mechanism index 规范：`docs/specs/MECHANISM_INDEX_SPEC.md`

## 3. 目标架构蓝图

```
core/
  engine/           # scripts 挂载、读文件、缓存、索引源
  lua/              # lexer/split/match/call-extract/table-parse
  parsers/          # prefab/loot/widget/strings/cooking/sg/brain
  indexers/         # resource/catalog/mechanism/simulation/i18n
  schemas/          # 数据契约 + meta + validators
  sim/              # farming/cooking/未来战斗/AI 模拟
  assets/           # atlas/tex/png 解析与转换
  config/           # 配置加载、路径解析、环境探测
  tools/            # 低耦合工具与跨索引共享逻辑
```

依赖方向：
- `parsers` 仅依赖 `lua` + 纯数据结构。
- `indexers` 依赖 `engine` + `parsers` + `schemas`。
- `apps` 与 `devtools` 仅通过 `core/indexers` + `core/schemas`。

## 4. 核心能力增强方向

### 4.1 DST 机制解析

- **组件解析优先**：组件 API 解析先落盘（属性、方法、数值推导，含 TUNING 链路）。
- 状态机解析：StateGraph、事件流与状态迁移图。
- AI/Brain 解析：行为树/脑图结构与条件。
- 资源链路：prefab → 组件 → 掉落 → 产物 → 配方。
  - 统一输出 `links`（source/target/id）用于跨索引 join。

### 4.2 模拟系统（轻量：种植优先）

- 种植模拟：提供“输入参数 → 结论”简化模型（生长时间/营养/水分/季节/肥料）。
- 烹饪模拟：保留规则解释与接近可做输出，不追求重型 UI。
- 输出标准化：模拟输入/输出 schema，为 CLI/报告优先，UI 后置。

### 4.3 分析工具与可视化

- CLI 分析工具：差异报告、机制覆盖率、解析失败样本。
- 可视化系统：机制图谱、状态机图、配方链路图、模拟参数面板。
- 报告模板统一化，便于快照与持续迭代。

### 4.4 服务器管理增强

- 统一命令与任务调度（备份/恢复/巡检/滚动更新）。
- 运行健康与日志聚合，支持结构化输出。
- 与数据分析层解耦，但共享同一配置与日志规范。

### 4.5 存储与查询（SQLite vNext）

- JSON 与 SQLite 同步落盘，保证产物一致性。
- SQLite 结构向“机制查询”优化，采用**中等粒度**拆表：
  - 关系表：`components` / `component_fields` / `component_methods`
  - 状态机：`stategraphs` / `stategraph_states` / `stategraph_events` / `stategraph_edges`
  - AI：`brains` / `brain_nodes` / `brain_edges`
  - 映射：`prefab_components` / `prefab_links`
- 同时保留 `raw_json` 字段或附表，保证解析原貌可追溯。
- 统一映射表：`links(source, source_id, target, target_id)` 作为跨索引连接入口。

## 5. 分阶段里程碑（建议）

### Phase A: Core 拆分与基础迁移
- 拆分 `core/analyzer.py` 为 `core/lua` + `core/parsers/*`。
- `core/engine` 专注挂载/IO/缓存，不承担解析与索引。
- 引入 `schemas/validators`，建立最小校验链路。
- 旧 `core/analyzer.py` 彻底移除，不保留兼容层。

交付标准：
- 旧模块依赖清除 80%+。
- `engine` 无领域逻辑，索引器只做组合。

### Phase B: 索引流水线重建
- 建立“step-based pipeline”（resource → catalog → mechanism → simulation → reports）。
- 缓存与增量构建统一到 `data/index/.build_cache.json`。
- vNext schemas 定义并在 indexers 输出时校验。
- JSON 与 SQLite 同步落盘，建立一致性校验。
  - 机制索引中 `links` 与 SQLite `links` 表保持一致。

交付标准：
- 每个 indexer 有清晰输入/输出契约。
- 可独立复用与单测。

### Phase C: 机制解析落盘
- 组件解析与索引产物优先落盘（component index + prefab 组件映射）。
- StateGraph/Brain 解析器与索引产物落盘。
- 组件方法/属性解析覆盖率提升（与 TUNING 统一追踪）。

交付标准：
- 机制索引产物可被 Web/CLI 查询。

### Phase D: 种植模拟与机制工具
- farming simulation 轻量化落盘，提供可解释输出（CLI/报告优先）。
- 机制/模拟报告工具（差异/覆盖/异常样本）。

交付标准：
- 种植模拟可驱动 UI 与 CLI。

### Phase E: 可视化与服务器管理增强
- 机制可视化页面落地（图谱/状态机/链路）。
- server 管理扩展到调度/巡检与结构化日志。

交付标准：
- 可视化系统与 server 工具进入主入口。

## 6. 迁移与清理策略

- 移除旧 `core/analyzer.py` 巨型模块，拆分为独立目录；不保留兼容层。
- `core/indexers` 内部依赖改为 `parsers` 输出结构，不直接解析 Lua。
- 将 `klei_atlas_tex.py` 迁移到 `core/assets/`，统一图像处理入口。
- 旧 CLI/Devtools 入口保留最小桥接，逐步迁移到新 pipeline。

## 7. 文档与归档策略

- 已完成的规划文档移入 `docs/archived/`。
- 总结性文档保留在 `docs/management/` 与 `docs/specs/`。
- vNext 计划与架构图作为新的主入口之一。

## 8. 质量与验证

- 每个 indexer 有最小测试用例（输入样本 + 输出快照）。
- 增量构建一致性检查（hash/size/field coverage）。
- 机制解析失败样本收敛为可复现的 bug 集合。

## 9. 风险与假设

- 破兼容会导致上层短期不可用，需要短暂冻结期。
- 机制解析覆盖率不足会影响可视化展示，需要逐步补齐。

## 10. 讨论点（请确认）

暂无新增讨论点。
```

### File: docs/management/WEBCRAFT_ATLAS_LEDGER_UI_REFACTOR.md
- mode: head
- size_bytes: 1395
- sha256_12: 45625a3ebd8b

```md
# WebCraft Cooking Tools Atlas Ledger UI Refactor

## Goal
- Apply Atlas Ledger layout to Cooking Tools desktop while preserving existing features.
- Mobile uses a dedicated workbench/results tab layout with no page scroll.
- Keep existing routes and the static lab page until cleanup is approved.

## Scope
- Template: apps/webcraft/templates/cooking_tools.html
- Styles: apps/webcraft/static/css/cooking.css (tool page only)
- JS: apps/webcraft/static/js/pages/cooking_tool.js (viewport vars + tool UX), apps/webcraft/static/js/pages/cooking_shared.js (shared cooking state)
- i18n: conf/i18n_ui.json (new tab labels)
- No API or route changes.

## Decisions
- Desktop: three-panel ledger grid (ingredients left, console top-right, results+detail bottom-right).
- Detail view embedded in results panel as a fixed dual-column layout.
- Mobile: workbench/results tab switch; results auto-open after explore/simulate.
- Internal panel scrolling only; no page scroll.

## Tasks
1. Replace tool page layout with ledger grid while keeping required IDs.
2. Add tool-scoped ledger styles and mobile tab rules.
3. Wire mobile tabs + viewport sizing in cooking_tool.js.
4. Update i18n strings and run `make i18n`.
5. Validate desktop + mobile behaviors and no-scroll workbench.

## Risks
- CSS bleed into cooking encyclopedia if selectors are not scoped.
- Missing i18n build if `make i18n` is skipped.
```

### File: docs/management/WEBCRAFT_MOBILE_APP_SHELL.md
- mode: head
- size_bytes: 2388
- sha256_12: b93409358ae9

```md
# WebCraft Mobile App Shell 规划

目标：先落地 Cooking（模拟/探索）移动端 App 风格体验，同时为 Craft/Catalog 全站统一 App Shell 做基础。

## 约束

- 遵循 `docs/guides/DEV_GUIDE.md`，不引入新构建链、不改变 API 形态。
- 仅使用模板 + CSS/JS，移动端样式通过 `data-app-shell="1"` opt-in。
- 桌面端布局不回退、不稀释现有信息密度。

## 设计方向（移动端）

- 单列主流程：锅位/操作 → 食材选择 → 结果 → 详情。
- App Shell：底部固定导航，顶部轻量标题栏。
- 触控优先：Chip/按钮 ≥ 44px，横向滚动过滤器，安全区（safe-area）适配。

## 架构方案

### 1) App Shell 基础（可复用）

- 约定：`body[data-app-shell="1"]` 启用移动端 App Shell。
- 通用样式：
  - `apps/webcraft/static/css/base.css` 提供 `.app-nav` 组件样式与移动端激活规则。
  - `layout` 增加 bottom padding，避开底部导航覆盖。
- 通用结构：
  - 模板新增 `<nav class="app-nav">`，包含 `appNavCraft/appNavCooking/appNavCatalog`。
  - 页面 JS 负责设置 href 与 active 状态。

### 2) Cooking 工具页落地

- `data-role="tool"` 页面移动端重排为：
  - `#toolBox`（粘性锅位条 + 操作）
  - `#ingredientPicker`（主交互区）
  - `#resultBox`（结果卡片/列表）
  - `#detail`（详情/规则）
- 结果触发后自动聚焦到结果区（移动端）。
- 结果区在移动端以底部抽屉呈现，可折叠/展开以保证食材选择区可用。

### 3) 扩展到全站（后续）

- Craft/Catalog 页面：
  - 添加 `data-app-shell="1"` 与底部 `app-nav`。
  - 在各自 JS 中设置 `appNav*` 的 href 与 active。
- 可选：抽取 App Shell 片段到模板片段（若后续引入更轻量的模板拼装）。

## 分阶段执行

- Phase A（本次）：
  - 仅落地 Cooking 的 App Shell 与移动端交互重排。
  - 建立 `app-nav` 组件与 `data-app-shell` 约定。
- Phase B（后续）：
  - Craft/Catalog 迁移至 App Shell。
  - 增加统一的移动端导航规则与交互规范。

## 验证清单

- Cooking 模拟/探索在移动端可完整使用，无需滚动到顶部操作。
- 底部导航可切换三大入口。
- 桌面端布局不受影响。

## 记录

- App Shell 规范写入 `DEV_GUIDE.md`。
- 变更写入 `PROJECT_STATUS.json`。
```

### File: docs/specs/CATALOG_V2_SPEC.md
- mode: head
- size_bytes: 11740
- sha256_12: e57b140a47fc
- note: TRUNCATED

```md
# Wagstaff Catalog v2 规范草案

目标：对 DST 做“全面理解与展示”，形成稳定可迁移的索引产物。

说明：本文件仅描述**数据/接口契约**；进度与迁移状态见 `docs/management/PROJECT_MANAGEMENT.md`。
vNext 破兼容重构规划见 `docs/management/VNEXT_REFACTOR_PLAN.md`。

## 1. 总体原则

- **全物品覆盖**：以 prefab 为基础实体，覆盖建筑/生物/角色/物品（不含皮肤）。
- **数据层不含语言**：名称/描述在独立 i18n index 中维护。
- **可迁移**：结构接近“可归一化表”，便于未来迁移到 SQLite/Parquet。
- **可追溯**：所有重要字段需要保留来源与构建元信息。
- **可裁剪**：可按需输出链路（trace）与结果（value）。

## 2. 产物目录建议

- `data/index/wagstaff_resource_index_v1.json`  
  原始资源索引（scripts + prefabs + data + bundles）。
- `data/index/wagstaff_catalog_v2.json`  
  主 Catalog（items + recipes + cooking + assets + tuning）。
- `data/index/wagstaff_catalog_v2.sqlite`  
  SQLite 版本（与 JSON 同构，便于运行时加载与后续迁移）。
- `data/index/wagstaff_catalog_index_v1.json`  
  紧凑索引（用于 WebCraft 列表/搜索与后续 DB 迁移）。
- `data/index/wagstaff_i18n_v1.json`  
  语言层索引（names/desc/quotes 等）。
- `data/index/tag_overrides_v1.json`  
  人工标签修订（可选）。
- `data/index/wagstaff_tuning_trace_v1.json`  
  独立链路索引（可选，便于裁剪）。

SQLite 产物表（派生，摘要）：
- v4 结构以 `db_schema_version=4` 标记，细节见 `docs/specs/SQLITE_V4_SPEC.md`。
- 文件名仍以 catalog JSON 的 schema 版本为准（v2），SQLite 结构版本通过 `db_schema_version` 区分。
- `items`：`id/kind/name/*_json/raw_json`
- `item_stats`：`item_id/stat_key/expr/value_json/trace_key/raw_json`
- `item_*` join 表：category/behavior/source/tag/component/slot
- `assets`：`id/name/icon/image/atlas/build/bank/raw_json`
- `craft_meta` / `craft_recipes` / `craft_ingredients`
- `cooking_recipes` / `cooking_ingredients`
- `catalog_index`：`id/name/icon/kind/*_json`（列表/检索）
- `tuning_trace`：`trace_key/raw_json`（可选）

## 3. 核心实体草案

### 3.0.1 顶层结构（当前实现）

`wagstaff_catalog_v2.json` 顶层结构如下：
- `schema_version`
- `meta`
- `items`
- `assets`
- `craft`
- `cooking`
- `cooking_ingredients`
- `stats`

### 3.0 Meta (统一产物元信息)

建议字段：
- `schema`：产物版本号
- `project_version`：项目版本（统一版本入口）
- `index_version`：索引版本（与 schema 独立）
- `generated`：构建时间 (ISO8601)
- `tool`：构建工具名
- `sources`：构建输入来源（scripts_zip / scripts_dir / resource_index 等）

当前实现额外字段：
- `tuning_mode`
- `scripts_sha256_12`
- `scripts_zip`
- `scripts_dir`

### 3.1 Item（统一实体）

每个 item 对应一个 prefab id，必要时允许 `aliases` 处理别名。

字段建议：
- `id`：prefab id（小写）
- `kind`：`character|creature|structure|item|plant|fx`
- `categories`：多值（见第 4 节）
- `behaviors`：多值（见第 4 节）
- `sources`：多值（craft/cook/loot/spawn/natural/event）
- `slots`：装备槽位（见第 4 节）
- `components`：prefab 组件名列表（用于行为推导）
- `tags`：prefab 标签（用于行为推导）
- `assets`：icon/atlas/image 等引用（可为空）
- `stats`：组件属性/方法推导出的关键数值（weapon_damage/uses/armor 等）
- `prefab_files`：prefab 源文件路径列表（用于溯源）
- `prefab_assets`：prefab Asset 原始声明（原始引用）
- `brains` / `stategraphs` / `helpers`：AI 与辅助函数线索

当前实现说明：
- `recipes` / `aliases` 尚未落盘。
- `stats` 单元结构为：
  - `expr`：原始表达式
  - `value`：解析值（可为空）
  - `expr_resolved`：解析后的表达式（可为空）
  - `trace_key`：可选，指向 trace 索引键

### 3.2 Recipe（Craft）

字段建议：
- `id` / `product` / `ingredients`
- `tab` / `filters` / `builder_tags` / `tech`
- `station_tag` / `builder_skill`
- `tuning`（对材料数量/消耗等可选追踪）

当前实现结构：
- `craft.schema`
- `craft.recipes`（每个 recipe 保留 `ingredients` + `amount_value`）
- `craft.aliases`
- `craft.filter_defs` / `craft.filter_order`

### 3.3 Cooking

字段建议：
- `id` / `card_ingredients` / `foodtype` / `tags`
- `priority` / `weight` / `cooktime`
- `hunger` / `health` / `sanity` / `perishtime`
- `tuning`（对上述字段）

当前实现说明：
- Cooking 字段在 `value_only` 下直接落盘数值。
- `tuning_mode=full` 时字段为 `{value, expr, trace}` 结构。

### 3.3.1 Cooking Ingredients

字段建议：
- `id`
- `tags`：料理标签贡献（如 meat/veggie/sweet/monster/egg）
- `tags_expr`：无法解析为数值的表达式（可选）
- `sources`：来源脚本列表（可选）
- `name` / `atlas` / `image` / `prefab` / `foodtype`（可选）

### 3.4 Assets

字段建议：
- `icons`：`{id: "static/icons/{id}.png"}`
- `atlas`/`image`/`build`/`bank` 等原始引用
- `sources`：来源路径（inventoryimages.xml / prefab Asset / data/*）

当前实现说明：
- `assets` 为 `{id: {icon, atlas?, image?}}` 的轻量映射。

### 3.5 I18n Index

字段建议：
- `names`：`{lang: {id: localized_name}}`
- `ui`：`{lang: {key: text}}`（WebCraft UI 词条）
- `meta`：构建来源（po 路径 / scripts.zip / ui.json）

### 3.6 Stats 解析覆盖面（当前实现）

- 组件方法解析：`weapon` / `combat` / `finiteuses` / `armor` / `edible` / `perishable` / `fueled` / `equippable` / `insulator` / `waterproofer` / `light` / `stackable` / `health` / `sanity` / `sanityaura` / `hunger` / `locomotor` / `rechargeable` / `heater` / `planardamage` / `planararmor` / `workable`
- 组件属性解析：覆盖 `equippable` / `edible` / `insulator` / `waterproofer` / `stackable` / `health` / `sanity` / `hunger` / `locomotor` / `planardamage` / `planararmor` / `workable` 等

## 4. 标签体系（建议）

### 4.1 kind（主类）
`character | creature | structure | item | plant | fx`

### 4.2 category（功能类别）
`weapon | armor | tool | food | resource | magic | container | light | deployable | trap | boat | farm | decor | toy`

### 4.3 behavior（组件行为）
`equippable | edible | stackable | burnable | perishable | repairable | fuel | tradable | hauntable | deployable`

### 4.4 source（来源）
`craft | cook | loot | spawn | natural | event`

### 4.5 slot（可选）
`head | body | hand | back`

## 5. TUNING trace 模式

字段可同时输出：
- `value`：数值（可直接展示）
- `expr`：原始表达式
- `trace`：链路结构（refs/steps/chain）

输出模式：
- `value_only`：只输出 value
- `full`：输出 value + trace

建议将 trace 拆分到 `wagstaff_tuning_trace_v1.json`，在需要时按 id 拉取。

当前实现说明：
- Trace 索引是 `{trace_key: trace}` 的映射。
- `trace_key` 形式：
  - `item:{id}:stat:{stat_key}`
  - `craft:{recipe}:ingredient:{item}`
  - `cooking:{recipe}:{field}`

## 6. tag overrides（人工修订）

建议结构示例见：`conf/samples/tag_overrides.example.json`

逻辑建议：
- `add`：追加标签
- `remove`：移除标签
- `set`：强制覆盖（必要时）

## 7. 迁移建议（上层改造）

- WebCraft `catalog` 页面读取 `wagstaff_catalog_v2.json` 的 `items`
- WebCraft 搜索改为 “items + recipes + cooking” 多源索引
- i18n 层独立加载（不依赖 catalog）
- icon 服务改为读取 `assets.icon` / icon index 统一入口
- CLI/wiki 使用 v2 结构，避免 direct prefab 解析
- 旧 v1 产物可删除（或仅保留构建回滚）

## 8. Catalog Index v1 规范（wagstaff_catalog_index_v1.json）

用途：提供 WebCraft 列表/搜索的紧凑索引，减少 UI 初始化负担。

### 8.1 顶层结构

```json
{
  "schema_version": 1,
  "meta": { ... },
  "counts": { "items_total": 0, "items_with_icon": 0, "icon_only": 0 },
  "items": [ ... ],
  "indexes": { ... }
}
```

### 8.2 meta（当前实现）

- `schema` / `generated` / `tool`
- `sources.catalog`: `wagstaff_catalog_v2.json`
- `sources.scripts_zip` / `sources.scripts_dir`
- `catalog_schema`: catalog v2 schema 版本
- `scripts_sha256_12` / `scripts_zip` / `scripts_dir`

### 8.3 items（紧凑条目）

每个条目结构：
```

### File: docs/specs/FARMING_MECHANICS_REPORT.md
- mode: head
- size_bytes: 4401
- sha256_12: ea09674a3e8e

```md
# 耕种机制报告 (DST)

本报告整理 DST 耕种系统的核心机制、关键调参与数据源，供模拟/规划系统使用。

## 数据来源

- `scripts/prefabs/farm_plant_defs.lua`：作物定义、季节/营养/湿度偏好、成长与巨型权重
- `scripts/prefabs/farm_plants.lua`：作物成长流程、压力测试与产出逻辑
- `scripts/components/farmplantstress.lua`：压力累计与最终压力等级
- `scripts/components/farming_manager.lua`：土壤湿度/营养循环、季节性杂草生成
- `scripts/prefabs/weed_defs.lua` + `scripts/prefabs/weed_plants.lua`：杂草成长与扩散
- `scripts/prefabs/veggies.lua`：作物种子权重 (randomseed 选择)
- `scripts/prefabs/fertilizer_nutrient_defs.lua`：肥料营养值
- `scripts/tuning.lua`：耕种相关调参

## 种子与杂草清单

### 可种植作物 (14)

`asparagus`、`garlic`、`pumpkin`、`corn`、`onion`、`potato`、`dragonfruit`、`pomegranate`、`eggplant`、`tomato`、`watermelon`、`pepper`、`durian`、`carrot`

对应种子：`{plant}_seeds`，详见 `farm_plant_defs.lua` 与 `veggies.lua`。

### 原始种子

`seeds` → `farm_plant_randomseed`，随机生成作物或杂草；详见 `seeds.lua`、`farm_plants.lua`。

### 农田杂草

`weed_forgetmelots`、`weed_tillweed`、`weed_firenettle`、`weed_ivy`；详见 `weed_defs.lua`、`weed_plants.lua`。

## 成长阶段与时间

作物阶段：`seed → sprout → small → med → full → rotten`

`farm_plant_defs.lua` 中 `MakeGrowTimes()` 定义：

- 发芽期：`seed = [germ_min, germ_max]`
- 生长期：`sprout/small/med` 由 `full_grow_min/max` 按 0.5/0.3/0.2 分摊
- 成熟腐烂：`full = 4 * TOTAL_DAY_TIME`
- 巨型腐烂：`oversized = 6 * TOTAL_DAY_TIME`
- 再生：`regrow = [4, 5] * TOTAL_DAY_TIME`

季节加速：若为作物偏好季节，发芽与生长期 *0.5（`farm_plants.lua`）。

## 压力系统

压力类别（`farm_plants.lua`）：

- `killjoys`：半径内“杀手”植物数量 > `FARM_PLANT_KILLJOY_TOLERANCE`
- `family`：同类数量 < `FARM_PLANT_SAME_FAMILY_MIN`（含自己）
- `overcrowding`：同 tile 作物数量 > `FARM_PANT_OVERCROWDING_MAX_PLANTS`
- `season`：非偏好季节
- `moisture`：湿度低于 `FARM_PLANT_DROUGHT_TOLERANCE`
- `nutrients`：营养不足
- `happiness`：未被照料 (tend)

最终压力等级（`farmplantstress.lua`）：

- `NONE`：累计压力点 ≤ 1
- `LOW`：≤ 6
- `MODERATE`：≤ 11
- `HIGH`：其他

产出影响（`farm_plants.lua`）：

- `NONE/LOW`：1 作物 + 2 种子
- `MODERATE`：1 作物 + 1 种子
- `HIGH`：仅 1 作物
- `NONE` 且允许巨型：可能生成巨型作物

## 营养系统

土壤营养三通道（index 1/2/3），每阶段成长结算一次（`farming_manager.lua`）：

- 消耗：按 `nutrient_consumption` 扣减
- 恢复：把消耗总量平均分配到“未消耗”的通道
- 不足：当前阶段压力直接成立，补肥仅影响后续阶段

关键调参：

- `FARM_PLANT_CONSUME_NUTRIENT_LOW/MED/HIGH`
- `STARTING_NUTRIENTS_MIN/MAX`
- 肥料营养值见 `fertilizer_nutrient_defs.lua`

## 湿度系统

`farming_manager.lua` 每 `SOIL_MOISTURE_UPDATE_TIME` 更新：

```
soil_moisture += dt * (rain_rate * SOIL_RAIN_MOD or temp_dry_rate + sum(drink_rate))
soil_moisture = clamp(soil_moisture, world_wetness, SOIL_MAX_MOISTURE_VALUE)
```

作物吸水率由 `farm_plant_defs.lua` 定义：`FARM_PLANT_DRINK_LOW/MED/HIGH`。

## 原始种子与杂草生成

`farm_plants.lua`：

- `FARM_PLANT_RANDOMSEED_WEED_CHANCE`：randomseed 生成杂草概率
- 否则基于 `VEGGIES.seed_weight` 抽取作物
- 偏好季节权重乘 `SEED_WEIGHT_SEASON_MOD`

`farming_manager.lua`：

- `SEASONAL_WEED_SPAWN_CAHNCE`：季节性杂草生成概率（秋/春）
- 生成窗口 = 当季剩余天数 * 0.25

## 杂草扩散

`weed_defs.lua` 定义扩散窗口与距离参数：

- `spread.stage`：触发扩散的生长阶段
- `time_min/time_var`：扩散计时
- `tilled_dist`：优先寻找犁地土
- `ground_dist` / `ground_dist_var`：地面扩散半径
- `tooclose_dist`：同类最小距离

`weed_plants.lua` 执行扩散，成功后延迟倍增。

## 数据产物

新增耕种机制索引产物：

- `data/index/wagstaff_farming_defs_v1.json`
- 由 `devtools/build_farming_defs.py` 构建
- 内容包含 `tuning`、`seed_weights`、`plants`、`weeds`、`fertilizers` 与统计信息
```

### File: docs/specs/MECHANISM_INDEX_SPEC.md
- mode: head
- size_bytes: 3037
- sha256_12: 0c1e16905f63

```md
# Wagstaff Mechanism Index v1 规范草案

目标：提供 DST 机制解析的最小可查询索引，优先落盘组件定义与 prefab 组件链路。

## 1. 产物与版本

- JSON: `data/index/wagstaff_mechanism_index_v1.json`
- SQLite: `data/index/wagstaff_mechanism_index_v1.sqlite`
- schema_version: 1
- JSON schema: `docs/specs/mechanism_index_v1.schema.json`
- SQLite 结构版本通过 meta 的 `db_schema_version` 标记（当前 v4）。

## 2. 顶层结构（JSON）

```yaml
schema_version: 1
meta: {schema, project_version, index_version, generated, tool, sources, scripts_sha256_12, ...}
counts:
  components_total: int
  prefabs_total: int
  components_used: int
  prefab_component_edges: int
components:
  total_files: int
  items: {component_id: Component}
prefabs:
  items: {prefab_id: PrefabLink}
component_usage:
  component_id: [prefab_id, ...]
links:
  prefab_component: [Link, ...]
```

### 2.1 Component

```yaml
type: "component"
id: "combat"
class_name: "Combat"
aliases: ["Combat", ...]
methods: ["SetDefaultDamage", ...]
fields: ["defaultdamage", ...]
events: ["attacked", ...]
requires: ["components/health", ...]
path: "scripts/components/combat.lua"
```

### 2.2 PrefabLink

```yaml
components: ["combat", "health", ...]
tags: ["monster", ...]
brains: ["brains/spiderbrain", ...]
stategraphs: ["SGspider", ...]
helpers: ["MakeSmallBurnable", ...]
files: ["scripts/prefabs/spider.lua", ...]
events: ["attacked", ...]
assets: [{type: "ANIM", path: "anim/spider.zip"}, ...]
component_calls:
  - component: "combat"
    methods: ["SetDefaultDamage(34)", ...]
    properties: ["defaultdamage = 34", ...]
```

### 2.3 Link

```yaml
source: "prefab"
source_id: "spider"
target: "component"
target_id: "combat"
```

## 3. SQLite 结构

核心表：

- `meta`：包含 `schema_version` 与 `db_schema_version`（SQLite 结构版本，当前 v4）
- `components(id, class_name, path, aliases_json, methods_json, fields_json, events_json, requires_json, raw_json)`
- `component_fields(component_id, field)`
- `component_methods(component_id, method)`
- `component_events(component_id, event)`
- `prefabs(id, components_json, tags_json, brains_json, stategraphs_json, helpers_json, files_json, events_json, assets_json, component_calls_json, raw_json)`
- `prefab_components(prefab_id, component_id)`
- `links(source, source_id, target, target_id, relation)`（relation 可选，缺省视为 prefab_component）

保留扩展表（后续解析落盘）：

- `stategraphs` / `stategraph_states` / `stategraph_events` / `stategraph_edges`
- `brains` / `brain_nodes` / `brain_edges`

## 4. 一致性与校验

- `links` 与 `prefab_components` 应可互相推导。
- `prefab_components.component_id` 必须存在于 `components.id`，否则记录为缺失组件。
- `component_usage` 与 `links` 数量应匹配，计入 `counts.prefab_component_edges`。

## 5. 兼容与演进

- v2 可加入 stategraph/brain 解析结果并填充扩展表。
- JSON/SQLite 输出需同步更新 schema_version 与 build_meta。
```

### File: docs/specs/SQLITE_V4_SPEC.md
- mode: head
- size_bytes: 7953
- sha256_12: 59fbc3f5852a
- note: TRUNCATED

```md
# Wagstaff SQLite v4 结构设计

目标：提供统一、可扩展的 SQLite 存储结构，支持 catalog / mechanism / i18n 等索引的一致落盘与查询。

## 1. 版本策略

- **db_schema_version**：SQLite 结构版本，当前为 `4`。
- **schema_version**：JSON 索引的 schema 版本（如 catalog v2 / mechanism v1）。
- **project_version / index_version**：来自 `conf/version.json`。

## 2. 文件布局（建议）

当前落盘仍保持按索引拆分：

- `data/index/wagstaff_catalog_v2.sqlite`：catalog v2（items/craft/cooking/assets 等）
- `data/index/wagstaff_mechanism_index_v1.sqlite`：mechanism v1（components/prefab links 等）

SQLite v4 结构以 **表分组** 方式定义；单个文件可只实现其所需的表分组。

## 3. 通用约定

- `id` 默认指 prefab id（小写）。
- `*_json` 字段为 JSON 文本（保持原始结构，便于回溯）。
- 所有 JSON/SQLite 产物在 `meta` 中写入 `project_version/index_version`。
- 规范的 join 表用于快速过滤；原始结构保留在 `raw_json`。

## 4. 通用表（所有 DB 必备）

```sql
CREATE TABLE meta (
  key TEXT PRIMARY KEY,
  value TEXT NOT NULL
);
```

meta 建议写入的 key：
- `schema_version`（JSON schema 版本）
- `db_schema_version`（SQLite 结构版本）
- `meta`（完整 meta JSON）
- `stats`（统计信息 JSON，可选）

## 5. Catalog 表分组

### 5.1 Items

```sql
CREATE TABLE items (
  id TEXT PRIMARY KEY,
  kind TEXT,
  name TEXT,
  categories_json TEXT,
  behaviors_json TEXT,
  sources_json TEXT,
  tags_json TEXT,
  components_json TEXT,
  slots_json TEXT,
  assets_json TEXT,
  prefab_files_json TEXT,
  raw_json TEXT NOT NULL
);

CREATE TABLE item_stats (
  item_id TEXT NOT NULL,
  stat_key TEXT NOT NULL,
  expr TEXT,
  expr_resolved TEXT,
  trace_key TEXT,
  value_json TEXT,
  raw_json TEXT,
  PRIMARY KEY (item_id, stat_key)
);
```

Join 表（过滤/检索）：

```sql
CREATE TABLE item_categories (item_id TEXT NOT NULL, category TEXT NOT NULL, PRIMARY KEY (item_id, category));
CREATE TABLE item_behaviors (item_id TEXT NOT NULL, behavior TEXT NOT NULL, PRIMARY KEY (item_id, behavior));
CREATE TABLE item_sources (item_id TEXT NOT NULL, source TEXT NOT NULL, PRIMARY KEY (item_id, source));
CREATE TABLE item_tags (item_id TEXT NOT NULL, tag TEXT NOT NULL, PRIMARY KEY (item_id, tag));
CREATE TABLE item_components (item_id TEXT NOT NULL, component TEXT NOT NULL, PRIMARY KEY (item_id, component));
CREATE TABLE item_slots (item_id TEXT NOT NULL, slot TEXT NOT NULL, PRIMARY KEY (item_id, slot));
```

### 5.2 Assets

```sql
CREATE TABLE assets (
  id TEXT PRIMARY KEY,
  name TEXT,
  icon TEXT,
  image TEXT,
  atlas TEXT,
  build TEXT,
  bank TEXT,
  raw_json TEXT NOT NULL
);
```

### 5.3 Craft / Cooking

```sql
CREATE TABLE craft_meta (key TEXT PRIMARY KEY, value_json TEXT NOT NULL);

CREATE TABLE craft_recipes (
  name TEXT PRIMARY KEY,
  product TEXT,
  tab TEXT,
  tech TEXT,
  builder_skill TEXT,
  station_tag TEXT,
  filters_json TEXT,
  builder_tags_json TEXT,
  raw_json TEXT NOT NULL
);

CREATE TABLE craft_ingredients (
  recipe_name TEXT NOT NULL,
  item_id TEXT NOT NULL,
  amount_num REAL,
  amount_value REAL,
  raw_json TEXT,
  PRIMARY KEY (recipe_name, item_id)
);

CREATE TABLE cooking_recipes (
  name TEXT PRIMARY KEY,
  priority REAL,
  weight REAL,
  foodtype TEXT,
  hunger_json TEXT,
  health_json TEXT,
  sanity_json TEXT,
  perishtime_json TEXT,
  cooktime_json TEXT,
  tags_json TEXT,
  card_ingredients_json TEXT,
  raw_json TEXT NOT NULL
);

CREATE TABLE cooking_ingredients (
  item_id TEXT PRIMARY KEY,
  tags_json TEXT,
  tags_expr TEXT,
  sources_json TEXT,
  raw_json TEXT NOT NULL
);
```

### 5.4 Catalog Index（列表/检索）

```sql
CREATE TABLE catalog_index (
  id TEXT PRIMARY KEY,
  name TEXT,
  icon TEXT,
  image TEXT,
  has_icon INTEGER,
  icon_only INTEGER,
  kind TEXT,
  categories_json TEXT,
  behaviors_json TEXT,
  sources_json TEXT,
  tags_json TEXT,
  components_json TEXT,
  slots_json TEXT
);
```

可选 FTS：

```sql
CREATE VIRTUAL TABLE catalog_index_fts
USING fts5(id, name, content='catalog_index', content_rowid='rowid');
```

### 5.5 Tuning Trace（可选）

```sql
CREATE TABLE tuning_trace (
  trace_key TEXT PRIMARY KEY,
  raw_json TEXT NOT NULL
);
```

### 5.6 关键索引（建议）

```sql
CREATE INDEX idx_items_kind ON items(kind);
CREATE INDEX idx_item_stats_key ON item_stats(stat_key);
CREATE INDEX idx_item_stats_item ON item_stats(item_id);
CREATE INDEX idx_item_cat ON item_categories(category);
CREATE INDEX idx_item_beh ON item_behaviors(behavior);
CREATE INDEX idx_item_src ON item_sources(source);
CREATE INDEX idx_item_tag ON item_tags(tag);
CREATE INDEX idx_item_comp ON item_components(component);
CREATE INDEX idx_item_slot ON item_slots(slot);
CREATE INDEX idx_craft_product ON craft_recipes(product);
CREATE INDEX idx_craft_tab ON craft_recipes(tab);
CREATE INDEX idx_craft_ing_item ON craft_ingredients(item_id);
CREATE INDEX idx_cooking_foodtype ON cooking_recipes(foodtype);
CREATE INDEX idx_catalog_name ON catalog_index(name);
```

## 6. Mechanism 表分组

```sql
CREATE TABLE components (
  id TEXT PRIMARY KEY,
  class_name TEXT,
  path TEXT,
  aliases_json TEXT,
  methods_json TEXT,
  fields_json TEXT,
  events_json TEXT,
  requires_json TEXT,
  raw_json TEXT NOT NULL
);

CREATE TABLE component_fields (
  component_id TEXT,
  field TEXT,
  PRIMARY KEY (component_id, field)
);
CREATE TABLE component_methods (
  component_id TEXT,
  method TEXT,
  PRIMARY KEY (component_id, method)
);
CREATE TABLE component_events (
  component_id TEXT,
  event TEXT,
  PRIMARY KEY (component_id, event)
);

CREATE TABLE prefabs (
  id TEXT PRIMARY KEY,
  components_json TEXT,
  tags_json TEXT,
  brains_json TEXT,
```

### File: tests/test_recipes.py
- mode: head
- size_bytes: 1653
- sha256_12: c854302c4b59

```py
#!/usr/bin/env python3
import sys
import time
from rich.console import Console
from rich.table import Table
import sys
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from core.engine import WagstaffEngine

console = Console()

def main():
    console.print("[bold blue]🧪 配方解析器验收测试 (基于 Wagstaff Engine)[/bold blue]")
    
    # 1. 启动引擎
    try:
        start_t = time.time()
        engine = WagstaffEngine(load_db=True)
        duration = (time.time() - start_t) * 1000
    except Exception as e:
        console.print(f"[red]引擎启动失败: {e}[/red]")
        return
    
    # 2. 统计
    count = len(engine.recipes.recipes)
    count_style = "green" if count > 500 else "red"
    
    console.print(f"加载耗时: [bold]{duration:.2f} ms[/bold]")
    console.print(f"发现配方: [{count_style}]{count}[/{count_style}]")

    # 3. 抽查
    check_list = ["spear", "armorwood", "hambat", "firestaff"]
    table = Table(title="关键物品验证", border_style="blue")
    table.add_column("Key", style="cyan")
    table.add_column("Name", style="dim")
    table.add_column("Ingredients", style="white")
    
    for item in check_list:
        real_name, data = engine.recipes.get(item)
        if data:
            ing_str = ", ".join([f"{i['item']}x{i['amount']}" for i in data['ingredients']])
            table.add_row(item, real_name, ing_str)
        else:
            table.add_row(item, "-", "[red]Not Found[/red]")
        
    console.print(table)

if __name__ == "__main__":
    main()
```

## 6. Snapshot Stats
```yaml
total_candidates: 122
included_records: 122
embedded_files: 120
hash_mode: embedded
embed_order: smart
timing_ms: {scan: 348, records: 6, render: 166, total: 906}
approx_total_tokens: 190741
max_total_bytes: 1500000
bytes_remaining: 724477
```
